<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.32.0-wmf.10</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Apache Ant</title>
    <ns>0</ns>
    <id>438891</id>
    <revision>
      <id>838409898</id>
      <parentid>836838747</parentid>
      <timestamp>2018-04-26T20:40:39Z</timestamp>
      <contributor>
        <username>Kiwi128</username>
        <id>12235997</id>
      </contributor>
      <comment>update stable version =&gt; 1.10.3</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15834">{{Infobox software
| name                   = Apache Ant
| logo                   = Apache-Ant-logo.svg
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| released               = {{Start date and age|df=yes|2000|07|19}}
| latest release version = 1.10.3
| latest release date    = {{Start date and age|2018|03|27}}&lt;ref&gt;{{cite web|url=https://ant.apache.org/antnews.html|accessdate=26 April 2018|title=Apache Ant Project News}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| platform               = [[Java SE]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Build tool]]
| license                = [[Apache License]] 2.0
}}

'''Apache Ant''' is a software tool for [[build automation|automating software build]] processes, which originated from the [[Apache Tomcat]] project in early 2000. It was a replacement for the [[Make (software)|Make]] build tool of Unix, and was created due to a number of problems with Unix's make.&lt;ref&gt;{{cite book | last1=Suereth | first1=Joshua | last2=Farwell | first2=Matthew | url=https://www.amazon.com/gp/product/1617291277?keywords=sbt%20in%20action&amp;qid=1417974741&amp;ref_=sr_1_1&amp;s=books&amp;sr=1-1 | title=SBT in Action: The simple Scala build tool | publisher=Manning Publications | year=2015 | isbn=9781617291272}}&lt;/ref&gt; It is similar to Make but is implemented using the [[Java (programming language)|Java]] language, requires the Java platform, and is best suited to building Java projects.

The most immediately noticeable difference between Ant and Make is that Ant uses [[XML]] to describe the code build process and its dependencies, whereas Make uses the [[Makefile#Makefiles|Makefile format]]. 
By default, the XML file is named &lt;code&gt;build.xml&lt;/code&gt;.

Released under an [[Apache License]] by the [[Apache Software Foundation]], Ant is an [[open-source software|open-source project]].

==History==
Ant ("Another Neat Tool")&lt;ref&gt;{{cite web | url=https://ant.apache.org/faq.html#ant-name | title=Why do you call it Ant? – Apache Ant FAQ}}&lt;/ref&gt; was conceived by [[James Duncan Davidson]] while preparing [[Sun Microsystems]]'s [[reference implementation|reference]] [[JavaServer Pages|JSP]] and [[Servlet]] engine, later [[Apache Tomcat]], for release as [[open-source]]. A [[proprietary software|proprietary]] version of Make was used to build it on the [[Solaris (operating system)|Solaris]] platform, but in the open-source world, there was no way of controlling which platform was used to build Tomcat; so Ant was created as a simple platform-independent tool to build Tomcat from directives in an XML "build file". Ant (version 1.1) was officially released as a stand-alone product on July 19, 2000.

Several proposals for an Ant version 2 have been made, such as AntEater by James Duncan Davidson, Myrmidon by [[Peter Donald (programmer)|Peter Donald]] and Mutant by [[Conor MacNeill (programmer)|Conor MacNeill]], none of which were able to find large acceptance with the developer community.&lt;ref&gt;{{cite web | url=http://codefeed.com/blog/?p=98 | title=The Early History of Ant Development | first=Conor | last=MacNeill}}&lt;/ref&gt;

At one time (2002), Ant was the build tool used by most Java development projects.&lt;ref&gt;{{cite book | title=Java Tools for eXtreme Programming | author=Wiley | year=2002 | page=76}}&lt;/ref&gt; For example, most open source Java developers include &lt;code&gt;build.xml&lt;/code&gt; files with their distribution.{{Citation needed|date=March 2010}} Because Ant made it trivial to integrate [[JUnit]] tests with the build process, Ant made it easy for willing developers to adopt [[test-driven development]], and even [[extreme programming]].

==Extensions==

WOProject-Ant&lt;ref&gt;{{cite web|url=http://www.objectstyle.org/confluence/display/WOL/WOProject-Ant |title=WOProject-Ant – WOProject / WOLips – Confluence |deadurl=yes |archiveurl=https://web.archive.org/web/20090108131210/http://www.objectstyle.org/confluence/display/WOL/WOProject-Ant |archivedate=2009-01-08 }}&lt;!-- Bot generated title --&gt;&lt;/ref&gt; is just one of many examples of a task extension written for Ant. These extensions are installed by copying their &lt;code&gt;.jar&lt;/code&gt; files into ant's &lt;code&gt;lib&lt;/code&gt; directory. Once this is done, these task extensions can be invoked directly in the typical &lt;code&gt;build.xml&lt;/code&gt; file. The WOProject extensions allow [[WebObjects]] developers to use ant in building their frameworks and apps, instead of using [[Apple Computer|Apple's]] [[Xcode]] suite.

&lt;code&gt;Antcontrib&lt;/code&gt;&lt;ref&gt;{{cite web | url=http://ant-contrib.sourceforge.net | title=Ant-Contrib}}&lt;/ref&gt; provides a collection of tasks such as conditional statements and operations on properties as well as other useful tasks.&lt;ref&gt;{{cite web | url=http://ant-contrib.sourceforge.net/tasks/tasks/index.html | title=Ant-Contrib Tasks}}&lt;/ref&gt;

&lt;code&gt;Ant-contrib.unkrig.de&lt;/code&gt;&lt;ref&gt;{{cite web | url=http://ant-contrib.unkrig.de | title=ant-contrib.unkrig.de}}&lt;/ref&gt; implements tasks and types for networking, [[Swing (Java)|Swing]] user interfaces, [[JSON]] processing and other.

Other task extensions exist for [[Perforce]], [[.NET Framework]], [[EJB]], and filesystem manipulations.&lt;ref&gt;{{cite web | url=https://ant.apache.org/manual/tasksoverview.html | title=Overview of Ant Tasks}}&lt;!-- Bot generated title --&gt;&lt;/ref&gt;

==Example==
Below is listed a sample &lt;code&gt;build.xml&lt;/code&gt; file for a simple Java "Hello, world" application. It defines four targets - &lt;code&gt;clean&lt;/code&gt;, &lt;code&gt;clobber&lt;/code&gt;, &lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;jar&lt;/code&gt; , each of which has an associated description. The &lt;code&gt;jar&lt;/code&gt; target lists the &lt;code&gt;compile&lt;/code&gt; target as a dependency. This tells Ant that before it can start the &lt;code&gt;jar&lt;/code&gt; target it must first complete the &lt;code&gt;compile&lt;/code&gt; target.

&lt;syntaxhighlight lang="xml"&gt;
&lt;?xml version="1.0"?&gt;
&lt;project name="Hello" default="compile"&gt;
    &lt;target name="clean" description="remove intermediate files"&gt;
        &lt;delete dir="classes"/&gt;
    &lt;/target&gt;
    &lt;target name="clobber" depends="clean" description="remove all artifact files"&gt;
        &lt;delete file="hello.jar"/&gt;
    &lt;/target&gt;
    &lt;target name="compile" description="compile the Java source code to class files"&gt;
        &lt;mkdir dir="classes"/&gt;
        &lt;javac srcdir="." destdir="classes"/&gt;
    &lt;/target&gt;
    &lt;target name="jar" depends="compile" description="create a Jar file for the application"&gt;
        &lt;jar destfile="hello.jar"&gt;
            &lt;fileset dir="classes" includes="**/*.class"/&gt;
            &lt;manifest&gt;
                &lt;attribute name="Main-Class" value="HelloProgram"/&gt;
            &lt;/manifest&gt;
        &lt;/jar&gt;
    &lt;/target&gt;
&lt;/project&gt;
&lt;/syntaxhighlight&gt;

Within each target are the actions that Ant must take to build that target; these are performed using built-in tasks. For example, to build the &lt;code&gt; compile &lt;/code&gt; target Ant must first create a directory called &lt;code&gt;classes&lt;/code&gt; (which Ant will do only if it does not already exist) and then invoke the Java compiler. Therefore, the tasks used are &lt;code&gt;mkdir&lt;/code&gt; and &lt;code&gt;javac&lt;/code&gt;.  These perform a similar task to the command-line utilities of the same name.

Another task used in this example is named &lt;code&gt;jar&lt;/code&gt;:

&lt;syntaxhighlight lang="xml"&gt;
&lt;jar destfile="hello.jar"&gt;
&lt;/syntaxhighlight&gt;

This Ant task has the same name as the common Java command-line utility, JAR, but is really a call to the Ant program's built-in JAR/ZIP file support. This detail is not relevant to most end users, who just get the JAR they wanted, with the files they asked for.

Many Ant tasks delegate their work to external programs, either native or Java. They use Ant's own {{tag|exec|open}} and {{tag|java|open}} tasks to set up the command lines, and handle all the details of mapping from information in the build file to the program's arguments and interpreting the return value. Users can see which tasks do this (e.g. {{tag|csv|open}}, {{tag|signjar|open}}, {{tag|chmod|open}}, {{tag|rpm|open}}), by trying to execute the task on a system without the underlying program on the path, or without a full Java Development Kit (JDK) installed.

==Portability==
One of the primary aims of Ant was to solve Make's portability problems. The first portability issue in a Makefile is that the actions required to create a target are specified as [[operating system shell|shell]] commands which are specific to the [[Platform (computing)|platform]] on which Make runs. Different platforms require different shell commands. Ant solves this problem by providing a large amount of built-in functionality that is designed to behave the same on all platforms. For example, in the sample &lt;code&gt;build.xml&lt;/code&gt; file above, the ''clean'' target deletes the &lt;code&gt;classes&lt;/code&gt; directory and everything in it. In a Makefile this would typically be done with the command:
 rm -rf classes/
&lt;code&gt;[[rm (Unix)|rm]]&lt;/code&gt; is a [[Unix]]-specific command unavailable in some other environments. [[Microsoft Windows]], for example, would use:
 rmdir /S /Q classes
In an Ant build file the same goal would be accomplished using a built-in command:
&lt;syntaxhighlight lang="xml"&gt;
 &lt;delete dir="classes"/&gt;
&lt;/syntaxhighlight&gt;

A second portability issue is a result of the fact that the symbol used to delimit elements of file system directory path components differs from one platform to another. Unix uses a forward slash (/) to delimit components whereas Windows uses a backslash (\). Ant build files let authors choose their favorite convention: forward slash or backslash for directories; semicolon or colon for path separators. It converts each to the symbol appropriate to the platform on which it executes.

==Limitations==
{{criticism section|date=September 2011}}
{{original research|section|date=September 2011}}

*Ant build files, which are written in [[XML]], can be complex and verbose, as they are hierarchical, partly ordered, and pervasively cross-linked. This complexity can be a barrier to learning. The build files of large or complex projects can become unmanageably large. Good design and modularization of build files can improve readability but not necessarily reduce size. Other build tools, such as [[Gradle]] or [[Apache Maven|Maven]], use more concise scripts at the expense of generality and flexibility.
*Many of the older tasks—the core ones that are used every day, such as {{tag|javac|open}}, {{tag|exec|open}} and {{tag|java|open}}—use default values for options that are not consistent with more recent versions of the tasks. Changing those defaults would break existing Ant scripts.
*When expanding properties in a string or text element, undefined properties are not raised as an error, but left as an unexpanded reference (e.g. &lt;code&gt;${unassigned.property}&lt;/code&gt;).
*Ant has limited fault handling rules.
*[[Lazy evaluation|Lazy property evaluation]] is not supported. For instance, when working within an Antcontrib {{tag|for|open}} loop, a property cannot be re-evaluated for a sub-value which may be part of the iteration. (Some third-party extensions facilitate a workaround; AntXtras flow-control tasksets do provide for cursor redefinition for loops.)
*In makefiles, any rule to create one file type from another can be written inline within the makefile. For example, one may transform a document into some other format by using rules to execute another tool. Creating a similar task in Ant is more complex: a separate task must be written in Java and included with the Ant build file in order to handle the same type of functionality. However, this separation can enhance the readability of the Ant script by hiding some of the details of how a task is executed on different platforms.

There exists a myriad of third-party Ant extensions (called ''antlibs'') that provide much of the missing functionality. Also, the [[Eclipse (software)|Eclipse]] [[integrated development environment]] (IDE) can build and execute Ant scripts, while the [[NetBeans]] IDE uses Ant for its internal build system. As both these IDEs are very popular development platforms, they can simplify Ant use significantly. (As a bonus, Ant scripts generated by NetBeans can be used outside that IDE as standalone scripts.)

==See also==
*[[Build automation]]
**[[List of build automation software]]
*[[Apache Jelly]], a tool for turning XML into executable code
*[[Apache Ivy]], a dependency manager which integrates tightly with Ant, subproject of Ant
*[[Apache Maven]], a project management and build automation tool primarily for Java
*[[Nant]], Ant-like tool targeted at the .NET Framework environment rather than Java
*[[Gradle]], a JVM build tool built with Groovy

==References==
{{Reflist}}

==Further reading==
{{refbegin}}
*{{cite book
| first1    = Steve
| last1     = Loughran
| first2    = Erik
| last2     = Hatcher
| title     = Ant in Action
| publisher = [[Manning Publications]]
| edition   = 2nd
| pages     = 600
| date      = July 12, 2007
| isbn      = 978-1-932394-80-1
| url       = 
}}
*{{cite book
| first1    = Steven
| last1     = Holzner
| title     = Ant – The Definitive Guide
| publisher = [[O'Reilly Media]]
| edition   = 2nd
| pages     = 334
| date      = April 13, 2005
| isbn      = 978-0-596-00609-9
| url       = http://oreilly.com/catalog/9780596006099/
}}
*{{cite book
| first1    = Matthew 
| last1     = Moodie
| title     = Pro Apache Ant
| publisher = [[Apress]]
| edition   = 1st
| pages     = 360 
| date      = November 16, 2005
| isbn      = 978-1-59059-559-6
| url       = http://www.apress.com/book/view/9781590595596
}}
*{{cite book
| first1    = Alexis T.
| last1     = Bell
| title     = ANT Java Notes: An Accelerated Intro Guide to the Java ANT Build Tool
| publisher = [[Virtualbookworm.com Publishing]]
| edition   = 1st
| pages     = 268 
| date      = July 7, 2005
| isbn      = 978-1-58939-738-5
| url       = http://www.virtualbookworm.com/mm5/merchant.mvc?Screen=PROD&amp;Store_Code=bookstore&amp;Product_Code=antjava
}}
*{{cite book
| first1    = Erik 
| last1     = Hatcher
| first2    = Steve 
| last2     = Loughran
| title     = Java Development with Ant
| publisher = [[Manning Publications]]
| edition   = 1st
| pages     = 672 
| date      = August 2002
| isbn      = 978-1-930110-58-8
| url       = 
}}
*{{cite book
| first1    = Glenn 
| last1     = Niemeyer
| first2    = Jeremy 
| last2     = Poteet
| title     = Extreme Programming with Ant: Building and Deploying Java Applications with JSP, EJB, XSLT, XDoclet, and JUnit
| publisher = [[SAMS Publishing]]
| edition   = 1st
| pages     = 456 
| date      = May 29, 2003
| isbn      = 978-0-672-32562-5
| url       = http://www.informit.com/store/product.aspx?isbn=0672325624
}}
*{{cite book
| first1    = Alan
| last1     = Williamson
| title     = Ant – Developer's Handbook
| publisher = [[SAMS Publishing]]
| edition   = 1st
| pages     = 456 
| date      = November 1, 2002
| isbn      = 978-0-672-32426-0
| url       = http://www.informit.com/store/product.aspx?isbn=0672324261
}}
*{{cite book
| first1    = Bernd 
| last1     = Matzke
| title     = ANT: The Java Build Tool In Practice
| publisher = [[Charles River Media]]
| edition   = 1st
| pages     = 280 
| date      = September 2003
| isbn      = 978-1-58450-248-7
| url       = http://www.powells.com/biblio?isbn=9781584502487
}}
{{refend}}

==External links==
{{Wikibooks|Apache Ant}}
*{{Official website}}


{{Apache}}

{{Authority control}}
[[Category:Apache Software Foundation|Ant]]
[[Category:Build automation]]
[[Category:Compiling tools]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) libraries]]
[[Category:Java development tools]]
[[Category:Software using the Apache license]]
[[Category:XML software]]</text>
      <sha1>08aao2388lm9l62e2krcit7u5dhl53o</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Portable Runtime</title>
    <ns>0</ns>
    <id>1825377</id>
    <revision>
      <id>807098096</id>
      <parentid>805144098</parentid>
      <timestamp>2017-10-25T21:54:33Z</timestamp>
      <contributor>
        <ip>77.119.130.74</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4102">{{Infobox software
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.6.3
| latest release date    = {{Start date and age|2017|10|22}}&lt;ref&gt;{{cite web|url=https://apr.apache.org/|title=The Apache Portable Runtime Project|accessdate=25 October 2017|at=Recommended releases section}}&lt;/ref&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[C (programming language)|C]]
| genre                  = Development [[Library (computer science)|library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//apr.apache.org/}}
}}

The '''Apache Portable Runtime''' (APR) is a supporting library for the [[Apache HTTP Server|Apache]] [[web server]]. It provides a set of [[API]]s that map to the underlying operating system (OS).&lt;ref name="Kerner_2005-12-02_internetnews" /&gt; Where the OS does not support a particular function, APR will provide an emulation. Thus programmers can use the APR to make a program truly portable across platforms.

APR originally formed a part of [[Apache HTTP Server]], but the [[Apache Software Foundation]] spun it off into a separate project. Other applications can use it to achieve platform independence.

== Functionality ==
The range of platform-independent functionality provided by APR includes:
* [[Memory allocation]] and [[memory pool]] functionality
* [[Atomic operations]]
* Dynamic [[Library (computer science)|library]] handling
* File [[I/O]]
* Command-argument parsing
* [[Lock (software engineering)|Locking]]
* [[Hash table]]s and [[array data structure|array]]s
* [[Mmap]] functionality
* [[Berkeley sockets|Network sockets]] and protocols
* [[Thread (computer science)|Thread]], [[Process (computing)|process]] and [[Mutual exclusion|mutex]] functionality
* [[Shared memory (interprocess communication)|Shared memory]] functionality
* Time routines
* User and group ID services

== Similar projects ==
* [[GLib]] – provides similar functionality. It supports many more data structures and OS-independent functions, but fewer [[Inter-process communication|IPC]]-related functions. (GLib lacks local and global locking and shared-memory management.)
* [[Netscape Portable Runtime]] (NSPR) is a cross-platform abstraction library used by the [[Mozilla]] project. It is used by another subproject of [[Mozilla application framework]] (XPFE) to provide cross-platform [[graphical user interface]] (GUI) functionality.
* [[Adaptive Communication Environment]] (ACE) is an object-oriented library written in C++ similar in functionality to APR. It is widely deployed in commercial products.&lt;ref&gt;{{cite web|title=ACE and TAO Success Stories |url=http://www.cs.wustl.edu/~schmidt/ACE-users.html |accessdate=2008-07-31 |archiveurl=https://web.archive.org/web/20080829164836/http://www.cs.wustl.edu/~schmidt/ACE-users.html |archivedate=29 August 2008 |deadurl=no }}&lt;/ref&gt;
* [http://www.hyperrealm.com/main.php?s=commoncpp commonc++] is a cross-platform C++ class library for systems programming, with much of the same functionality as APR.
* [[POCO C++ Libraries|POCO]] is a modern C++ framework similar in concept but more extensive than APR.
* [[WxWidgets]] is an object-oriented cross-platform GUI library that also provides abstraction classes for database communication, [[Inter-process communication|IPC]] and networking functionality.
* [[KDE Frameworks]] – used by [[KDE SC]]

== References ==
{{reflist|refs=

&lt;ref name="Kerner_2005-12-02_internetnews"&gt;[http://www.internetnews.com/dev-news/article.php/3568241/Stable+Apache+Release+Hits.htm Stable Apache Release Hits], Sean Michael Kerner, 2 December 2005, "Apache Portable Runtime (APR) 1.0 API, which provides libraries that interface between the underlying operating system and the server."&lt;/ref&gt;

}}

== External links ==
{{wikibooks|APR tutorial||APR wikibooks tutorial}}
* {{official website|//apr.apache.org/}}

{{Apache}}

[[Category:Apache Software Foundation|Portable Runtime]]
[[Category:Application programming interfaces]]
[[Category:Software using the Apache license]]</text>
      <sha1>0861fmgn70du1wkiigl05h7wm5eq2la</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Beehive</title>
    <ns>0</ns>
    <id>1789039</id>
    <revision>
      <id>754865761</id>
      <parentid>744596060</parentid>
      <timestamp>2016-12-14T22:36:03Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7417">{{About|a Java application framework|the [[Hadoop]]-based data warehouse infrastructure|Apache Hive}}
{{Infobox software
| name                   = Apache Beehive
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version = 1.0.2
| latest release date    = {{release date|2006|12|04}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = Java Application Framework
| license                = [[Apache License]] 2.0
| website                = {{URL|https://beehive.apache.org}}
}}

'''Apache Beehive''' is a discontinued Java Application Framework that was designed to simplify the development of [[Java EE]]-based applications. It makes use of various open-source projects at [[Apache Software Foundation|Apache]] such as [[XMLBeans]]. It leverages innovations in Java 5 which include [[Java Community Process|JSR-175]], which is a facility for annotating fields, methods and classes so that they can be treated in special ways by runtime tools. It builds on the framework developed for [[BEA Systems]] [[Weblogic]] Workshop for its 8.1 series. BEA later decided to donate the code to Apache.

==History==
Version 8.1 of BEA's Weblogic Workshop includes a number of significant enhancements to version 7.0. The previous version was more focused on creating industrial-strength [[web services]] quickly. However, 7.0 did not have many customers, and it failed to create a stir in the market. However, for version 8.1, BEA created a whole new [[Integrated development environment|IDE]] which helped programmers to develop [[Java EE]]-based applications more quickly. This was significantly better than 7.0 with more advanced features and also won several awards and gained a lot of critical acclaim. However, a new revolution was brewing in the [[Java (programming language)|Java]] universe in the form of [[Eclipse (computing)|Eclipse]] and it seemed like everyone was moving towards it. Although Workshop 8.1 did not succeed as much as it intended to, the Weblogic Workshop Framework which was developed for 8.1 version Workshop was recognized as a good solid framework. In order that it can be used with other [[Java EE]]-based application servers, BEA decided to open-source the project under the purview of the [[Apache Software Foundation]].
Latest version of Beehive was released in December 4, 2006; its lifetime has ended in January 2010, when it has been retired and moved to [[Apache Attic]].

==Beehive components==
===Netui Page Flows===
This is an application framework built on top of [[Jakarta Struts|Apache Struts]] which allows easier tooling and automatic updating of the various Struts configuration files.

===Controls===
This is the heart of the Beehive framework. A control can be defined as a program which can be used by the developer to quickly gain access to enterprise-level resources such as [[Enterprise Java Beans]] (EJBs), [[web services]] etc. For example consider accessing an old [[Enterprise_JavaBean#Legacy|legacy EJB 2]] bean. It involved a lot of boiler-plate code like getting access to an home interface, then creating/finding an EJB using finder methods and then accessing the remote methods of the bean. Using a control simplified this because it did most of the boiler-plate or routine coding for the developer, who could then concentrate more on business logic rather than worrying about the inner-details of [[Java EE]] technology. If the developer was sufficiently advanced, even then it was useful because then the developer could concentrate on more useful things like constructing a [[Facade pattern|Facade]] to a complex set of application APIs. In essence a control to a legacy EJB 2 bean ensured that the developer could simply use the control and call any business method of the EJB, using it in the same way as any other [[Java (programming language)|Java]] class. When EJB 3 came around, such simplification was already provided by the EJB specification itself,&lt;ref&gt;"This release made it much easier to write EJBs, using 'annotations' rather than the complex 'deployment descriptors' used in version 2.x. The use of home and remote interfaces and the ejb-jar.xml file were also no longer required..." [[Enterprise_JavaBean##EJB_3.0.2C_final_release_.282006-05-11.29|EJB]]&lt;/ref&gt;&lt;ref&gt;[[Enterprise_JavaBean#Example|EJB 3 example]]&lt;/ref&gt;&lt;ref&gt;"Enterprise Java Beans (EJB) 3.0 is a deep overhaul and simplification of the EJB specification." http://www.jboss.org/ejb3&lt;/ref&gt;&lt;ref&gt;"... the heavyweight programming paradigm in EJB 2.x, the flawed persistence model in EJB 2.x entity beans..." "In our view, one of the most important changes in EJB 3.1 is the redefinition of EJBs as simple managed bean POJOs with additional services."  http://blog.caucho.com/?p=384&lt;/ref&gt; and Beehive controls were of little further use here.&lt;ref&gt;"... the EJB 3 client model has essentially standardized much of the value-add that the [Beehive] EJB control offered in terms of simplifying the EJB 2.1 client model" http://markmail.org/message/mh43akcleflzes3r&lt;/ref&gt;&lt;ref&gt;Andre McCulloch, "OK, these are great points that lead me to believe that and [sic] EJB3 control does not provide much value add for Beehive right now." http://markmail.org/message/ktec5f4gsbw22ijb&lt;/ref&gt; The Controls come with a standard set of controls wiz EJB Control, Webservice Control, Database Control and JMS Control. Custom controls can also be developed which in turn could make use of the controls already built-in.

===Webservices===
This is the third component of Beehive and it enables a developer to create webservices using meta-data/annotations quickly. In essence by using meta-data/annotations one can create complex [[web services]] utilizing features like conversation, state etc quickly and since all the meta-data/annotations are in one file, it is easier to debug and maintain. Using this approach any plain Java class can be converted into a web service just by the addition of annotations into the Java source files. This is based on [[Java Community Process|JSR-181]] which builds on [[Java Community Process|JSR-175]].

==See also==
{{Portal|Java}}
&lt;!-- formatting; please do not remove until some more text lines are added to compensate spacing --&gt;

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{citation
| first1     = Kunal
| last1      = Mittal
| first2     = Srinivas
| last2      = Kanchanavally
| date       = August 15, 2005
| title      = Pro Apache Beehive
| edition    = 1st
| publisher  = [[Apress]]
| pages      = 240
| isbn       = 978-1-59059-515-2
| url        = http://www.apress.com/book/view/9781590595152
}}
{{Refend}}

==External links==
*[http://beehive.apache.org Apache Beehive home site]
*[http://www.bea.com/framework.jsp?CNT=index.htm&amp;FP=/content/products/weblogic/workshop/ Weblogic Workshop]
*[https://web.archive.org/web/20100629011458/http://archive.eclipse.org/technology/archives/pollinate-project.tar.gz Pollinate Project] (An Eclipse plugin for Apache Beehive, now [http://www.eclipse.org/technology/archived.php archived] and inactive)

{{Apache}}

{{DEFAULTSORT:Apache Beehive}}
[[Category:Apache Software Foundation|Beehive]]
[[Category:Java platform software]]</text>
      <sha1>eny1ai1ys7vrzv2lhk101odfj9bl32d</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Cocoon</title>
    <ns>0</ns>
    <id>795882</id>
    <revision>
      <id>789524406</id>
      <parentid>785201988</parentid>
      <timestamp>2017-07-07T21:45:12Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5936">{{ Infobox Software
| name                   = Apache Cocoon
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 2.2.0
| latest release date    = {{release date|2008|05|15}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[web application framework]]
| license                = [[Apache License]] 2.0
| website                = {{url|//cocoon.apache.org}}
}}
'''Apache Cocoon''', usually just called '''Cocoon''', is a [[web application framework]] built around the concepts of [[Pipeline (software)|pipeline]], [[separation of concerns]] and component-based web development.  The framework focuses on [[XML]] and [[XSLT]] publishing and is built using the [[Java (programming language)|Java programming language]].  The flexibility afforded by relying heavily on XML allows rapid content publishing in a variety of formats including [[HTML]], [[PDF]], and [[Wireless Markup Language|WML]].  The [[content management system]]s [[Apache Lenya]] and [[Daisy (software)|Daisy]] have been created on top of the framework.  Cocoon is also commonly used as a [[data warehousing]] [[Extract, transform, load|ETL]] tool or as [[Middleware (distributed applications)|middleware]] for transporting data between systems.

==Sitemap==

The sitemap is at the core of Cocoon. It's here that the web site developer configures the different Cocoon components, and defines the [[client–server model|client–server]] interactions in what Cocoon refers to as the ''[[XML pipeline|Pipelines]]''.

==Components==

The components within Cocoon are grouped by function.

===Matchers===

Matchers are used to match user requests such as [[Uniform Resource Locator|URL]]s or [[http cookie|cookie]]s against [[Wildcard character|wildcard]] or [[regular expression]] patterns. Each user request is tested against matchers in the sitemap until a match is made. It is within a matcher that the response to a particular request is specified.

===Generators===

Generators create a [[stream (computing)|stream]] of data for further processing. This stream can be generated from an existing XML document or there are generators that can create XML from scratch to represent something on the server, such as a directory structure or image data.

====XSP====
One type of generator is an XML Server Page ('''XSP''' [http://cocoon.apache.org/1.x/xsp.html]), an XML document containing tag-based directives that specify how to generate dynamic content at request time. Upon Cocoon processing, these directives are replaced by generated content so that the resulting, augmented XML document can be subject to further processing (typically an XSLT transformation). XSPs are transformed into Cocoon producers, typically as Java classes, though any scripting language for which a Java-based processor exists could also be used.

Directives can be either built-in ("XSP") or user-defined processing tags, both of which are defined in ''logicsheets''.  Tags are defined using XSLT templates that describe how the tags (represented as XML nodes) are transformed into other XML nodes or into procedural code such as Java. The tags are used to embed procedural logic, substitute expressions, retrieve information from the web server environment, and other operations.

Note that XSP is deprecated in recent releases of Cocoon.

===Transformers===

Transformers take a stream of data and change it in some way. The most common transformations are performed with XSLT to change one xml format into another. But there are also transformers that take other forms of data ([[SQL]] commands for example).

===Serializers===

A serializer turns an XML event stream into a sequence of bytes (such as HTML) that can be returned to the client. There are serializers that allow you to send the data in many different formats including [[HTML]], [[XHTML]], [[portable document format|PDF]], [[Rich Text Format|RTF]], [[Scalable Vector Graphics|SVG]], [[Wireless Markup Language|WML]] and [[plain text]], for example.

===Selectors===
Selectors offer the same capabilities as a [[switch statement]]. They are able to select particular elements of a request and choose the correct pipeline part to use.

===Views===

Views are mainly used for testing. A view is an exit point in a pipeline. You can put out the XML-Stream which is produced till this point. So you can see if the application is working right.

===Readers===
Publish content without parsing it (no [[XML]] processing). Used for images and such.

===Actions===
Actions are Java classes that execute some business logic or manage new content production.

==The Pipeline==

A ''[[XML pipeline|pipeline]]'' is used to specify how the different Cocoon components interact with a given request to produce a [[Output (computing)|response]]. A typical pipeline consists of a generator, followed by zero or more transformers, and finally a serializer.

==See also==
* [[Reactor pattern]] - the design pattern that Cocoon is based on.
* [[XProc]] - a W3C Standard for modelising of XML pipeline.

==External links==
* [//cocoon.apache.org/ The Apache Cocoon Project]
* [//cocoon.apache.org/2.1/ Cocoon 2.1 Documentation]
* [//www.apache.org/ The Apache Software Foundation]
* [https://code.google.com/p/pycoon/ Pycoon] - A Python port of cocoon.
* [https://web.archive.org/web/20071011175608/http://www.nexista.org/ Nexista] - A PHP cocoon clone
* [http://www.paloose.org/pp/index.html/ Paloose] - Cocoon without Java (A PHP Clone)
* [https://web.archive.org/web/20120514225102/http://cocoon.zones.apache.org/cocoon21/samples/ Cocoon Sample programs]
{{Application frameworks}}
{{apache}}

{{DEFAULTSORT:Apache Cocoon}}
[[Category:Apache Software Foundation|Cocoon]]
[[Category:Web frameworks]]
[[Category:Java platform software]]</text>
      <sha1>80dmbj91za4ohlz8m5weaohok7vaj4d</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Directory</title>
    <ns>0</ns>
    <id>3672620</id>
    <revision>
      <id>845145456</id>
      <parentid>845145140</parentid>
      <timestamp>2018-06-09T18:32:04Z</timestamp>
      <contributor>
        <username>Pyschobbens</username>
        <id>11634328</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3665">{{ Infobox Software
| name                   = Apache Directory Server
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.0.0-M24
| latest release date    = {{release date|2017|06|07}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[LDAP]]
| license                = [[Apache License]] 2.0
| website                = {{url|//directory.apache.org/}}
}}
'''Apache Directory''' is an [[open source]] project of the [[Apache Software Foundation]]. Its flagship product, the '''Apache Directory Server''', originally written by Alex Karasulu, is an embeddable [[directory server]] entirely written in [[Java (programming language)|Java]]. It was certified [[Lightweight Directory Access Protocol|LDAP]]v3-compatible by [[The Open Group]] in 2006.&lt;ref&gt;{{Cite web |title=The Apache Software Foundation Is Latest Technology Leader To Embrace LDAP Standard And Achieve Open Group Certification |url=http://opengroup.org/comm/press/11oct06.html |publisher=The Open Directory |date=11 October 2006 |accessdate=30 June 2009 }}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=What Apache Directory Server is |url=http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html |accessdate=30 June 2009 |quote=Please note that the server has been successfully certified by the Open Group in September 2006 ("LDAP certified").| archiveurl= https://web.archive.org/web/20090531195522/http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html| archivedate= 31 May 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Besides LDAP, the server supports other protocols as well, and a [[Kerberos (protocol)|Kerberos]] server.&lt;ref&gt;{{Cite web |title=What Apache Directory Server is |url=http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html |accessdate=30 June 2009 |quote=Other network protocols like Kerberos and NTP are supported| archiveurl= https://web.archive.org/web/20090531195522/http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html| archivedate= 31 May 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

There exist these subprojects: 
* '''Apache Directory Studio''' - is an LDAP browser/editor for data, schema, [[LDAP Data Interchange Format|LDIF]], and [[Directory Service Markup Language|DSML]] written in an [[Eclipse (software)|Eclipse]]-based framework. 
* '''Apache eSCIMo''' - is a Java-based implementation of the [[System for Cross-domain Identity Management|SCIM]] protocol. 
* [[Apache Fortress]] - is a standards-based access management system, written in Java. 
* '''Apache Kerby''' - is a Kerberos implementation written in Java. 
* [[Apache LDAP API]] - is an SDK for directory access in Java. 
* '''Apache Mavibot''' - is a database application for Java applications. 

== See also ==
{{Portal|Java}} 
*[[List of LDAP software]]

== References ==
{{Reflist}}

== External links ==
*[//directory.apache.org/ Apache Directory Server]
*[//directory.apache.org/studio/ Apache Directory Studio]
*[//directory.apache.org/mavibot/ Apache Directory Mavibot]
*[//directory.apache.org/escimo/ Apache Directory eSCIMo]
*[//directory.apache.org/fortress/ Apache Directory Fortress]
*[//directory.apache.org/kerby/ Apache Directory Kerby]
*[//directory.apache.org/api/ Apache Directory LDAP API]

{{apache}}

[[Category:Apache Software Foundation|Directory]]
[[Category:Directory services]]
{{network-software-stub}}</text>
      <sha1>iz4aozv5u0aogdehpkryrofvf2ffx56</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Forrest</title>
    <ns>0</ns>
    <id>5252792</id>
    <revision>
      <id>785205393</id>
      <parentid>780092241</parentid>
      <timestamp>2017-06-12T06:01:26Z</timestamp>
      <contributor>
        <username>Kiwi128</username>
        <id>12235997</id>
      </contributor>
      <comment>format using {{url}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2039">{{ Infobox Software
| name                   = Apache Forrest
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 0.9
| latest release date    = {{release date|2011|02|07}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[web framework|Web Framework]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://forrest.apache.org}}
}}
'''Apache Forrest''' is a web-publishing framework based on [[Apache Cocoon]]. It is an XML [[single source publishing]] framework that allows multiple types of data-files as input, such as various popular word processing and spreadsheet files, as well as two wiki dialects.  Plugins are available to support additional formats, both for input as well as output (such as [[PDF]]).

Forrest is not a [[content management system|content management system (CMS)]], as it lacks the full workflow and admin functions of a CMS. Its primary use is in integrating and aggregating content from various sources and presenting them in a unified format for human consumption. "Single source" is this context does not mean that it is restricted to aggregating from ''only'' a single source, but rather that the multiple output formats can be maintained whilst still only needing to maintain a single source document.

==Trivia==
While Apache Forrest has {{as of|2017|lc=y}} not seen a new release for 6 years and was started in 2002, the website still claims "Apache Forrest is fairly new".

==See also==
* [[Apache Software Foundation]]
* [[Apache Cocoon]]
* [[Apache Lenya]]

==External links==
* [http://forrest.apache.org/ Apache Forrest website]
{{apache}}

{{Authority control}}
[[Category:Apache Software Foundation|Forrest]]
[[Category:Web design]]
[[Category:Technical communication tools]]

{{web-software-stub}}</text>
      <sha1>mzvspzb9uofnuoxk8kdnjgegyxq8u58</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Geronimo</title>
    <ns>0</ns>
    <id>1614341</id>
    <revision>
      <id>837239538</id>
      <parentid>829583464</parentid>
      <timestamp>2018-04-19T15:48:45Z</timestamp>
      <contributor>
        <username>SheriffIsInTown</username>
        <id>21573361</id>
      </contributor>
      <comment>Filled in 3 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10259">{{Infobox Software
| name                   = Apache Geronimo
| logo                   = [[File:Apache Geronimo Logo Large.png|100px|Apache Geronimo Logo]]
| screenshot             = Apache Geronimo Administration Console Screenshot.png
| screenshot size        = 300px
| caption                = Apache Geronimo Web Administration Console
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = {{Latest stable software release/Geronimo Application Server}}
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]] ([[JVM]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[application server|Web Application Server]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://geronimo.apache.org}}
}}
'''Apache Geronimo''' is an [[open source]] [[application server]] developed by the [[Apache Software Foundation]] and distributed under the [[Apache license]].

Geronimo 3, the current version, is compatible with the [[Java Platform, Enterprise Edition|Java Enterprise Edition]] (Java EE) 6 specification and therefore supports technologies such as [[Java Message Service|JMS]], [[Enterprise JavaBean]]s, [[Java EE Connector Architecture|Connector]]s, [[servlets]], [[JavaServer Page|JSP]], [[JavaServer Faces|JSF]], [[Unified Expression Language]] and [[JavaMail]]. This allows developers to create enterprise applications that are portable and scalable, and that integrate with legacy technologies. The older Geronimo 2 is compatible with Java EE 5.

[[IBM]] has in the past provided considerable support to the project through marketing, code contributions, and the funding of several project committers. In October 2005, IBM announced a free edition of its [[WebSphere]] application server named [[IBM WebSphere Application Server Community Edition|Websphere Application Server Community Edition]] (WASCE), which is actually a distribution of Geronimo and despite its name not related to the commercial WebSphere server.&lt;ref&gt;{{cite web|url=http://www14.software.ibm.com/webapp/iwm/web/preLogin.do?lang=en_US&amp;source=wsced|title=IBM WebSphere Application Server Community Edition 2018/04/19 11:48:21|author=|date=24 February 2005|website=www14.software.ibm.com|accessdate=19 April 2018}}&lt;/ref&gt; However, IBM has withdrawn marketing and support for WASCE in 2013.&lt;ref&gt;{{cite web|url=http://www-01.ibm.com/common/ssi/rep_ca/1/897/ENUS913-081/ENUS913-081.PDF|title= Withdrawal Announcement|author=|date=|website=ibm.com|accessdate=19 April 2018}}&lt;/ref&gt; Other commercial supporters included [[AMD]], Chariot Solutions, Simula Labs, and Virtuas.

Activity on Apache Geronimo has now largely ceased. Previous prolific Geronimo committers like David Jencks and others who are at IBM are now working on the [[IBM WebSphere Application Server#Version 8.5|Liberty Profile]] application server.&lt;ref&gt;{{cite web|url=http://arjan-tijms.omnifaces.org/2014/05/implementation-components-used-by.html?showComment=1399508132460#c5604447523115853997|title=Implementation components used by various Java EE servers|author=|date=|website=arjan-tijms.omnifaces.org|accessdate=19 April 2018}}&lt;/ref&gt;

== Components ==
Like an enterprise [[operating system]], Geronimo is built on a [[Kernel (computer science)|kernel]]—a [[microkernel]] that lays the foundation for everything above it. Geronimo's kernel is Java EE agnostic. Its sole purpose is to manage Geronimo's building blocks. Geronimo is marked by an architectural design that is based on the concept of [[Inversion of Control]] (IoC) (sometimes called [[Dependency Injection]]), which means that the kernel has no direct dependency on any of its [[Component-based software engineering|components]]. The kernel is a framework for services that controls the service life cycle and [[metadata registry|registry]]. The kernel is based on Java EE. It works with Java EE services and components to build specific configurations—one of which is a full Java EE [[solution stack]].

A majority of the Geronimo services are added and configured through GBeans to become a part of the overall application server. A ''GBean'' is the interface that connects the component to the kernel. Each GBean can maintain state, depend on, and interrelate with other GBeans, and operate on events from the kernel and other GBeans. The GBeans interface makes it possible to switch between two [[servlet container]]s, for example [[Jetty (web server)|Jetty]] or [[Apache Tomcat|Tomcat]], without affecting the whole architecture using a GBeans interface. This flexible architecture makes it possible for the Geronimo developers to integrate several existing field-tested [[open source software]] projects.

Here a list of the open source components that are included in the Geronimo project.

{| class="wikitable"
|-
! Component
! Description
|-
| [[Apache Tomcat]] 
| HTTP server and Servlet container supporting [[Java Servlet]] 2.5 and [[JavaServer Pages]] (JSP) 2.1.
|-
| [[Jetty (web server)|Jetty]] 
| HTTP server and Servlet container supporting Java Servlet 2.5 and JavaServer Pages 2.1—an alternative to the Tomcat server.
|-
| [[Apache ActiveMQ]] 
| Open source [[Java Message Service|Java Message Service (JMS)]] 1.1 applications provider and supporter of message-driven beans (MDBs).
|-
| [[Apache OpenEJB]] 
| Open source [[Enterprise JavaBean]]s (EJB) Container System and EJB Server that supports Enterprise JavaBeans at the 3.0 level, including [[Java Persistence API|Container Managed Persistence]] 2 (CMP2) and [[EJB QL|EJB Query Language]] (EJB QL).
|-
| [[Apache OpenJPA]] 
| Open source [[Java Persistence API]] (JPA) 1.0 implementation.
|-
| [[Apache ServiceMix]] 
| Open source [[Enterprise Service Bus]] (ESB) and component suite based on the [[Java Business Integration]] (JBI) standard on JSR 208.
|-
| [[Apache Axis]] and [[Apache Scout]] 
| Axis is a Simple Object Access Protocol ([[SOAP]]) implementation, while Scout is a JSR 93 ([[JAXR]]) implementation. These provide support for [[Web Services]] and [[Web Services Interoperability]] Organization (WS-I) Basic Profile support.
|-
| [[Apache CXF]] 
| [[Web Services]] frameworks with variety of protocols such as SOAP, XML/HTTP, [[Representational State Transfer|RESTful]] [[HTTP]], or [[CORBA]] and work over a variety of transports such as [[HTTP]], [[Java Message Service|JMS]] or [[Java Business Integration|JBI]].
|-
| [[Apache Derby]] 
| Full-fledged [[relational database management system]] (RDBMS) with native [[Java Database Connectivity]] (JDBC) support.
|-
| [[Apache WADI]] 
| [[Cluster (computing)|Cluster]]ing, [[Load balancing (computing)|load balancing]] and [[failover]] solution for the [[Web application framework|web application container tier]]. (The project is currently in incubation under the [[Apache Incubator]].)
|-
| [[MX4J]] 
| [[Java Management Extensions]] that supplies tools for managing and monitoring applications, system objects, devices and service-oriented networks.
|}

== See also ==
Other Java EE application servers:
* [[Apache TomEE]]
* [[JBoss AS]]
* [[IBM WebSphere Application Server|WebSphere AS]]
* [[WebLogic|WebLogic Server]]
* [[Comparison of application servers#Java EE|Comparison of application servers]]
* [[GlassFish]]
* [[Payara Server]]

==References==
{{reflist}}

==Bibliography==
{{refbegin}}
*{{citation
| first     = Aaron
| last      = Mulder
| year      = 2007
| title     = Apache Geronimo Development and Deployment
| publisher = [[Addison-Wesley Professional]]
| isbn      = 0-321-33483-3
| url       = http://www.chariotsolutions.com/geronimo/index.html
}}
*{{citation
| first     = Kishore
| last      = Kumar
| year      = 2006
| title     = Pro Apache Geronimo
| publisher = [[Apress]]
| isbn      = 1-59059-642-0
| url       = http://www.apress.com/book/view/9781590596425
}}
*{{citation
| first1    = Jeff 
| last1     = Genender
| first2    = Bruce
| last2     = Snyder
| first3    = Sing
| last3     = Li
| year      = 2006
| title     = Professional Apache Geronimo
| publisher = [[Wrox Press|Wrox]]
| isbn      = 0-471-78543-1
| url       = http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471785431.html
}}
*{{citation
| first     = David
| last      = Blevins
| year      = 2004
| title     = Geronimo: A Developer's Notebook
| publisher = [[O'Reilly Media]]
| isbn      = 0-596-00671-3
}}
{{refend}}

==External links==
*[http://geronimo.apache.org Apache Geronimo]
*[http://www-128.ibm.com/developerworks/opensource/top-projects/geronimo.html Geronimo resources area at IBM developerWorks]
*[http://www.ibm.com/developerworks/websphere/zones/was/wasce.html WebSphere Application Server Community Edition resources area at IBM developerWorks]
*[http://www-306.ibm.com/software/info1/websphere/index.jsp?tab=landings/was-ce Announcing IBM WebSphere Application Server Community Edition]
*[http://people.apache.org/~hogstrom/performance/geronimo/2.0/Geronimo2.0.2PerformanceReport-v01draft.pdf Geronimo 2.0.2 vs 1.1.1 Performance report]
*[http://www.ibm.com/developerworks/websphere/library/techarticles/0709_jain/0709_jain.html What's new in WebSphere Application Server Community Edition V2.0]
*[http://www.ibm.com/developerworks/websphere/library/techarticles/0807_jain/0807_jain.html?S_TACT=105AGX10&amp;S_CMP=WASCE What’s new in WebSphere Application Server Community Edition V2.1]
*[http://publib.boulder.ibm.com/wasce/V3.0.0/en/whats-new-in-ce-30.html What’s new in WebSphere Application Server Community Edition V3.0]

=== Presentations ===
*[http://parleys.com/display/PARLEYS/Apache+Geronimo+Unleashed?showComments=true Apache Geronimo Unleashed at javapolis 2006]
* [http://www-128.ibm.com/developerworks/forums/servlet/JiveServlet/download/541-215779-14114573-327254/Impact-2486-WAS-CE.pdf Impact 2008 IBM Websphere CE compared to Jboss]
*[https://web.archive.org/web/20080916165122/http://cwiki.apache.org/geronimo/presentations.html Presentations listed on Geronimo Wiki]

{{apache}}

[[Category:Apache Software Foundation|Geronimo]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>hplngz3bh5488zxng5wxhlgeq38ykb2</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Derby</title>
    <ns>0</ns>
    <id>2328658</id>
    <revision>
      <id>807884352</id>
      <parentid>804364896</parentid>
      <timestamp>2017-10-30T17:09:09Z</timestamp>
      <contributor>
        <ip>182.235.82.204</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6892">{{Infobox Software
| name                   = Apache Derby
| logo                   = [[File:Derby Logo.png|200px|The Apache Derby Project]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| author                 = Cloudscape Inc (Later IBM)
| latest release version = 10.14.1.0
| latest release date    = {{release date|2017|10|22}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Relational Database Management System]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://db.apache.org/derby/}}
}}

'''Apache Derby''' (previously distributed as '''IBM Cloudscape''') is a [[relational database management system]] (RDBMS) developed by the [[Apache Software Foundation]] that can be embedded in [[Java (programming language)|Java]] programs and used for [[online transaction processing]]. It has a 3.5 [[Megabyte|MB]] disk-space footprint.&lt;ref&gt;{{cite web | title=Apache Derby | publisher=Apache.org| url=http://db.apache.org/derby/}}&lt;/ref&gt;

Apache Derby is developed as an [[open source]] project under the [[Apache License|Apache 2.0 license]]. [[Oracle Corporation|Oracle]] distributes the same binaries under the name '''Java DB'''.&lt;ref&gt;[http://www.oracle.com/technetwork/java/javadb/overview/faqs-jsp-156714.html#1q2 Java DB - FAQs: "Is Java DB a fork of Apache Derby?"]&lt;/ref&gt;

==Derby technologies==
===Derby embedded database engine===
The core of the technology, Derby's database engine, is a full-functioned relational embedded database-engine, supporting [[JDBC]] and [[SQL]] as programming APIs. It uses [[IBM DB2]] [[SQL]] syntax.

===Derby Network Server===
The Derby network server increases the reach of the Derby database engine by providing traditional client server functionality. The network server allows clients to connect over TCP/IP using the standard [[DRDA]] protocol. The network server allows the Derby engine to support networked [[JDBC]], [[ODBC]]/[[Call Level Interface|CLI]], [[Perl]] and [[PHP]].

===Embedded Network Server===
An embedded database can be configured to act as a hybrid server/embedded RDBMS; to also accept TCP/IP connections from other clients in addition to clients in the same JVM.&lt;ref&gt;see Embedded Server Example in [http://db.apache.org/derby/docs/10.4/adminguide/ http://db.apache.org/derby/docs/10.4/adminguide/]&lt;/ref&gt;

===Database Utilities===
*ij: a tool that allows SQL scripts to be executed against any JDBC database.
*dblook: Schema extraction tool for a Derby database.
*sysinfo: Utility to display version numbers and class path.

==History==
Apache Derby originated at Cloudscape Inc, an [[Oakland, California|Oakland]], [[California]], start-up founded in 1996 by Nat Wyatt and Howard Torf to develop Java [[database]] technology. The first release of the database engine, then called JBMS, was in 1997. Subsequently, the product was renamed Cloudscape and releases were made about every six months.

In 1999 [[Informix]] Software, Inc., acquired Cloudscape, Inc. In 2001 [[IBM]] acquired the database assets of Informix Software, including Cloudscape. The database engine was re-branded to IBM Cloudscape and releases continued, mainly focusing on embedded use with IBM's Java products and middleware.

In August 2004 IBM contributed the code to the [[Apache Software Foundation]] as Derby, an incubator project sponsored by the [[Apache DB]] project.&lt;ref&gt;{{cite web | title=Why IBM is open sourcing Cloudscape as Derby | publisher= IBM | url=http://www.ibm.com/developerworks/data/library/techarticle/dm-0410prial/}}&lt;/ref&gt; In July 2005 the Derby project graduated from the Apache incubator and is now being developed as a sub-project of the [[Apache DB|DB]] Top Level Project at Apache. Prior to Derby's graduation from incubation, Sun joined the Derby project with an intent to use Derby as a component in their own products,&lt;ref&gt;{{cite web | title=Apache Derby graduates with Sun onboard | publisher= CNET news.com | url=http://news.cnet.com/Apache+Derby+graduates+with+Sun+onboard/2100-7344_3-5818473.html}}&lt;/ref&gt; and with the release of Java 6 in December 2006, Sun started packaging Derby in the [[Java Development Kit|JDK]] branded as Java DB.

In March 2007 IBM announced that they would withdraw marketing and support for the Cloudscape product, but would continue to contribute to the Apache Derby project.&lt;ref&gt;{{cite web | title=Changes in Cloudscape Availability and Support | publisher=IBM | url=http://www-1.ibm.com/support/docview.wss?rs=636&amp;uid=swg21256502}}&lt;/ref&gt;

The Java DB database is Oracle's supported distribution of Apache Derby.

==See also==
{{Portal|Free software}}
*[[List of relational database management systems]]
*[[Comparison of relational database management systems]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{Cite journal|format= |first1=Paul C. |last1=Zikopoulos |first2=George |last2=Baklarz |first3=Dan |last3=Scott |date=November 6, 2005 |title=Apache Derby—Off to the Races: Includes Details of IBM Cloudscape |edition=First |publisher=[[IBM Press]] |pages=600 |isbn=0-13-185525-5 |url=http://www.ibmpressbooks.com/bookstore/product.asp?isbn=0131855255 |postscript=&lt;!--None--&gt; |deadurl=yes |archiveurl=https://web.archive.org/web/20090215042408/http://www.ibmpressbooks.com/bookstore/product.asp?isbn=0131855255 |archivedate=February 15, 2009 |df= }} 
{{Refend}}

==External links==
*{{Official website}}
*[http://www-306.ibm.com/software/data/cloudscape/ IBM Cloudscape Site]
*[http://db.apache.org/derby/binaries/ApacheDerbyInternals_1_1.pdf Internals of Derby, An Open Source Pure Java Relational Database Engine] deployable in an embedded [[OSGi]] environment
*[http://www.oracle.com/technetwork/java/javadb/overview/index.html Oracle Java DB Site]
*[http://www-106.ibm.com/developerworks/db2/library/techarticle/dm-0410prial/ Why IBM is open sourcing Cloudscape as Derby], IBM developerWorks site
*[http://www.diva-portal.org/smash/get/diva2:347471/FULLTEXT01.pdf Apache Derby SMP scalability]
*[http://www.jpab.org/Derby.html Apache Derby performance results in the JPA Benchmark]
*[http://wiki.apache.org/db-derby/SQLvsDerbyFeatures Compliance matrix with SQL 2003]
*[https://www.beginnersheap.com/download-and-install-apache-derby/ Configure and Install Derby Database]
*[https://web.archive.org/web/20131004220227/http://www.mamstricks.com/2013/09/a-simple-java-program-with-derby.html Derby Database connection using netbeans with screenshots]

{{Apache}}

[[Category:Apache Software Foundation|Derby]]
[[Category:Free database management systems]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Software using the Apache license]]</text>
      <sha1>7vj9gobrd7rggz5ivyc230siiiy2osf</sha1>
    </revision>
  </page>
  <page>
    <title>Apache iBATIS</title>
    <ns>0</ns>
    <id>3946076</id>
    <revision>
      <id>831283704</id>
      <parentid>831283548</parentid>
      <timestamp>2018-03-19T19:59:55Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Edited Apache capital letter</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8526">{{Infobox Software
| name                   = Apache iBATIS
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Inactive (see [[MyBatis]])
| latest release version = 
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]], [[.NET Framework|.NET]] and [[Ruby (programming language)|Ruby]]
| genre                  = [[persistence framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://ibatis.apache.org}}
}}

'''iBATIS''' is a [[persistence framework]] which automates the mapping between [[SQL]] databases and objects in Java, .NET, and Ruby on Rails.  In Java, the objects are POJOs ([[Plain Old Java Object]]s).  The mappings are decoupled from the application logic by packaging the SQL statements in [[XML]] configuration files.  The result is a significant reduction in the amount of code that a developer needs to access a relational database using lower level APIs like [[JDBC]] and [[ODBC]].

Other persistence frameworks such as [[Hibernate (Java)|Hibernate]] allow the creation of an object model (in Java, say) by the user, and create and maintain the relational database automatically.  iBATIS takes the reverse approach: the developer starts with a SQL database and iBATIS automates the creation of the Java objects.  Both approaches have advantages, and iBATIS is a good choice when the developer does not have full control over the SQL database schema.  For example, an application may need to access an existing SQL database used by other software, or access a new database whose schema is not fully under the application developer's control, such as when a specialized database design team has created the schema and carefully optimized it for high performance.

On May 21, 2010 the development team forked the code creating a new project called [[MyBatis]] and making new releases there. As a consequence the Apache iBATIS project became inactive and was moved to the [[Apache Attic]] in June 2010.

==Usage==
For example, assume there is a database table {{mono|'''PRODUCT''' (PROD_ID ''INTEGER'', PROD_DESC ''VARCHAR(64)'')}} and a Java class {{mono|'''com.example.Product''' (id: ''int'', description: ''String'')}}.  To read the product record having the key {{mono|'''PROD_ID'''}} into a new {{mono|'''Product'''}} POJO, the following mapping is added into an iBATIS XML mapping file:

&lt;source lang="xml+velocity"&gt;
    &lt;select id="getProduct" parameterClass="java.lang.Long" resultClass="com.example.Product"&gt;
 	select PROD_ID as id,
               PROD_DESC as description
          from PRODUCT
         where PROD_ID = #value#
    &lt;/select&gt;
&lt;/source&gt;

A new Java '''Product''' object can then be retrieved from the database for product number 123 as follows:

&lt;source lang="java"&gt;
    Product resultProduct = (Product) sqlMapClient.queryForObject("getProduct", 123);
&lt;/source&gt;

In the mapping file example, &lt;code&gt;#value#&lt;/code&gt; refers to the long integer value passed into the query. If the parameter is a Java object, then values from properties on that object can be inserted into the query using a similar &lt;code&gt;#&lt;/code&gt; notation. For example, if the parameter class is a &lt;code&gt;com.example.Product&lt;/code&gt; which has a property called &lt;code&gt;id&lt;/code&gt;, then &lt;code&gt;#value#&lt;/code&gt; can be replaced with &lt;code&gt;#id#&lt;/code&gt;. The &lt;code&gt;sqlMapClient&lt;/code&gt; object is an instance of class &lt;code&gt;com.ibatis.sqlmap.client.SqlMapClient&lt;/code&gt;.

==Availability==
The founder of iBATIS has [https://web.archive.org/web/20110813174630/http://clintonbegin.blogspot.com/2008/02/clintons-java-5-rant.html publicly stated his dismay with Java 5], but has continued to release new versions of iBATIS for Java.  Versions 2.3.1 and 2.3.2 came out in April 2008, and 2.3.3 in July.

The framework is currently available in [[Java (programming language)|Java]], [[.NET Framework|.NET]], and [[Ruby (programming language)|Ruby]] (RBatis) versions. The [http://code.google.com/p/jbati/ jBati] project is a JavaScript [[Object-relational mapping|ORM]] inspired by iBATIS.

The Apache [[iBator]] tool is closely related: it connects to your database and uses its metadata to generate iBATIS mapping files and Java classes.

==History==
In 2001 a project called iBATIS was started by Clinton Begin. Originally the focus was on the development of cryptographic software solutions. The first product to be released by iBATIS was Secrets,&lt;ref&gt;[http://sourceforge.net/projects/ibatissecrets iBATIS Secrets]&lt;/ref&gt; a personal data encryption and signing tool much like PGP. Secrets was written entirely in Java and was released under an open source license.

That year [[Microsoft]] published a paper&lt;ref&gt;[http://onjava.com/pub/a/onjava/2001/11/28/catfight.html Cat Fight in a Pet Store: J2EE vs. .NET]&lt;/ref&gt; to demonstrate that its recent [[.NET Framework|.NET]] 1.0 framework was more productive than [[Java (programming language)|Java]]. For that purpose Microsoft built its own version of Sun's Web "Pet Store", a Web project that Sun had used to show Java best practices ([[Java BluePrints]]). [[Microsoft]] claimed that [[.NET Framework|.NET]] was 10 times faster and 4 times more productive than [[Java (programming language)|Java]].

In 2002 Clinton developed an application called JPetStore&lt;ref&gt;[http://www.clintonbegin.com/downloads/JPetStore-1-2-0.pdf JPetStore 1.0]&lt;/ref&gt; to demonstrate that [[Java (programming language)|Java]] could be more productive than [[.NET Framework|.NET]] and could also do so while achieving a better architecture than was used in the [[Microsoft]] implementation.

JPetStore 1.0 had a big impact&lt;ref&gt;[http://www.theserverside.com/news/thread.tss?thread_id=14243 JPetStore 1.0 announcement on TheServerside.com]&lt;/ref&gt; and the [[Database abstraction layer|database layer]] that Clinton used attracted the attention of the community. Soon, iBATIS Database Layer 1.0 project started, composed of two components: iBATIS DAO and iBATIS SQL Maps.

iBATIS 2.0 was released in June 2004.&lt;ref&gt;[http://www.theserverside.com/news/thread.tss?thread_id=26844 iBATIS 2.0 announcement]&lt;/ref&gt; It was a complete redesign while keeping the same features. Clinton donated the iBATIS name and code to [[Apache Software Foundation]] and the project stayed in the ASF for six years.

Eventually iBATIS DAO was deprecated, considering that better DAO frameworks were available, such as [[Spring Framework]].

On May 19, 2010 iBATIS 3.0 was published and simultaneously the development team decided to continue the development of the framework at [[Google Code]].&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/ibatis-user-java/201005.mbox/%3CAANLkTimXoLiHwI-3kbW6It7mH0771xJP4RqT609VKCXC@mail.gmail.com%3E iBATIS Project Team Moving to Google Code]&lt;/ref&gt; under a new project called [[MyBatis]].

On June 16, 2010 Apache announced that iBATIS was retired and moved to the Apache Attic.

==See also==
*[[MyBatis]]
*[[Java Persistence API]]
*[[Hibernate (framework)|Hibernate]]
*[[EclipseLink]]
*[[Apache Cayenne]]
*[[Spring Framework]]
*[[IBM PureQuery]]
*[[nHydrate]]
*[[OpenJPA]]
*[[ActiveJPA]]
*[http://www.orbroker.org O/R Broker], similar framework for [[Scala (programming language)|Scala]]

==References==
{{Reflist|colwidth=30em}}

==Bibliography==
{{refbegin}}
* {{cite book 
| last      = Begin
| first     = Clinton
| title     = iBATIS in Action
|author2=Brandon Goodin |author3=Larry Meadors
 | publisher = [[Manning Publications|Manning]] 
| edition   = 1st
| date      = January 17, 2007
| pages     = 384
| isbn      = 978-1-932394-82-5 
}}
* {{cite book 
| last      = Richardson
| first     = Chris
| title     = POJOs In Action 
| publisher = [[Manning Publications|Manning]] 
| edition   = 1st
| date      = January 23, 2006
| pages     = 456 
| isbn      = 1-932394-58-3
}}
{{refend}}

==External links==
*{{Official website}}
*[https://ibatis.apache.org/ibator The iBator project at Apache]
*[http://www.linksdaddy.com/blogs/show-blog/Integrating-iBatis-in-Spring-based-enterprise-applications Integrating iBatis in Spring based enterprise applications]: A simple step by step guide for using iBatis in Spring framework

{{Apache}}

[[Category:Apache Software Foundation|iBATIS]]
[[Category:Java (programming language)]]
[[Category:Object-relational mapping]]
[[Category:Persistence frameworks]]</text>
      <sha1>4d7u39mtdml53i2v60trfpl3i03ov4z</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Jackrabbit</title>
    <ns>0</ns>
    <id>2889426</id>
    <revision>
      <id>834186379</id>
      <parentid>832967015</parentid>
      <timestamp>2018-04-04T11:31:39Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <comment>/* Features */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3128">{{refimprove|date=February 2018}}
{{Infobox Software
| name                   = Apache Jackrabbit
| logo                   = [[File:Jackrabbitlogo.png|200px|Apache Jackrabbit Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.16.1
| latest release date    = {{Release date|2018|02|09}}&lt;ref&gt;https://jackrabbit.apache.org/jcr/downloads.html#v2.16&lt;/ref&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Content repository]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//jackrabbit.apache.org/}}
|repo={{URL|https://github.com/apache/jackrabbit}}}}
'''Apache Jackrabbit''' is an [[open source]] [[content repository]] for the [[Java platform]]. The Jackrabbit project was started on August 28, 2004, when [[Day Software]] licensed an initial implementation of the [[Content repository API for Java|Java Content Repository API (JCR)]]. Jackrabbit was also used as the [[reference implementation]] of [[JSR-170]], specified within the [[Java Community Process]]. The project graduated from the [[Apache Incubator]] on March 15, 2006, and is now a Top Level Project of the [[Apache Software Foundation]].

JCR specifies an API for application developers (and application frameworks) to use for interaction with modern content repositories that provide content services such as searching, versioning, transactions, etc.

==Features==
* Fine and coarse-grained content access
* Hierarchical content
* Structured content 
* Node types and mixins
* Property types - text, number, date
* Binary properties
* [[XPath]] queries
* [[SQL]] queries
* Unstructured content
* Import and export
* Referential integrity
* Access control
* Versioning
* JTA support
* Observation
* Locking
* Clustering
* Multiple persistence models

==See also==
{{Portal|Free software}}
* [[Apache Sling]] - a web framework for building applications on top of Apache Jackrabbit
* [[Hippo CMS]] - an Open Source content management system based on Apache Jackrabbit
* [[Jahia]] - Open Source [[Enterprise Content Management|ECM]] based on Apache Jackrabbit
* [[Magnolia (CMS)]] - an Open Source content management system based on Apache Jackrabbit
* [[OpenKM]] - Open Source [[Knowledge Management|KM]] based on Apache Jackrabbit
* [[Sakai Project]] - Open Source Collaboration and Learning Environment based on Apache Sling and Apache Jackrabbit

==References==
{{Reflist}}

==External links==
*[//jackrabbit.apache.org/ Jackrabbit's Home Page]
*[https://web.archive.org/web/20060411151108/http://www.apachenews.org/archives/000849.html Jackrabbit 1.0 released]
*[https://www.jcp.org/en/jsr/detail?id=170 JSR-170: Content Repository for Java(TM) Technology API]
*[https://www.jcp.org/en/jsr/detail?id=283 JSR-283: Content Repository for Java(TM) Technology API, version 2.0]
{{apache}}

[[Category:Apache Software Foundation|Jackrabbit]]
[[Category:Java enterprise platform]]
[[Category:Structured storage]]</text>
      <sha1>pd8aovnrlernbuliuk56q32lvqk3tu4</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Lenya</title>
    <ns>0</ns>
    <id>795725</id>
    <revision>
      <id>829390098</id>
      <parentid>722156091</parentid>
      <timestamp>2018-03-08T10:00:29Z</timestamp>
      <contributor>
        <username>Renamed user 2560613081</username>
        <id>16847332</id>
      </contributor>
      <comment>/* top */ Cleanup. Removed erroneous parameters from the infobox. For a list of supported parameters please consult [[Template:Infobox software/doc]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2557">{{multiple issues|
{{More footnotes|date=August 2008}}
{{refimprove|date=October 2014}}
{{notability|Products|date=October 2014}}
}}
{{Infobox software
| name                   = Apache Lenya
| logo                   = 
| screenshot             = &lt;!--  Commented out because image was deleted: [[Image:Apache-lenya-menubar.png|250px]] --&gt;
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Discontinued
| discontinued           = yes
| programming language   = [[Java (programming language)|Java]], [[XML]]
| platform               = [[Java SE]]
| genre                  = [[Content management system]]
| license                = [[Apache License 2.0]]
| website                = {{URL|https://lenya.apache.org}}
}}

'''Apache Lenya''' is a [[Java (programming language)|Java]]/[[XML]] [[open-source]] [[content management system]] based on the [[Apache Cocoon]] [[content management framework]].  Features include [[revision control]], scheduling, search capabilities, [[workflow]] support, and browser-based [[WYSIWYG]] editors.

Lenya was originally started by [[Michael Wechner]] in early 1999 to manage the content of the journal of pattern formation. Michael previously did basic research in physics by writing computer simulations on dendritic growth.

In early 2000 Michael co-founded [[Wyona]], which continued to develop Lenya on the basis of the interactive newspaper edition of [[Neue Zürcher Zeitung]]. The name Lenya is a combination of the names of his two sons Levi and Vanya.

In the spring of 2003, Wyona donated Lenya to the [[Apache Software Foundation]], where Lenya was incubated and became a Top Level Project in September 2004.

In 2006 Michael started a new [[Content management system|CMS]] called "Yanel" (an [[anagram]] of Lenya) which featured versioned interfaces to provide backwards compatibility at all times by continuous deployment and hence was an improvement on the classical approach of periodical releases.

The Lenya project was retired to the [[Apache Attic]] in April 2015.&lt;ref&gt;http://attic.apache.org/projects/lenya.html&lt;/ref&gt;

==See also==
{{Portal|Free software}}
*[[List of content management systems]]

==References==
{{Reflist}}

==External links==
*[https://lenya.apache.org Apache Lenya website]
*[https://wiki.apache.org/lenya/ Apache Lenya wiki]

{{Apache}}

[[Category:Free content management systems]]
[[Category:Apache Software Foundation|Lenya]]
[[Category:Free software programmed in Java (programming language)]]


{{cms-software-stub}}</text>
      <sha1>3ixuqkacjpa2gbtfx22uo4qw5nkv4j2</sha1>
    </revision>
  </page>
  <page>
    <title>Apache JServ Protocol</title>
    <ns>0</ns>
    <id>4980158</id>
    <revision>
      <id>810194333</id>
      <parentid>804567601</parentid>
      <timestamp>2017-11-13T21:34:19Z</timestamp>
      <contributor>
        <username>Luyseyal</username>
        <id>87490</id>
      </contributor>
      <comment>Document and cite request attributes, a major driver for using AJP</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6046">The '''Apache JServ Protocol''' ('''AJP''') is a [[binary protocol]] that can [[Proxy server|proxy]] inbound requests from a [[web server]] through to an [[application server]] that sits behind the web server.

It also supports some monitoring in that the web server can [[Ping (networking utility)|ping]] the application server. Web implementors typically use AJP in a [[Load balancing (computing)|load-balanced]] deployment where one or more front-end web servers feed requests into one or more application servers. Sessions are redirected to the correct application server using a routing mechanism wherein each application server instance gets a name (called a ''route''). In this scenario the web server functions as a [[reverse proxy]] for the application server. Lastly, AJP supports request attributes which, when populated with environment-specific settings in the reverse proxy, provides for secure communication between the reverse proxy and application server.&lt;ref&gt;{{cite web|title=NativeSPAttributeAccess|url=https://wiki.shibboleth.net/confluence/display/SHIB2/NativeSPAttributeAccess|website=Shibboleth Consortium|accessdate=13 November 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Apache Module mod_proxy_ajp|url=https://httpd.apache.org/docs/2.4/mod/mod_proxy_ajp.html|website=Apache HTTP Server Project|accessdate=13 November 2017}}&lt;/ref&gt;

AJP runs in [[Apache HTTP Server]] 1.x using the [[mod_jk]] [[Plug-in (computing)|plugin]] and in Apache 2.x using the provided Proxy AJP, [[mod_proxy]] and proxy balancer modules together. Implementations exist for the not-yet-released [[lighttpd]] version 1.5,&lt;ref&gt;{{cite web|url=http://redmine.lighttpd.net/projects/1/wiki/Docs_ModProxyCore|title=Docs ModProxyCore - Lighttpd - lighty labs|website=redmine.lighttpd.net|accessdate=9 October 2017}}&lt;/ref&gt; [[nginx]],&lt;ref&gt;{{cite web|url=https://github.com/yaoweibin/nginx_ajp_module|title=nginx_ajp_module: support AJP protocol proxy with Nginx|first=Weibin|last=Yao(姚伟斌)|date=6 October 2017|publisher=|accessdate=9 October 2017|via=GitHub}}&lt;/ref&gt; [[GlassFish|Grizzly]] 2.1,&lt;ref&gt;{{cite web | url=https://grizzly.java.net/nonav/docs/docbkx2.3/html/ajp.html | title=AJP | publisher=java.net | work=Grizzly 2.3 User's Guide | accessdate=2013-04-29}}&lt;/ref&gt; and the [[Internet Information Server]].&lt;ref&gt;{{cite web|url=http://boncode.net/connector/webdocs/Tomcat_Connector.htm|title=BonCode Apache Tomcat AJP 1.3 Connector|website=boncode.net|accessdate=9 October 2017}}&lt;/ref&gt;

The [[Apache Tomcat]] and [[WildFly|JBoss AS/WildFly]] [[Java servlet|servlet]] containers support AJP.

==History==
Alexei Kosut originally developed the Apache JServ Protocol in July 1997&lt;ref name="AJPv21"&gt;{{cite web |last1=Barbieri |first1=Federico |last2=Fumagalli |first2=Pierpaolo |last3=Kluft |first3=Ian |last4=Korthof |first4=Ed |last5=Mazzocchi |first5=Stefano |last6=Pool |first6=Martin |title=Apache JServ Protocol Version 2.1 |date=June 30, 1998 |work=Java Apache Project |url=http://java.apache.org/jserv/protocol/AJPv21.html |archiveurl=https://web.archive.org/web/20030804213216/http://java.apache.org/jserv/protocol/AJPv21.html |archivedate=2003-08-04}}&lt;/ref&gt; but the version 1.0 specification was published later on July 29, 1998.&lt;ref name="AJPv1"&gt;{{cite web |last1=Kosut |first1=Alexei |title=Apache JServ Protocol Version 1.0 |date=July 29, 1998 |work=Java Apache Project |url=http://java.apache.org/jserv/protocol/AJPv1.html |archiveurl=https://web.archive.org/web/20030415202643/http://java.apache.org/jserv/protocol/AJPv1.html |archivedate=2003-04-15}}&lt;/ref&gt; He also wrote the first implementations of it in the same month, with the releases of the Apache JServ Servlet Engine 0.9 and the Apache mod_jserv 0.9a (released on July 30, 1997).&lt;ref name="jservchanges"&gt;{{cite web |title=History of Changes - Apache JServ Project |work=Java Apache Project |url=http://java.apache.org/jserv/changes.html |archiveurl=https://web.archive.org/web/20030416024540/http://java.apache.org/jserv/changes.html |archivedate=2003-04-16}}&lt;/ref&gt;

The specification was updated to version 1.1 on September 9, 1998.&lt;ref name="AJPv11"&gt;{{cite web |last1=Kosut |first1=Alexei |title=Apache JServ Protocol Version 1.1 |date=September 9, 1998 |work=Java Apache Project |url=http://java.apache.org/jserv/protocol/AJPv11.html |archiveurl=https://web.archive.org/web/20030804080332/http://java.apache.org/jserv/protocol/AJPv11.html |archivedate=2003-08-04}}&lt;/ref&gt; Also in 1998, a revamped protocol was created and published in specification versions 2&lt;ref name="AJPv2"&gt;{{cite web |last1=Kluft |first1=Ian |last2=Korthof |first2=Ed |last3=Mazzocchi |first3=Stefano |title=Apache JServ Protocol Version 2 |date=February 15, 1998 |work=Java Apache Project |url=http://java.apache.org/jserv/protocol/AJPv2.html |archiveurl=https://web.archive.org/web/20030805035548/http://java.apache.org/jserv/protocol/AJPv2.html |archivedate=2003-08-05}}&lt;/ref&gt; and 2.1,&lt;ref name="AJPv21"/&gt; however it was never adopted.

The current specification remains at version 1.3,&lt;ref name="AJPv13a"&gt;{{cite web |title=AJP Protocol Reference - AJPv13 |work=Apache Tomcat |url=https://tomcat.apache.org/connectors-doc/ajp/ajpv13a.html |accessdate=2016-08-20}}&lt;/ref&gt; however there is a published extension proposal.&lt;ref name="AJPv13ext"&gt;{{cite web |title=AJP Protocol Reference - AJPv13 Extension Proposal |work=Apache Tomcat |url=https://tomcat.apache.org/connectors-doc/ajp/ajpv13ext.html |accessdate=2016-08-20}}&lt;/ref&gt;

==See also==
*[[Web Services for Remote Portlets]]

==References==
{{Reflist}}

==External links==
* [http://tomcat.apache.org/connectors-doc/ajp/ajpv13a.html The Apache Tomcat Connector - AJP Protocol Reference] AJPv13
* [http://tomcat.apache.org/tomcat-3.3-doc/AJPv13.html Apache JServ Protocol version 1.3] Dan Milstein, December 2000.
*{{cite web | title=BonCode Connector | website=BonCode | date=2016-08-16 | url=http://www.boncode.net/boncode-connector | access-date=2017-10-09}} BonCode IIS implementation of AJP

{{Web interfaces}}

[[Category:Apache Software Foundation|JServ]]</text>
      <sha1>qj7iitytle672130a31ke977dw4ltz1</sha1>
    </revision>
  </page>
  <page>
    <title>ApacheBench</title>
    <ns>0</ns>
    <id>3348493</id>
    <revision>
      <id>713747874</id>
      <parentid>654194347</parentid>
      <timestamp>2016-04-05T17:35:09Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2775">{{notability|Products|date=November 2014}}
{{ref improve|date=November 2014}}
'''ApacheBench''' (&lt;tt&gt;ab&lt;/tt&gt;) is a single-threaded command line computer program for [[Web server benchmarking|measuring the performance]] of [[HyperText Transfer Protocol|HTTP]] [[web server]]s.&lt;REF&gt;{{cite web| url=http://httpd.apache.org/docs/2.2/en/programs/ab.html| title=ab - Apache HTTP server benchmarking tool| website=Apache| accessdate=9 October 2014}}&lt;/REF&gt;  Originally designed to test the [[Apache HTTP Server]], it is generic enough to test any web server.

The &lt;tt&gt;ab&lt;/tt&gt; tool comes bundled with the standard Apache source distribution, and like the Apache web server itself, is free, [[open source]] software and distributed under the terms of the [[Apache License]].

==Example usage==
&lt;syntaxhighlight lang="bash"&gt;
ab -n 100 -c 10 "http://en.wikipedia.org/wiki/Main_Page"
&lt;/syntaxhighlight&gt;

This will execute 100 [[HTTP GET]] requests, processing up to 10 requests concurrently, to the specified URL, in this example, "&lt;nowiki&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/nowiki&gt;".&lt;ref&gt;{{cite web| url=http://www.petefreitag.com/item/689.cfm| title=Using Apache Bench for Simple Load Testing| website=Pete Freitag's ColdFusion, Java and Web Development Blog| accessdate=9 October 2014}}&lt;/ref&gt;

==Concurrency versus Threads==
Note that ApacheBench will only use one operating system thread regardless of the concurrency level (specified by the &lt;tt&gt;-c&lt;/tt&gt; parameter).  In some cases, especially when benchmarking high-capacity servers, a single instance of ApacheBench can itself be a bottleneck.  When using ApacheBench on hardware with multiple processor cores, additional instances of ApacheBench may be used in parallel to more fully saturate the target URL.

==Detecting ApacheBench==
The ApacheBench [[User agents|User Agent]] string is the following:
&lt;tt&gt;ApacheBench/MAJOR.MINOR&lt;/tt&gt;

where MAJOR and MINOR represent the major and minor version numbers of the program.&lt;REF&gt;{{cite web| url=http://user-agent-string.info/list-of-ua/browser-detail?browser=AB%20%28Apache%20Bench%29| title=Useragent detail: AB (Apache Bench)| website=User-Agent-String-Info| accessdate=9 October 2014}}&lt;/REF&gt;  It is usually not correctly categorised by web server log analysers such as [[Webalizer]] or [[AWStats]], so running ApacheBench with a great number of requests may skew the results of the reports generated by these programs.

==See also==
{{Portal|Software Testing}}
* [[Web server benchmarking]]

==References==
{{Reflist}}

==External links==
* [http://httpd.apache.org/docs/2.4/programs/ab.html Manual page for the 'ab' tool]
* [http://httpd.apache.org Apache HTTPD official website]

[[Category:Apache Software Foundation]]
[[Category:Load testing tools]]


{{web-software-stub}}</text>
      <sha1>hq3sz8wp1czyum0287gbk6e5fga63q1</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Axis</title>
    <ns>0</ns>
    <id>1888026</id>
    <revision>
      <id>785200420</id>
      <parentid>762474756</parentid>
      <timestamp>2017-06-12T05:24:21Z</timestamp>
      <contributor>
        <username>Kiwi128</username>
        <id>12235997</id>
      </contributor>
      <comment>update URLs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5376">{{Infobox software
| name                   = Apache Axis
| logo                   = [[File:Apache Axis Logo.jpg|150px|Apache Axis Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 1.4
| latest release date    = {{release date|2006|04|22}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]] and [[C++]]
| genre                  = [[Web service]]
| license                = [[Apache License]] 2.0
| website                = {{url|//axis.apache.org/}}
}}

'''Apache Axis''' ('''A'''pache e'''X'''tensible '''I'''nteraction '''S'''ystem) is an [[open-source]], [[XML]] based [[Web service]] framework. It consists of a [[Java (programming language)|Java]] and a [[C++]] implementation of the [[SOAP (protocol)|SOAP]] server, and various utilities and [[API]]s for generating and deploying [[WWW|Web]] service applications. Using Apache Axis, developers can create interoperable, distributed computing applications. Axis development takes place under the auspices of the [[Apache Software Foundation]].

==Axis for Java==
When using the Java version of Axis there are two ways to expose Java code as Web service. The easiest one is to use Axis native JWS (Java Web Service) files.
Another way is to use custom deployment. Custom deployment enables you to customize resources that should be exposed as Web services.

See also [[Apache Axis2]].

===JWS Web service creation===
JWS files contain Java class source code that should be exposed as Web service. The main difference between an ordinary java file and jws file is the file extension. Another difference is that jws files are deployed as [[source code]] and not compiled [[class file]]s.

The following example is taken from http://axis.apache.org/axis/java/user-guide.html#Publishing_Web_Services_with_Axis .
It will expose methods ''add'' and ''subtract'' of class Calculator.
&lt;source lang="java"&gt;
 public class Calculator 
 {
   public int add(int i1, int i2) 
   {
     return i1 + i2; 
   }
 
   public int subtract(int i1, int i2) 
   {
     return i1 - i2;
   }
 }
&lt;/source&gt;

====JWS Web service deployment====
Once the Axis servlet is deployed, you need only to copy the jws file to the Axis directory on the server. This will work if you are using an
[[Apache Tomcat]] container. In the case that you are using another web container, custom [[WAR (Sun file format)|WAR]] archive creation will be required  .

====JWS Web service access====
JWS Web service is accessible using the URL &lt;nowiki&gt;http://localhost:8080/axis/Calculator.jws&lt;/nowiki&gt; . If you are running a custom configuration of [[Apache Tomcat]] or a different container, the URL might be different.

===Custom deployed Web service===
Custom Web service [[Software deployment|deployment]] requires a specific deployment descriptor called WSDD (Web Service Deployment Descriptor) syntax. It can be used to specify resources that should be exposed as Web services. Current version (1.3) supports
* [[Remote procedure call|RPC]] services
* EJB - stateless ([[Enterprise Java Bean]])

====Automated generation of WSDL====
When a Web service is exposed using Axis it will generate a [[Web Services Description Language|WSDL]] file automatically when accessing the Web service URL with ''?WSDL'' appended to it.

==Axis for C++==
An example for implementing and deploying a simple web-service with the C++ version of Axis can be found in the Axis-CPP Tutorial (link in the Reference section below).

The steps necessary are:
* Create the wsdl file
* Generate client and server stubs using wsdl2ws
* Provide the server side web service implementation (e.g. the add method of the calculator service)
* Build the server-side code and update the generated deploy.wsdd with the .dll path
* Deploy the binaries to the directory specified in the wsdd
* Build client
* Run and enjoy...

For more information on the individual steps go directly to the tutorial.

==Related technologies==
* [[Apache Axis2]] - re-design/write of Axis
* [[Java Web Services Development Pack]] - web services framework
* [[Apache CXF]] - other Apache web services framework (old [[Codehaus XFire|XFire]] &amp; [[Celtix]])
* [[XML Interface for Network Services]] - RPC/web services framework
* [[Web Services Invocation Framework]] - Java API for invoking Web services
* [[webMethods Glue]] - commercial web services enabling product
* AlchemySOAP - open source C++ web services framework

==See also==
{{Portal|Java}}
&lt;!-- formatting; please do not remove until some more text lines are added to compensate spacing --&gt;
*[[Apache Axis2]]

==External links==
* [//axis.apache.org/ Apache Axis Homepage] at the Apache Software Foundation
* [//axis.apache.org/axis/cpp/ Apache Axis C++ Homepage] at the Apache Software Foundation
* [//axis.apache.org/axis/cpp/arch/End-2-End-Sample.html Axis-C++ tutorial] at the Apache Software Foundation
* [//axis.apache.org/axis2/ Apache Axis2/Java] at the Apache Software Foundation
* [//axis.apache.org/axis2/c/ Apache Axis2/C] at the Apache Software Foundation
* [https://www.stylusstudio.com/open_ws_framework.html Stylus Studio Tools for Apache Axis], see also Stylus Studio

{{Apache}}

[[Category:Apache Software Foundation|Axis]]
[[Category:Web services]]
[[Category:Web service specifications]]</text>
      <sha1>oizpvfb8o46wlp0zrxaklqhr4eqw0lu</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Batik</title>
    <ns>0</ns>
    <id>5145427</id>
    <revision>
      <id>845167190</id>
      <parentid>840097321</parentid>
      <timestamp>2018-06-09T21:59:51Z</timestamp>
      <contributor>
        <ip>82.53.101.172</ip>
      </contributor>
      <comment>fix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4219">{{Infobox software
| name                   = Batik
| logo                   = Batik (software) logo.png
| logo size              = 190px
| screenshot             = Batik Screenshot.png
| screenshot size        = 250px
| caption                = Batik running Solitaire Sample
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = 1.9
| latest release date    = {{Start date and age|2017|04|10}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| language               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[SVG|Scalable Vector Graphics (SVG)]]
| license                = [[Apache License]] 2.0
| website                = {{URL|xmlgraphics.apache.org/batik}}
}}

'''Batik''' is a pure-[[Java (programming language)|Java]] library that can be used to render, generate, and manipulate [[Scalable Vector Graphics|SVG]] graphics (SVG is an [[XML]] markup language for describing two-dimensional [[vector graphics]]). IBM supported the project and then donated the code to the [[Apache Software Foundation]], where other companies and teams decided to join efforts.
Batik provides a set of core modules that provide functionality to:

* Render and dynamically modify SVG content,
* Transcode SVG content to some raster [[Graphics file format]]s, such as [[Portable Network Graphics|PNG]], [[JPEG]] and [[TIFF]],
* Transcode [[Windows Metafile]]s to SVG (WMF or Windows Metafile Format is the vector format used by [[Microsoft Windows]] applications),
* And manage scripting and user events on SVG documents.

The Batik distribution also contains a ready-to-use SVG browser (called Squiggle) making use of the above modules.

The name of the library comes from the [[Batik|Batik painting technique]].

==Status==
Batik was long the most conformant existing [http://www.w3.org/TR/SVG11/ SVG 1.1] implementation&lt;ref&gt;[http://xmlgraphics.apache.org/batik/status.html Batik 1.7 Status]&lt;/ref&gt;&lt;ref&gt;{{cite web
|url=http://codedread.com/svg-support.php
|title=Welcome To CodeDread 1.1
|last=Schiller
|first=Jeff
|date=2009-01-18
|accessdate=2009-02-08
| archiveurl= https://web.archive.org/web/20090216041528/http://www.codedread.com/svg-support.php| archivedate= 16 February 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;&lt;ref&gt;almost 94% of the official SVG tests are OK with the 1.7 version.&lt;/ref&gt; and {{as of | 2011 | lc = on }} is just a small fraction behind [[Opera (web browser)|Opera]].{{Citation needed|date=April 2011}}

The latest 1.7 version, made available on January 10, 2008, has an "almost full" implementation of the current state of the [[sXBL]] specification,&lt;ref&gt;[http://www.apache.org/dist/xmlgraphics/batik/README.txt Batik 1.7 Readme] {{webarchive |url=https://web.archive.org/web/20080409125426/http://www.apache.org/dist/xmlgraphics/batik/README.txt |date=April 9, 2008 }}&lt;/ref&gt; a nearly complete implementation of SVG [http://www.w3.org/TR/SVG11/animate.html#AnimateElementsIntro declarative animation] [[Synchronized Multimedia Integration Language|SMIL]] features, and some of the [http://www.w3.org/TR/2004/WD-SVG12-20041027/ SVG 1.2] late October 2004 working draft (see [[Scalable Vector Graphics#Development history|SVG's Development history]]).

==See also==
{{Portal|Free software}}
*[[Scalable Vector Graphics]]
*[[Synchronized Multimedia Integration Language]]
*[[sXBL]]: a mechanism for defining the presentation and interactive behavior of elements described in a namespace other than SVG files
*[[Comparison of layout engines (SVG)]]

==References==
{{Reflist|2}}

==External links==
*[http://xmlgraphics.apache.org/batik/ Apache Batik Project]
*[http://wiki.apache.org/xmlgraphics-batik/SupportedSVG12Features Current status of Batik's sXBL implementation]
*[http://www.w3.org/Graphics/SVG/ The official SVG page at W3C], SVG Working Group

{{Apache}}
{{SVG Plugins}}

[[Category:Apache Software Foundation|Batik]]
[[Category:Graphics libraries]]
[[Category:Java platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Scalable Vector Graphics]]
[[Category:Java (programming language) libraries]]</text>
      <sha1>ipdw8l9amejyrdc0epiohvnup8wbq6y</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Incubator</title>
    <ns>0</ns>
    <id>1641276</id>
    <revision>
      <id>677419597</id>
      <parentid>610566243</parentid>
      <timestamp>2015-08-23T04:12:36Z</timestamp>
      <contributor>
        <username>Koavf</username>
        <id>205121</id>
      </contributor>
      <comment>added [[Category:Internet properties established in 2002]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1501">{{Refimprove|date=February 2012}}

'''Apache Incubator''' is the gateway for [[open-source]] projects intended to become fully fledged [[Apache Software Foundation]] projects.

The Incubator project was created in October 2002 to provide an entry path to the Apache Software Foundation for projects and codebases wishing to become part of the Foundation's efforts. All code donations from external organizations and existing external projects wishing to move to Apache must enter through the Incubator.

The Apache Incubator project serves on the one hand as a temporary container project until the incubating project is accepted and becomes a top-level project of the [[Apache Software Foundation]] or becomes subproject of a proper project such as the [[Jakarta Project]] or [[Apache XML]]. On the other hand, the Incubator project documents how the Foundation works, and how to get things done within its framework. This means documenting process, roles and policies within the [[Apache Software Foundation]] and its member projects.


==External links==
* [http://incubator.apache.org/ Apache Incubator]
** [http://incubator.apache.org/projects/ All projects in incubator]
* [http://wiki.apache.org/incubator/OpenOfficeProposal Apache Incubator OpenOfficeProposal]
* [http://qnalist.com/g/hcatalog HCatalog Mailing List Archives]

{{apache}}

&lt;!--Interwikies--&gt;

[[Category:Apache Software Foundation|Incubator]]
[[Category:Computing websites]]
[[Category:Internet properties established in 2002]]</text>
      <sha1>49x48i6dc3bwc7unzfj5eb88441ldiw</sha1>
    </revision>
  </page>
  <page>
    <title>Apache POI</title>
    <ns>0</ns>
    <id>169055</id>
    <revision>
      <id>801120642</id>
      <parentid>801120396</parentid>
      <timestamp>2017-09-17T19:56:10Z</timestamp>
      <contributor>
        <username>Hervegirod</username>
        <id>1430229</id>
      </contributor>
      <comment>/* Version history */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8708">{{Infobox software
| name                   = Apache POI
| logo                   = [[Image:Jakarta POI Logo.gif|170px|Jakarta POI Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 3.17
| latest release date    = {{release date and age|2017|09|15}}&lt;ref&gt;{{cite web |url=https://mail-archives.apache.org/mod_mbox/poi-user/201709.mbox/%3C7c7bc413-5f87-0cea-16e1-043b2fd3b80a%40apache.org%3E |title=[ANNOUNCE] Apache POI 3.17 released |date=2017-09-15 |accessdate=2017-09-17}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| programming language   =
| operating system       = [[Cross-platform]]
| genre                  = [[API]] to access [[Microsoft Office]] [[file format|formats]]
| license                = [[Apache License]] 2.0
| website                = https://poi.apache.org
}}
'''Apache POI''', a project run by the [[Apache Software Foundation]], and previously a sub-project of the [[Jakarta Project]], provides pure [[Java platform|Java]] libraries for reading and writing files in [[Microsoft Office]] [[file format|formats]], such as [[Microsoft Word|Word]], [[Microsoft PowerPoint|PowerPoint]] and [[Microsoft Excel|Excel]].

==History and roadmap==
The name was originally an [[acronym]] for "Poor Obfuscation Implementation",&lt;ref&gt;{{citation | url = http://www.javaworld.com/javaworld/jw-03-2004/jw-0322-poi.html | title = Excelling in Excel with Java | first = Elango | last = Sundaram | publisher = Java World | date = 2004-03-22}}&lt;/ref&gt; referring humorously to the fact that the file formats seemed to be deliberately [[Obfuscated code|obfuscated]], but poorly, since they were successfully [[reverse engineering|reverse-engineered]]. This explanation – and those of the similar names for the various sub-projects – were removed from the official web pages in order to better market the tools to businesses who would not consider such humor appropriate.  The original authors ([[Andrew C. Oliver]] and Marc Johnson) also noted the existence of the Hawaiian [[poi (food)|poi]] dish, made of mashed [[taro root]], which had similarly derogatory connotations.&lt;ref&gt;{{citation | url = http://www.coyotesong.com/poi/ | archiveurl = https://web.archive.org/web/20041015142404/http://www.coyotesong.com/poi/ | archivedate = 2004-10-15 | title = POI homepage from October 2004 | publisher = Coyote Song}}, showing original explanations for naming.&lt;/ref&gt;

===Office Open XML support===
POI supports the ISO/IEC 29500:2008 [[Office Open XML]] file formats since version 3.5.  A significant contribution for OOXML support came from [[Sourcesense]],&lt;ref&gt;{{citation | url = http://www.sourcesense.com/ | title = SourceSense}}&lt;/ref&gt; an [[open source]] company which was commissioned by [[Microsoft]] to develop this contribution.&lt;ref&gt;{{cite web
| url=http://www.informationweek.com/news/windows/showArticle.jhtml?articleID=206905858
| title= Microsoft Eyes Open Source Components for Office 2007
| publisher= Information Week
| date=26 March 2008
| accessdate=1 March 2009}}&lt;/ref&gt;  This link spurred controversy, some POI contributors questioning POI OOXML patent protection regarding Microsoft's [[Open Specification Promise]] patent license.&lt;ref&gt;{{citation | url = http://mail-archives.apache.org/mod_mbox/poi-dev/200803.mbox/%3c47EAE71C.6040603@buni.org%3e | title = POI development mailing list archives | date = March 2008}}&lt;/ref&gt;

==Architecture==
The Apache POI project contains the following subcomponents (meaning of acronyms is taken from old documentation):

* POIFS (Poor Obfuscation Implementation File System) – This component reads and writes [[Microsoft]]'s [[OLE 2]] [[COM Structured storage|Compound document]] format.  Since all [[Microsoft Office]] files are [[OLE 2]] files, this component is the basic building block of all the other POI elements. POIFS can therefore be used to read a wider variety of files, beyond those whose explicit decoders are already written in POI.
* HSSF (Horrible SpreadSheet Format) – reads and writes [[Microsoft Excel]] (XLS) format files. It can read files written by  [[Microsoft Excel|Excel]] 97 onwards; this [[file format]] is known as the ''BIFF 8'' format. As the Excel file format is complex and contains a number of tricky characteristics, some of the more advanced features cannot be read.
* XSSF (XML SpreadSheet Format) – reads and writes [[Office Open XML]] (XLSX) format files.  Similar feature set to HSSF, but for Office Open XML files.
* HPSF (Horrible Property Set Format) – reads "Document Summary" information from [[Microsoft Office]] files. This is essentially the information that one can see by using the ''File|Properties'' menu  item within an [[Microsoft Office|Office]] application.
* HWPF (Horrible Word Processor Format) – aims to read and write [[Microsoft Word|Microsoft Word 97]] (DOC) format files. This component is in initial stages of development.
* XWPF (XML Word Processor Format) – similar feature set to HWPF, but for Office Open XML files.
* HSLF (Horrible Slide Layout Format) – a pure Java implementation for [[Microsoft PowerPoint]] files. This provides the ability to read, create and edit presentations (though some things are easier to do than others)
* HDGF (Horrible DiaGram Format) – an initial pure Java implementation for [[Microsoft Visio]] binary files. It provides an ability to read the low level contents of the files.
* HPBF (Horrible PuBlisher Format) – a pure Java implementation for Microsoft Publisher files.
* HSMF (Horrible Stupid Mail Format&lt;ref&gt;{{citation | url = http://npoi.codeplex.com/discussions/233518 | publisher = Microsoft | title = Codeplex NPOI}}&lt;/ref&gt;{{Better source|date=January 2012}}) – a pure Java implementation for Microsoft Outlook MSG files.&lt;ref&gt;{{citation | title = POI-HSMF | publisher = Apache | url = http://poi.apache.org/hsmf/}}&lt;/ref&gt;
* DDF (Dreadful Drawing Format) – a package for decoding the Microsoft Office Drawing format.

The HSSF component is the most advanced feature of the library.&lt;ref&gt;{{citation | url = http://poi.apache.org/hssf/ | title = POI-HSSF | publisher = Apache}}&lt;/ref&gt;  Other components (HPSF, HWPF, and HSLF) are usable, but less full-featured.&lt;ref&gt;{{citation | url = http://poi.apache.org/hwpf/ | title = POI-HWPF | publisher = Apache}}&lt;/ref&gt;&lt;ref&gt;{{citation | url = http://poi.apache.org/hslf/ | title = POI-HSLF | publisher = Apache}}&lt;/ref&gt;

The POI library is also provided as a [[Ruby (programming language)|Ruby]]&lt;ref&gt;{{citation | title = POI-Ruby | publisher = Apache | url = http://poi.apache.org/poi-ruby.html}}&lt;/ref&gt; or [[ColdFusion]] extension.

== Version history ==
{{Version|t|show=11101}}

{| class="wikitable"
|- class="hintergrundfarbe5"
! 
Version number
! Date of release
|-
| {{Version |c |3.17}}
| 15. September 2017
|-
| {{Version |co |3.16}}
| 19. April 2017
|-
| {{Version |co |3.15}}
| 21. September 2016
|-
| {{Version |co |3.14}}
| 2. March 2016
|-
| {{Version |co |3.13}}
| 29. September 2015
|-
| {{Version |co |3.12}}
| 11. May 2015
|-
| {{Version |co |3.11}}
| 21. December 2014
|-
| {{Version |co |3.10.1}}
| 18. August 2014
|-
| {{Version |o |3.10}}
| 8. February 2014
|-
| {{Version |o |3.9}}
| 3. December 2012
|-
| {{Version |o |3.8}}
| 26. March 2012
|-
| {{Version |o |3.7}}
| 29. October 2010
|-
| {{Version |o |3.6}}
| 14. December 2009
|-
| {{Version |o |3.5}}
| 28. September 2009
|-
| {{Version |o |3.2}}
| 19. October 2008
|-
| {{Version |o |3.1}}
| 29. June 2008
|-
| {{Version |o |3.0.2}}
| 4. February 2008
|-
| {{Version |o |3.0.1}}
| 5. July 2007
|-
| {{Version |o |3.0}}
| 18. May 2007
|-
| {{Version |o |2.5.1}}
| 29. February 2004
|-
| {{Version |o |2.5}}
| 29. February 2004
|-
| {{Version |o |2.0}}
| 26. January 2004
|-
| {{Version |o |1.5.1}}
| 16. June 2002
|-
| {{Version |o |1.5}}
| 6. May 2002
|-
| {{Version |o |1.2.0}}
| 19. January 2002
|-
| {{Version |o |1.1.0}}
| 4. January 2002
|-
| {{Version |o |1.0.2}}
| 11. January 2002
|-
| {{Version |o |1.0.1}}
| 4. January 2002
|-
| {{Version |o |1.0.0}}
| 30. December 2001
|}

==See also==
*[[Open Packaging Conventions]]
*[[Office Open XML software]]

==References==
{{Reflist|2}}

==External links==
* {{citation | url = https://poi.apache.org/ | title = Apache POI}} – the official Apache POI project page.
* {{citation | url = http://oscon2006.sourceforge.net/ | title = POI presentation at OSCON2006 | publisher = Source Forge}}.

{{Apache}}

{{DEFAULTSORT:Apache Poi}}
[[Category:Apache Software Foundation|POI]]
[[Category:Microsoft Office-related software]]
[[Category:Java platform]]
[[Category:Java (programming language) libraries]]
[[Category:Cross-platform free software]]</text>
      <sha1>8s6lbq6yqoi094fb8568yhgzo74a6t2</sha1>
    </revision>
  </page>
  <page>
    <title>Jakarta Slide</title>
    <ns>0</ns>
    <id>332045</id>
    <revision>
      <id>710358373</id>
      <parentid>670362327</parentid>
      <timestamp>2016-03-16T14:03:44Z</timestamp>
      <contributor>
        <username>Hebalers</username>
        <id>27789855</id>
      </contributor>
      <minor/>
      <comment>minot sentence style improvement.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2145">{{Primary sources|date=August 2008}}
{{Infobox software
| name                   = Jakarta Slide
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version =  5.0 M4
| latest release date    = {{release date|2007|12|19}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Content management system]]
| license                = [[Apache License]] 2.0
| website                = http://jakarta.apache.org/slide
}}
'''Jakarta Slide''' is an open-source [[content management system]] from the [[Jakarta Project|Jakarta]] project. It is written in [[Java (programming language)|Java]] and implements the [[WebDAV]] protocol. Slide is a set of APIs to implement the WebDAV client. Thanks to that, Slide can also be seen as a Content Management Framework. The use of WebDAV, which is a superset of [[HTTP]], makes Slide an ideal candidate for web-based content management. Among the applications of Slide are its use as a file server, in intranet applications, and as an excellent repository for XML both as properties and versioned files for persistence of [[JavaBeans]]. It also has an extensible storage mechanism that can be used for Integration and adaptation.

The Apache Jakarta PMC has announced the retirement
of the Jakarta Slide subproject at 2007-11-03. An alternative implementation that is actively maintained is the WebDAV component of the [[Apache Jackrabbit]] project that provides Java-based content repository software.

==External links==
{{Portal|Free software}}
*[http://jakarta.apache.org/slide/ Official Slide website at apache.org]
*[http://jackrabbit.apache.org/doc/components/webdav.html Apache Jackrabbit library at apache.org]
{{Apache}}

{{DEFAULTSORT:Slide}}
[[Category:Free content management systems]]
[[Category:Apache Software Foundation]]
[[Category:Java platform software]]
[[Category:Cross-platform software]]

{{cms-software-stub}}</text>
      <sha1>pkiwxbjpb4uiazvw2v2vgk5cebc4cnc</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Tapestry</title>
    <ns>0</ns>
    <id>2052309</id>
    <revision>
      <id>830728568</id>
      <parentid>830629485</parentid>
      <timestamp>2018-03-16T16:04:56Z</timestamp>
      <contributor>
        <username>BarrelProof</username>
        <id>13560851</id>
      </contributor>
      <comment>When was it created?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16354">{{Infobox software
| name                   = Apache Tapestry
| logo                   = Tapestry.png
| screenshot             = 
| caption                = "Tapestry 5: Code Less, Deliver More"
| author                 = Howard Lewis Ship
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 5.4.2
| latest release date    = {{release date|2017|04|13}}
| operating system       = [[Cross-platform]] ([[Java Virtual Machine]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web application framework|Web Framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://tapestry.apache.org/}}
}}
'''Apache Tapestry''' is an [[open-source]] component-oriented [[Java (programming language)|Java]] [[web application framework]] conceptually similar to [[JavaServer Faces]] and [[Apache Wicket]].&lt;ref&gt;{{cite web|url=http://devrates.com/post/show/345948/howard-lewis-ship-of-tapestry-interview-%5Bpart-1%5D|title=Howard Lewis Ship of Tapestry interview [part 1] (2012-10-22)|accessdate=2013-01-28 }}&lt;/ref&gt; Tapestry was created by Howard Lewis Ship,{{when|date=March 2018}} and was adopted by the [[Apache Software Foundation]] as a top-level project in 2006.&lt;ref&gt;Drobiazko 2012, p. 1.&lt;/ref&gt;

Tapestry emphasizes simplicity, ease of use, and developer productivity. It adheres to the [[Convention over Configuration]] paradigm, eliminating almost all XML configuration.&lt;ref&gt;http://tapestryjava.blogspot.com/2006/07/tapestry-5-updates.html&lt;/ref&gt; Tapestry uses a modular approach to web development, by having a strong [[UI data binding|binding]] between each [[user interface]] component (object) on the web page and its corresponding [[Java (programming language)|Java]] class. This component-based architecture borrows many ideas from [[WebObjects]].&lt;ref&gt;Tapestry in Action - Preface by Howard Lewis Ship&lt;/ref&gt;

== Notable Features ==

; Live Class Reloading: Tapestry monitors the file system for changes to Java page classes, component classes, service implementation classes, HTML templates and component property files, and it hot-swaps the changes into the running application without requiring a restart. This provides a very short code-save-view feedback cycle that is claimed to greatly improve developer productivity.&lt;ref&gt;http://tapestry.apache.org/class-reloading.html&lt;/ref&gt;
; Component-based : Pages may be constructed with small nestable components, each having a template and a component class. Custom components are purportedly trivial to construct.&lt;ref&gt;Drobiazko 2012, p. 20.&lt;/ref&gt;
; Convention over configuration: Tapestry uses naming conventions and annotations, rather than XML, to configure the application.&lt;ref&gt;Drobiazko 2012, p. 7.&lt;/ref&gt;
; Spare use of HTTPSession : By making minimal use of the HTTPSession, Tapestry is designed to be highly efficient in a clustered, session-replicated environment.&lt;ref&gt;http://tapestry.apache.org/performance-and-clustering.html&lt;/ref&gt;
; Post/Redirect/Get: Most form submissions follow the [[Post/Redirect/Get]] (PRG) pattern, which reduces multiple form submission accidents and makes URLs friendlier and more bookmarkable, along with enabling the browser Back and Refresh buttons to operate normally.&lt;ref&gt;http://tapestry.apache.org/forms-and-validation.html&lt;/ref&gt;
; Inversion of Control (IoC): Tapestry is built on a lightweight [[Inversion of Control]] layer with similarities to [[Google Guice]], but designed to make nearly all aspects of Tapestry's behavior configurable and replaceable.&lt;ref&gt;Drobiazko 2012, p. 7.&lt;/ref&gt;

== Hello World Example ==

A minimal, templated Tapestry application needs only three files:

; HelloWorld.tml
: The (X)HTML template for the /helloworld page. Tapestry templates can contain any well-formed (X)HTML markup.
 &lt;source lang="xml"&gt;
&lt;!DOCTYPE html&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml" 
      xmlns:t="http://tapestry.apache.org/schema/tapestry_5_3.xsd"&gt;
&lt;body&gt;
    &lt;p&gt;Hello, ${username}&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

; HelloWorld.java
: The page class associated with the template. Here, it merely provides a *username* property that the template can access.
&lt;source lang="java"&gt;
package org.example.demo.pages;

/** A page class (automatically associated with the template file of the same name) */
public class HelloWorld {

    /** An ordinary getter */
    public String getUsername() {
        return "World";
    }
}
&lt;/source&gt;

; web.xml
: The [[Java servlet|servlet]] application [[Deployment Descriptor]], which installs Tapestry as a servlet filter.
&lt;source lang="xml"&gt;
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE web-app
        PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"
        "http://java.sun.com/dtd/web-app_2_3.dtd"&gt;
&lt;web-app&gt;
    &lt;display-name&gt;Tapestry Example&lt;/display-name&gt;
    &lt;context-param&gt;
        &lt;!-- Tell Tapestry 5 where to look for pages, components and mixins --&gt;
        &lt;param-name&gt;tapestry.app-package&lt;/param-name&gt;
        &lt;param-value&gt;org.example.demo&lt;/param-value&gt;
    &lt;/context-param&gt;
    &lt;filter&gt;
        &lt;!-- Define the Tapestry servlet filter --&gt;
        &lt;filter-name&gt;app&lt;/filter-name&gt;
        &lt;filter-class&gt;org.apache.tapestry5.TapestryFilter&lt;/filter-class&gt;
    &lt;/filter&gt;
    &lt;filter-mapping&gt;
        &lt;!-- Tell the servlet container that requests to send to the Tapestry servlet filter --&gt;
        &lt;filter-name&gt;app&lt;/filter-name&gt;
        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
    &lt;/filter-mapping&gt;
&lt;/web-app&gt;
&lt;/source&gt;

== Class transformation ==

Tapestry uses bytecode manipulation to transform page and component classes at runtime. This approach allows the page and component classes to be written as simple [[Plain Old Java Object|POJOs]], with a few naming conventions and annotations potentially triggering substantial additional behavior at class load time. Tapestry versions 5.0, 5.1 and 5.2 used the [[Javassist]] bytecode manipulation library. Subsequent versions replaced Javassist with a new bytecode manipulation layer called ''Plastic'' that is based on [[ObjectWeb ASM]].&lt;ref&gt;http://tawus.wordpress.com/2011/04/18/meeting-plastic/&lt;/ref&gt;&lt;ref&gt;http://mail-archive.ow2.org/asm/2011-04/msg00033.html&lt;/ref&gt;

== Client-side support ==

Tapestry 5 versions up through 5.3 bundled the [[Prototype JavaScript Framework|Prototype]] and [[script.aculo.us]] JavaScript frameworks, along with a Tapestry-specific library, so as to support Ajax operations as first-class citizens. Third party modules are available to integrate jQuery instead of, or in addition to, Prototype/Scriptaculous.

Starting with version 5.4, Tapestry includes a new JavaScript layer that removes built-in components' reliance on Prototype, allowing jQuery or another JavaScript framework to be plugged in.&lt;ref&gt;http://tapestryjava.blogspot.com/2012/10/zeroing-in-on-tapestry-54.html&lt;/ref&gt;

Version 5.4 also introduces support for JavaScript ''modules'' using the RequireJS module loading system.

== Core principles ==

The Tapestry project documentation cites four "principles" that govern all development decisions for Tapestry, starting with version 5 in 2008:&lt;ref&gt;{{cite web|url=http://tapestry.apache.org/principles.html |title=Principles |accessdate=2012-10-12 |date=2010-12-21 | archiveurl= https://web.archive.org/web/20121012021755/http://tapestry.apache.org/principles.html| archivedate= 12 October 2012 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

* Static Structure, Dynamic Behavior—page and component structure is essentially static, eliminating the need to construct (and store in session memory) large page and component trees.
* Adaptive API—the framework is designed to adapt to the code, rather than having the code adapt to the framework
* Differentiate Public vs. Internal APIs—all APIs are explicitly "internal" (private) except those that are necessarily public.
* Ensure Backwards Compatibility—The Tapestry developers are reportedly committed to ensuring that upgrading to the latest version of Tapestry is always easy.

== Criticism ==

Tapestry has been criticized as not being backward-compatible across major versions, especially noted in the transition from version 4 to version 5, where no clean migration path was available for existing applications.&lt;ref&gt;{{cite web|url=http://tapestry.1045711.n5.nabble.com/Tapestry5-future-compatiblity-td2431085.html |title=Tapestry5 future {{sic|nolink=y|compatiblity}} |accessdate=2013-01-21 |date=2009-04-30}}&lt;/ref&gt; Project team members have acknowledged this as a major problem for Tapestry's users in the past, and backward compatibility was made a major design goal for Tapestry going forward. From early on in the development of version 5, backward compatibility was listed as one of Tapestry's four new "Core Principles", and two of the other three were intended to make the evolution of the framework possible without sacrificing backward compatibility. Project team members claim that all Tapestry releases since 5.0 have been highly backward compatible.

Early criticisms of Tapestry 5 also mentioned documentation as a shortcoming. Project members now claim that this deficiency has been largely addressed with a thoroughly revised and updated User's Guide and other documentation.

Since version 5.0, Tapestry has bundled the Prototype and Scriptaculous JavaScript libraries. According to Howard Lewis Ship, in the 2008-2009 timeframe these were reasonable choices. Since then, however, Prototype's popularity has declined, and jQuery's has risen dramatically. In response, the Tapestry community developed modules that allowed jQuery to be used in addition to, or instead of, Prototype. Meanwhile, the current version of Tapestry, 5.4, removes the dependency on Prototype entirely, replacing it with a compatibility layer into which either jQuery or Prototype (or potentially any other JavaScript framework) can be plugged.

== Relation to other frameworks ==

According to Howard Lewis Ship, Tapestry was initially conceived as an attempt to implement in Java some of the general concepts and approaches found in WebObjects, which was at that time written in [[Objective-C]] and closed-source.&lt;ref&gt;http://devrates.com/post/show/345948/howard-lewis-ship-of-tapestry-interview-%5Bpart-1%5D&lt;/ref&gt;

[[Apache Wicket]] was developed as a response to the complexity of early versions of Tapestry, according to Wicket originator Jonathan Locke.&lt;ref&gt;https://web.archive.org/web/20040909074534/http://www.theserverside.com/news/thread.tss?thread_id=28162&lt;/ref&gt;

[[Facelets]], the default view technology in [[JavaServer Faces]], was reportedly inspired by early versions of Tapestry, as an attempt to fill the need for "a framework like Tapestry, backed by JavaServer Faces as the industry standard".&lt;ref&gt;https://web.archive.org/web/20070706220453/https://facelets.dev.java.net/&lt;/ref&gt;&lt;ref&gt;https://web.archive.org/web/20130113100928/http://www.jsfcentral.com/articles/facelets_1.html&lt;/ref&gt;

== History ==

{| class="wikitable sortable"
|-
! Version !! Date !! Description
|-
| {{Version  |o  |1.0}}
| 2000 || Developed by Howard Lewis Ship for internal use
|-
| {{Version  |o  |2.0}}
| 2002-04 || First made available on [[SourceForge]] under the [[GNU Lesser General Public License]].&lt;ref&gt;{{cite web|url=http://www.theserverside.com/discussions/thread.tss?thread_id=12750|title=Tapestry: Java Web Components Release 2.0 is Out|accessdate=2013-01-20 }}&lt;/ref&gt;
|-
| {{Version  |o  |3.0}}
| 2004-04 || The first release under Apache, as a Jakarta sub-project.&lt;ref&gt;{{cite web|url=http://www.theserverside.com/discussions/thread.tss?thread_id=25354|title=Tapestry 3.0 Final Release|accessdate=2013-01-20 }}&lt;/ref&gt;
|-
| {{Version  |o  |4.0}}
| 2006-01 || Introduced support for JDK 1.5 annotations, a new input validation subsystem, and improved error reporting &lt;ref&gt;{{cite web|url=http://www.theserverside.com/news/thread.tss?thread_id=38407|title=Tapestry 4.0 Released|accessdate=2013-01-20 }}&lt;/ref&gt;
|-
| {{Version  |co |5.0}}
| 2008-12 || A nearly complete rewrite from Tapestry 4, introducing a new POJO-based component model emphasizing convention over configuration, and replaced Hivemind with a new no-XML Inversion of Control layer.
|-
| {{Version  |co |5.1}}
| 2009-04 || Performance and memory improvements, automatic GZIP compression, JavaScript aggregation, but remained backwards compatible to Tapestry 5.0. 
|-
| {{Version  |co |5.2}}
| 2010-12 || Added [[Bean Validation|JSR 303 Bean Validation]].&lt;ref&gt;{{cite web|url=http://blog.tapestry5.de/index.php/2010/01/04/tapestry-and-jsr-303-bean-validation-api/ |title=Tapestry and JSR-303 Bean Validation API |accessdate=2010-03-13 |date=2010-01-04 | archiveurl= https://web.archive.org/web/20100416154003/http://blog.tapestry5.de/index.php/2010/01/04/tapestry-and-jsr-303-bean-validation-api/| archivedate= 16 April 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Extended live class reloading to service implementations. Removed page pooling.&lt;ref&gt;{{cite web|url=http://tapestry.apache.org/2010/12/17/announcing-tapestry-52.html |title=Announcing Tapestry 5.2 |accessdate=2012-11-14 |date=2010-12-17 | archiveurl= https://web.archive.org/web/20121114182121/http://tapestry.apache.org/2010/12/17/announcing-tapestry-52.html/| archivedate= 14 November 2012 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
|-
| {{Version  |co |5.3}}
| 2011-11 || Added support for HTML5 doctype, JSR-330 annotations for injection,&lt;ref&gt;http://tapestry.apache.org/using-jsr-330-standard-annotations.html&lt;/ref&gt; performance and memory improvements, new components, switched from JavaAssist to ASM bytecode manipulation
|-
| {{Version  |co |5.3.1 - 5.3.8}}
| 2012-2014 || Bug fixes and minor enhancements
|-
| {{Version  |c |5.4-5.4.3}}
| 2015-17 || Current stable version. Major client-side enhancements. New JavaScript layer for switchable jQuery/Prototype support, uses Require.js for its JavaScript module system, [[Twitter Bootstrap]] for its default styling.&lt;ref&gt;{{cite web|url=http://tapestry.apache.org/javascript-rewrite.html|title=JavaScript Rewrite|accessdate=2013-01-20 | archiveurl= https://web.archive.org/web/20121114180615/http://tapestry.apache.org/javascript-rewrite.html| archivedate=2012-11-14}}&lt;/ref&gt;
|}

== Related projects ==
*[http://tynamo.org/ Tynamo Framework aka Trails 2] is based on Tapestry 5.
*[http://jumpstart.doublenegative.com.au/home.html JumpStart tutorial] Tapestry by examples.

== See also ==
* [[Apache Wicket]]
* [[Comparison of web frameworks]]
* [[Facelets]]
* [[Java Platform, Enterprise Edition|Java EE]]
* [[Java view technologies and frameworks]]

== References ==
{{refbegin}}
* {{citation
| first1      = Igor 
| last1       = Drobiazko
| year        = 2012
| title       = Tapestry 5: Rapid web application development in Java
| publisher   = Igor Drobiazko
| pages       = 482
| url         = http://www.tapestry5book.com/
| postscript  = &lt;!--none--&gt; 
}}
* {{citation
| first1      = Alexander 
| last1       = Kolesnikov
| date        = January 15, 2008
| title       = Tapestry 5: Building Web Applications: A step-by-step guide to Java Web development with the developer-friendly Apache Tapestry framework
| publisher   = [[Packt|Packt Publishing]]
| pages       = 280
| isbn        = 1-84719-307-2
| url         = http://www.packtpub.com/tapestry-5/book
| postscript  = &lt;!--none--&gt; 
}}
* {{citation
| first      = Ka
| last       = Iok Tong
| date       = January 1, 2007
| title      = Enjoying Web Development with Tapestry
| publisher  = 
| pages      = 497 
| edition    = 3rd 
| asin       = B00262M3HS
| url        =
| postscript = &lt;!--none--&gt; 
}}
* {{citation
| first      = Howard M. Lewis
| last       = Ship
| year       = 2004
| title      = Tapestry in Action
| publisher  = [[Manning Publications|Manning]]
| pages      = 580
| isbn       = 1932394117
| url        =
| postscript = &lt;!--none--&gt; 
}}
{{refend}}

=== Notes ===
{{reflist}}

== External links ==
* [http://jumpstart.doublenegative.com.au/jumpstart/ Getting Started Examples]
* [http://howardlewisship.com/ Howard Lewis Ship]
* [https://github.com/code8/tapestry-boot/ Integration with Spring Boot]
* [http://tapestry.apache.org/ Tapestry Home Page]
* [http://tynamo.org/ Tynamo Project]
{{Application frameworks}}
{{apache}}

{{Authority control}}

[[Category:Apache Software Foundation|Tapestry]]
[[Category:Java enterprise platform]]
[[Category:Web frameworks]]</text>
      <sha1>t3cvycxsfd6jpoucytnowk95syj6bge</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Wicket</title>
    <ns>0</ns>
    <id>1934392</id>
    <revision>
      <id>836772492</id>
      <parentid>836769653</parentid>
      <timestamp>2018-04-16T19:20:17Z</timestamp>
      <contributor>
        <username>Janux de</username>
        <id>32052570</id>
      </contributor>
      <comment>Add first release of current major version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17483">{{Infobox software
| name                   = Apache Wicket
| logo                   = Apache Wicket logo.png
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 7.10.0
| latest release date    = {{release date|2018|02|15}}&lt;ref&gt;[https://wicket.apache.org/news/2018/02/15/wicket-7.10.0-released.html Apache Wicket - Apache Wicket 7.10.0 released]. wicket.apache.org. Retrieved on 2018-04-15.&lt;/ref&gt;
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web application framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://wicket.apache.org}}
}}

'''Apache Wicket''', commonly referred to as '''Wicket''', is a lightweight component-based [[web application framework]] for the [[Java (programming language)|Java programming language]] conceptually similar to [[JavaServer Faces]] and [[Apache Tapestry|Tapestry]].  It was originally written by Jonathan Locke in April 2004.  Version 1.0 was released in June 2005. It graduated into an [[Apache Software Foundation|Apache]] top-level project in June 2007.&lt;ref&gt;{{cite web|url = http://martijndashorst.com/blog/2007/06/20/3-2-1|title = Wicket graduates from Apache Incubation|date = 2007-07-20|accessdate = 2008-03-07|first = Martijn|last = Dashorst}}&lt;/ref&gt;

== Rationale ==
Traditional [[model-view-controller]] (MVC) frameworks work in terms of whole [[HTTP request|requests]] and whole pages.  In each request cycle, the incoming request is mapped to a method on a ''controller'' object, which then generates the outgoing response in its entirety, usually by pulling data out of a ''model'' to populate a ''view'' written in specialized [[web template|template markup]].  This keeps the application's [[control flow|flow-of-control]] simple and clear, but can make [[code reuse]] in the controller difficult.

In contrast, Wicket is closely patterned after [[stateful]] [[GUI]] frameworks such as [[Swing (Java)|Swing]].  Wicket applications are trees of ''components'', which use listener [[delegation (programming)|delegates]] to react to [[HTTP]] requests against links and forms in the same way that Swing components react to mouse and keystroke events. Wicket is categorized as a component-based framework.

== Design ==
Wicket uses plain [[XHTML]] for templating (which enforces a clear separation of presentation and [[business logic]] and allows templates to be edited with conventional [[WYSIWYG]] design tools&lt;ref&gt;{{cite web|first = Daniel|last = Carleton|date = 2007-10-12|accessdate = 2008-03-07|url = http://www.devx.com/Java/Article/35620|title = Java Web Development the Wicket Way|publisher = DevX| archiveurl= https://web.archive.org/web/20080310152030/http://www.devx.com/Java/Article/35620| archivedate= 10 March 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;).  Each component is bound to a named element in the XHTML and becomes responsible for rendering that element in the final output. The ''page'' is simply the top-level containing component and is paired with exactly one XHTML template.  Using a special tag, a group of individual components may be abstracted into a single component called a ''panel'', which can then be reused whole in that page, other pages, or even other panels.

Each component is backed by its own model, which represents the state of the component. The framework does not have knowledge of how components interact with their models, which are treated as [[black box|opaque]] objects automatically [[serialization|serialized]] and [[object persistence|persisted]] between requests. More complex models, however, may be made ''detachable'' and provide [[software hook|hooks]] to arrange their own storage and restoration at the beginning and end of each request cycle.  Wicket does not mandate any particular object-persistence or [[object-relational mapping|ORM]] layer, so applications often use some combination of [[Hibernate (Java)|Hibernate]] objects, [[EJB]]s or [[POJO]]s as models.

In Wicket, all server side state is automatically managed. You should never directly use an HttpSession object or similar wrapper to store state. Instead, state is associated with components. Each server-side page component holds a nested hierarchy of stateful components, where each component’s model is, in the end, a POJO (Plain Old Java Object)

Wicket is all about simplicity. There are no configuration files to learn in Wicket. Wicket is a simple class library with a consistent approach to component structure.

== Example ==
A [[Hello World]] Wicket application, with four files:
; HelloWorld.html
: The XHTML template.
 &lt;source lang="xml"&gt;
&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml" 
      xmlns:wicket="http://wicket.apache.org/dtds.data/wicket-xhtml1.3-strict.dtd"
      xml:lang="en" lang="en"&gt;

&lt;body&gt;
    &lt;span wicket:id="message" id="message"&gt;Message goes here&lt;/span&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

; HelloWorld.java
: The page component that will be bound to the template.  It, in turn, binds a child component (the Label component named "message").
&lt;source lang="java"&gt;
package org.wikipedia.wicket;

import org.apache.wicket.markup.html.WebPage;
import org.apache.wicket.markup.html.basic.Label;

public class HelloWorld extends WebPage {
    /**
     * Constructor
     */
    public HelloWorld() {
        add(new Label("message", "Hello World!"));
    }
}
&lt;/source&gt;

; HelloWorldApplication.java
: The main application class, which routes requests for the homepage to the HelloWorld page component.
&lt;source lang="java"&gt;
package org.wikipedia.wicket;

import org.apache.wicket.protocol.http.WebApplication;

public class HelloWorldApplication extends WebApplication {
    /**
     * Constructor.
     */
    public HelloWorldApplication() {
    }

    /**
     * @see org.apache.wicket.Application#getHomePage()
     */
    public Class getHomePage() {
        return HelloWorld.class;
    }
}
&lt;/source&gt;

; web.xml
: The [[Java servlet|servlet]] application [[Deployment Descriptor]], which installs Wicket as the default handler for the servlet and arranges for HelloWorldApplication to be instantiated at startup.
&lt;source lang="xml"&gt;
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
         xmlns="http://java.sun.com/xml/ns/javaee" 
         xmlns:web="http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" 
         xsi:schemaLocation="http://java.sun.com/xml/ns/javaee 
                             http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" 
         id="WebApp_ID" version="2.5"&gt;
    &lt;display-name&gt;Wicket Example&lt;/display-name&gt;
    &lt;filter&gt;
        &lt;filter-name&gt;HelloWorldApplication&lt;/filter-name&gt;
        &lt;filter-class&gt;org.apache.wicket.protocol.http.WicketFilter&lt;/filter-class&gt;
        &lt;init-param&gt;
            &lt;param-name&gt;applicationClassName&lt;/param-name&gt;
            &lt;param-value&gt;org.wikipedia.wicket.HelloWorldApplication&lt;/param-value&gt;
        &lt;/init-param&gt;
    &lt;/filter&gt;
    &lt;filter-mapping&gt;
        &lt;filter-name&gt;HelloWorldApplication&lt;/filter-name&gt;
        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
    &lt;/filter-mapping&gt;
&lt;/web-app&gt;
&lt;/source&gt;

== Components ==
*Basic components like form, links, repeaters, and so on are built-in. See http://www.wicket-library.com/wicket-examples/compref/
*More are on https://cwiki.apache.org/confluence/display/WICKET/Index

== Releases ==
{| class="wikitable sortable"
|-
! Release
! style="width:60px;"| Date
! Notes
|-
|1.3.7
|2009-07-30
|&lt;ref&gt;[http://wicket.apache.org/2009/07/30/wicket-1.3.7-released.html Apache Wicket - Apache Wicket 1.3.7 marks end of life for Wicket 1.3]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20110105171218/http://wicket.apache.org/2009/07/30/wicket-1.3.7-released.html |date=January 5, 2011 }}&lt;/ref&gt;
|-
|1.4
|2009-07-30
|" ... a departure from the past where we leave Java 1.4 behind and we require Java 5 as the minimum JDK version. By moving to Java 5 as the required minimum platform, we were able to utilize Java 5 idioms and increase the type safety of our APIs." &lt;ref&gt;[http://wicket.apache.org/2009/07/30/wicket-1.4-takes-typesafety-to-the-next-level.html Apache Wicket - Apache Wicket 1.4 takes typesafety to the next level] {{webarchive |url=https://web.archive.org/web/20120425121244/http://wicket.apache.org/2009/07/30/wicket-1.4-takes-typesafety-to-the-next-level.html |date=April 25, 2012 }}. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.1
|2009-08-21
|&lt;ref&gt;[http://wicket.apache.org/2009/08/21/wicket-1.4.1-released.html Apache Wicket - Wicket 1.4.1 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20100922010156/http://wicket.apache.org/2009/08/21/wicket-1.4.1-released.html |date=September 22, 2010 }}&lt;/ref&gt;
|-
|1.4.9
|2010-05-24
|"... over fifteen bug fixes and improvements" &lt;ref&gt;[http://wicket.apache.org/2010/05/24/wicket-1.4.9-released.html Apache Wicket - Wicket 1.4.9 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20141006101210/http://wicket.apache.org/2010/05/24/wicket-1.4.9-released.html |date=October 6, 2014 }}&lt;/ref&gt;
|-
|1.4.10
|2010-08-11
|"...  over thirty bug fixes and improvements." &lt;ref&gt;[http://wicket.apache.org/2010/08/11/wicket-1.4.10-released.html Apache Wicket - Wicket 1.4.10 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20120425121158/http://wicket.apache.org/2010/08/11/wicket-1.4.10-released.html |date=April 25, 2012 }}&lt;/ref&gt;
|-
|1.4.16
|2011-02-25
|"This is primarily a minor bugfix release on the 1.4.x (stable) branch." &lt;ref&gt;[http://wicket.apache.org/2011/02/25/wicket-1.4.16-released.html Apache Wicket - Wicket 1.4.16 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20120425121203/http://wicket.apache.org/2011/02/25/wicket-1.4.16-released.html |date=April 25, 2012 }}&lt;/ref&gt;
|-
|1.4.17
|2011-04-02
|"This is primarily a minor bugfix release on the 1.4.x (stable) branch." &lt;ref&gt;[http://wicket.apache.org/2011/04/02/wicket-1.4.17-released.html Apache Wicket - Wicket 1.4.17 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20141006121345/http://wicket.apache.org/2011/04/02/wicket-1.4.17-released.html |date=October 6, 2014 }}&lt;/ref&gt;
|-
|1.4.18
|2011-08-09
|"This is primarily a minor bugfix release on the 1.4.x (stable) branch." &lt;ref&gt;[http://wicket.apache.org/2011/08/09/wicket-1.4.18-released.html Apache Wicket - Wicket 1.4.18 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20141006102352/http://wicket.apache.org/2011/08/09/wicket-1.4.18-released.html |date=October 6, 2014 }}&lt;/ref&gt;
|-
|1.4.19
|2011-10-19
|"This is primarily a minor bugfix release on the 1.4.x (stable) branch." &lt;ref&gt;[http://wicket.apache.org/2011/10/17/wicket-1.4.19-released.html Apache Wicket - Wicket 1.4.19 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20141006100642/http://wicket.apache.org/2011/10/17/wicket-1.4.19-released.html |date=October 6, 2014 }}&lt;/ref&gt;
|-
|1.5.0
|2011-09-07
|"Apache Wicket 1.5 has been in development for the last two years and brings many improvements over previous versions." &lt;ref&gt;[http://wicket.apache.org/2011/09/07/wicket-1.5-released.html Apache Wicket - Apache Wicket releases Wicket 1.5]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20141006081714/http://wicket.apache.org/2011/09/07/wicket-1.5-released.html |date=October 6, 2014 }}&lt;/ref&gt;
|-
|1.5.1
|2011-09-29
|"... over 40 bug fixes and 15 improvements."&lt;ref&gt;[http://wicket.apache.org/2011/09/29/wicket-1.5.1-released.html Apache Wicket - Wicket 1.5.1 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20141006093633/http://wicket.apache.org/2011/09/29/wicket-1.5.1-released.html |date=October 6, 2014 }}&lt;/ref&gt;
|-
|1.5.2 
|2011-10-24 
|"... over 25 bug fixes and 5 improvements."&lt;ref&gt;[http://wicket.apache.org/2011/10/24/wicket-1.5.2-released.html Apache Wicket - Wicket 1.5.2 released]. Wicket.apache.org. Retrieved on 2013-08-13. {{webarchive |url=https://web.archive.org/web/20111101220715/http://wicket.apache.org/2011/10/24/wicket-1.5.2-released.html |date=November 1, 2011 }}&lt;/ref&gt;
|-
|1.5.3 
|2011-11-14 
|"... over 40 bug fixes and improvements."&lt;ref&gt;{{cite web|url=http://wicket.apache.org/2011/11/14/wicket-1.5.3-release.html |accessdate=November 16, 2011 }}{{dead link|date=April 2017|bot=medic}}{{cbignore|bot=medic}}&lt;/ref&gt;
|-
|6.0.0
|2012-09-05
| Out-of-the box jQuery integration, complete control over AJAX requests, improved event registration in browsers, support for large datasets, dependency management for client side JavaScript libraries, experimental support for websockets.&lt;ref&gt;{{cite web|url=https://wicket.apache.org/news/2012/09/05/wicket-6.0.0-released.html |accessdate=April 16, 2018 }}&lt;/ref&gt;
|-
|6.3.0
|2012-11-16
| jQuery 1.8.2; fixed JavaScript errors in IE7 and IE8.&lt;ref&gt;{{cite web|url=https://wicket.apache.org/news/2012/11/16/wicket-6.3.0-released.html |accessdate=April 16, 2018 }}&lt;/ref&gt;
|-
|6.4.0
|2012-12-14
|[[jQuery]] 1.8.3, [[Bootstrap (front-end framework)|bootstrap]] 2.2.2, JSR 303 BeanValidation support, Hierarchical feedback panel.&lt;ref&gt;{{cite web|url=https://wicket.apache.org/news/2012/12/14/wicket-6.4.0-released.html |accessdate=April 16, 2018 }}&lt;/ref&gt;
|-
|7.0.0
|2015-07-28
| Java 7 support, cross site request forgery prevention, support for inline images.&lt;ref&gt;[https://wicket.apache.org/news/2015/07/28/wicket-7.0-released.html Apache Wicket v7.0 released]. wicket.apache.org. Retrieved on 2018-04-16.&lt;/ref&gt;
|}

==See also==
{{Portal|Free software}}
*[[Vaadin]]
*[[Apache Tapestry|Tapestry]]
*[[Apache Click|Click]]
*[[ZK (framework)|ZK]]
*[[Richfaces]]
*[[Echo (framework)|Echo]]

==References==
{{refbegin}}
* {{cite book
| first1    = João Sávio 
| last1     = Ceregatti Longo  
| date      = August 26, 2013
| title     = Instant Apache Wicket 6
| publisher = [[Packt Publishing]]
| edition   = 1st
| page      = 54 
| isbn      = 1783280018
| url       = https://www.packtpub.com/web-development/instant-apache-wicket-6-instant
}}
* {{cite book
| first     = Jochen
| last      = Mader 
| date      = March 28, 2012
| title     = Wicket: Komponentenbasiert und objektorientiert
| publisher = [[Entwickler]]
| edition   = 1st
| page      = 220
| isbn      = 3868020810
| url       = http://www.apress.com/book/view/9781590597224
}}
* {{cite book
| first1    = Igor 
| last1     = Vaynberg
| date      = May 15, 2011
| title     = Apache Wicket Cookbook 
| publisher = [[Packt Publishing]]
| edition   = 1st
| page      = 312 
| isbn      = 1-84951-160-8
| url       = https://www.packtpub.com/apache-wicket-cookbook/book
}}
* {{cite book
| first1    = Martijn
| last1     = Dashorst
| first2    = Eelco
| last2     = Hillenius
| date      = September 15, 2008
| title     = Wicket in Action
| publisher = [[Manning Publications]]
| edition   = 1st
| page      = 392
| isbn      = 1-932394-98-2
| url       = https://www.amazon.com/Wicket-Action-Martijn-Dashorst/dp/1932394982
}}
* {{cite book
| first     = Karthik
| last      = Gurumurthy
| date      = September 7, 2006
| title     = Pro Wicket
| publisher = [[Apress]]
| edition   = 1st
| page      = 328
| isbn      = 1-59059-722-2
| url       = http://www.apress.com/book/view/9781590597224
}}
{{refend}}

===Notes===
{{Reflist}}

==External links==

===Introductory articles===
*[http://www.ibm.com/developerworks/web/library/wa-aj-wicket/?S_TACT=105AGY82&amp;S_CMP=GENSITE IBM Wicket: A simplified framework for building and testing dynamic Web pages]
*[http://ensode.net/wicket_first_look.html A First Look at the Wicket Framework]
*[http://www.theserverside.com/news/thread.tss?thread_id=34725 The Server Side discussion on Wicket 1.0]
*[https://web.archive.org/web/20040909074534/http://www.theserverside.com/news/thread.tss?thread_id=28162 The Server Side discussion]
*[http://www.javalobby.org/java/forums/t105230.html Javalobby interview with Martijn Dashorst (project chairman)]
*[https://web.archive.org/web/20100124153441/http://www.viddler.com/explore/oredev/videos/61 Wicket introduction presentation by Martijn Dashorst]

===Blogs===
*[http://wicketinaction.com Wicket in Action]
*[http://martijndashorst.com Martijn Dashorst]
*[https://web.archive.org/web/20090909161213/http://blog.brunoborges.com.br/ Bruno Borges]
*[http://mysticcoders.com Mystic Coders]

===Documentation===
*[http://wicketstuff.org Reusable components and patterns for Wicket]
*[https://web.archive.org/web/20100620082228/http://wicketstuff.org/wicket14/ Site that has live demos and a repository of components]
*[http://cwiki.apache.org/WICKET Wiki with how-tos, a manual and more]
*[http://wicketbyexample.com Plethora of examples of using Wicket in the real world]
*[https://code.google.com/p/wicket-guide/ A free and comprehensive user guide to Wicket]

{{Apache}}
{{Application frameworks}}
{{Authority control}}

[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Web frameworks]]
[[Category:Apache Software Foundation|Wicket]]</text>
      <sha1>c7mxary0qhso73tie7xuk7w2w0le7ia</sha1>
    </revision>
  </page>
  <page>
    <title>Apache XML</title>
    <ns>0</ns>
    <id>1004521</id>
    <revision>
      <id>541095499</id>
      <parentid>538308294</parentid>
      <timestamp>2013-02-28T00:57:49Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2662299]] ([[User talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1891">{{Unreferenced|date=December 2009}}
The '''Apache XML''' project is part of the [[Apache Software Foundation]] and focuses on [[XML]]-related projects. 

It consists of several sub projects:

==Active sub projects==
*'''[[Xerces]]''': An XML parser for [[Java (programming language)|Java]], [[C++]] and [[Perl]]
*'''[[Xalan]]''': An [[XSLT]] stylesheet processor for Java and C++ which implements the [[XPath]] query language.
*'''[[Apache Forrest|Forrest]]''': A standards-based documentation framework
*'''XML-Security''': A project providing security functionality for XML data 
*'''Xindice''': A native [[XML database]] 
*'''XML Commons''': A project focusing on common code and guidelines for XML projects 
*'''[[XMLBeans]]''': An XML-Java binding tool

==Projects related to webservices==
*'''SOAP''': Is an old implementation of the [[SOAP]]. This project based on [[IBM]]'s SOAP4J implementation. It should no longer be used for new projects. Instead you should favour the Axis implementation.
*'''XML-RPC''': Apache XML-RPC is a Java implementation of [[XML-RPC]], a protocol that uses XML over [[HyperText Transfer Protocol|HTTP]] to implement remote procedure calls.
*'''[[Axis (computer program)|Axis]]''': Apache Axis is the current implementation of the [[SOAP]] for Java and C++. It is the successor for the SOAP project.
*'''WSIF''': [[Web Services Invocation Framework]] is a simple Java [[Application programming interface|API]] for invoking [[Web services]].

==No longer developed projects==
*'''[[AxKit]]''': An XML-based web publishing framework in [[mod_perl]] 
*'''Crimson''': A Java XML parser derived from the [[Sun Microsystems|Sun]] Project X Parser 
*'''Xang''': Framework for rapid development of dynamic server pages in [[ECMAScript]] ([[JavaScript]])
{{Apache}}

{{DEFAULTSORT:Apache Xml}}
[[Category:XML software]]
[[Category:Apache Software Foundation|XML]]</text>
      <sha1>t9z10bgh5rte83uwp6kg9jpi1t3wsp5</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OFBiz</title>
    <ns>0</ns>
    <id>7528242</id>
    <revision>
      <id>832586016</id>
      <parentid>830780236</parentid>
      <timestamp>2018-03-26T21:27:50Z</timestamp>
      <contributor>
        <ip>81.198.129.182</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4825">{{Multiple issues|
{{More citations needed|date=June 2011}}
{{third-party|date=September 2015}}
}}

{{Infobox software
| name                   = Apache OFBiz
| logo                   = OFBiz-logoV3-apache.png
| logo size              = 190px
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = &lt;!-- If you update this, remember to also update [[Comparison of shopping cart software]]--&gt; 16.11.04
| latest release date    = {{release date and age|2018|01|02}}&lt;ref&gt;{{cite web|url=https://ofbiz.apache.org/|accessdate=2 January 2018|title=The Apache OFBiz® Project}}&lt;/ref&gt;
| latest preview version =
| latest preview date    =
| programming language   = [[Java (programming language)|Java]] [[XML]] [[FreeMarker]] [[Groovy (programming language)|Groovy]]
| operating system       = [[Cross-platform]]
| genre                  = [[Business process]]
| license                = [[Apache License]] 2.0
| website                = {{url|//ofbiz.apache.org}}
}}
'''Apache OFBiz''' is an [[open source]] [[enterprise resource planning]] (ERP) system. It provides a suite of enterprise applications that integrate and automate many of the [[business process]]es of an enterprise.

OFBiz is an [[Apache Software Foundation]] top level project.

== Overview ==
Apache OFBiz is a framework that provides a common [[data model]] and a rich set of [[business process]]es.
All applications are built around a common architecture using common data, logic and process components.
Beyond the framework itself, Apache OFBiz offers functionality including:
* [[Accounting]] (agreements, invoicing, vendor management, [[general ledger]])
* Asset maintenance
* Catalogue and product management
* Facility and [[warehouse management system]] (WMS)
* Manufacturing execution / [[manufacturing operations management]] (MES/MOM)
* Order processing
* Inventory management, automated stock replenishment etc.
* [[Content management system]] (CMS)
* [[Human resources]] (HR)
* People and group management
* [[Project management]]
* [[Sales force management system|Sales force automation]]
* Work effort management
* Electronic [[point of sale]] (ePOS)
* [[Electronic commerce]] (eCommerce)
* [[Scrum (development)]] (Scrum software development support)

== Technology ==

All Apache OFBiz functionality is built on a common framework. The functionality can be divided into the following distinct layers:

=== Presentation Layer ===
Apache OFBiz uses the concept of "screens" to represent the Apache OFBiz pages. Each page is, normally, represented as a screen. A page in Apache OFBiz consists of components. A component can be a header, footer, etc. When the page is rendered all the components are combined together as specified in the screen definition. Components can be Java Server Pages ([JSP]s) &lt;deprecated&gt;, FTL pages built around [[FreeMarker]] [[Template engine (web)|template engine]], Forms or Menus Widgets. Widgets are an OFBiz specific technology.

=== Business Layer ===
The business, or application layer defines services provided to the user. The services can be of several types: Java methods, SOAP, simple services, workflow, etc. A service engine is responsible for invocation, transactions and security.

Apache OFBiz uses a set of well-established open source technologies and standards such as  [[Java platform|Java]], [[Java Platform, Enterprise Edition|Java EE]], [[XML]] and [[SOAP]]. Although Apache OFBiz is built around the concepts used by Java EE, many of its concepts are implemented in different ways; either because Apache OFBiz was designed prior to many recent improvements in Java EE or because Apache OFBiz authors didn’t agree with those implementations.

=== Data Layer ===
The data layer is responsible for database access, storage and providing a common data interface to the Business layer.
Data is accessed not in [[Object Oriented]] fashion but in a [[Relational database|relational]] way.
Each [[entity]] (represented as a row in the database) is provided to the business layer as a set of generic values.
A generic value is not typed, so fields of an entity are accessed by the [[Column (database)|column]] name.

==See also==
* [[Comparison of shopping cart software]]
* [[Comparison of accounting software]]
* [[Comparison of project management software]]
* [[List of ERP software packages]]

== References ==
{{reflist}}

== External links ==
*[//ofbiz.apache.org Official Apache OFBiz website]

{{ERP software}}
{{Apache}}

[[Category:Apache Software Foundation|OFBiz]]
[[Category:Free accounting software]]
[[Category:Free e-commerce software]]
[[Category:Free industrial software]] &lt;!-- because of MRP --&gt;
[[Category:Free ERP software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Web applications]]</text>
      <sha1>rp9vtgyf9d9ejpjugrygdpudr2lobxf</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ServiceMix</title>
    <ns>0</ns>
    <id>7528444</id>
    <revision>
      <id>847791838</id>
      <parentid>813786399</parentid>
      <timestamp>2018-06-27T20:10:59Z</timestamp>
      <contributor>
        <username>Lzer</username>
        <id>1845577</id>
      </contributor>
      <minor/>
      <comment>Clean up double spaced lines</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5917">{{Infobox software
| name                   = Apache ServiceMix
| logo                   = &lt;div style="background:#4d83b1;"&gt;[[file:Servicemix-logo.png|200px]]&lt;/div&gt;
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 7.0
| latest release date    = {{Start date and age|2017|1}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Enterprise service bus]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://servicemix.apache.org}}
}}

'''Apache ServiceMix''' is an enterprise-class [[open source|open-source]] distributed [[enterprise service bus]] (ESB) based on the [[service-oriented architecture]] (SOA) model. It is a project of the [[Apache Software Foundation]] and was built on the semantics and [[application programming interface]]s of the [[Java Business Integration]] (JBI) specification [[Java Community Process|JSR]] 208. The software is distributed under the [[Apache License]].

The productized and supported release of ServiceMix 4 is from JBoss and called [[Fuse ESB]]. 

[[Fabric8]] is a free Apache 2.0 Licensed upstream community for the JBoss Fuse product from Red Hat.

The current version of ServiceMix fully supports the [[OSGi]] framework. ServiceMix is lightweight and easily embeddable, has integrated [[Spring Framework]] support and can be run at the edge of the network (inside a client or server), as a standalone ESB provider or as a service within another ESB. ServiceMix is compatible with [[Java Platform, Standard Edition|Java SE]] or a [[Java Platform, Enterprise Edition|Java EE]] [[application server]].
ServiceMix uses [[Apache ActiveMQ|ActiveMQ]] to provide remoting, clustering, reliability and distributed failover. The basic frameworks used by ServiceMix are Spring and XBean.&lt;ref&gt;{{cite web|last=Irriger|first=Axel |title=Apache ServiceMix|url=http://www.methodsandtools.com/tools/tools.php?servicemix| accessdate = 17 February 2011}}&lt;/ref&gt;

ServiceMix is composed the latest versions of [[Apache ActiveMQ]], [[Apache Camel]], [[Apache CXF]], and [[Apache Karaf]].

Additional installation features include:
* BPM engine via [[Activiti (software)|Activiti]]
* JPA support via [[Apache OpenJPA]]
* XA transaction management via JTA via [[Apache Aries]]

ServiceMix is an [[enterprise service bus]] that provides: {{Citation needed|date=June 2009}}
*Federation, clustering and container provided failover
*Hot deployment and lifecycle management of business objects
*Vendor independence from vendor-licensed products
*Compliance with the JBI specification JSR 208
*Compliance with the OSGi 4.2 specification through Apache Felix&lt;ref&gt;{http://felix.apache.org/ Announcement} by [[Brian Taylor (Software Architect)]] {{webarchive |url=https://www.webcitation.org/6HeO8qd6m?url=http://felix.apache.org/ |date=June 25, 2013 }}&lt;/ref&gt;
*Support for OSGi Enterprise through Apache Aries

It was accepted as an official Apache project by the ASF Board of Directors on September 19, 2007.&lt;ref&gt;[http://gnodet.blogspot.com/2007/09/servicemix-has-graduated.html Announcement] by [[Guillaume Nodet]]&lt;/ref&gt;

==See also==
*[[Service-oriented architecture]] (SOA)
*[[Service Component Architecture]] (SCA)
*[[Composite Application Service Assembly]] (CASA)
*[[Composite application]]
*[[Apache ActiveMQ]]
*[[Apache Camel]]
*[[Apache CXF]]
*[[Enterprise messaging system]]
*[[Event-driven SOA]]
*[[Message-oriented middleware]]
*[[OpenShift]]
*[[Akka (toolkit)]]
*[[JBoss Developer Studio]] Community Edition
*[[Apache ODE]]
*[[Open ESB]]
*[http://logicoy.com/blogs/running-apache-camel-in-openesb/ Running Apache Camel in OpenESB as alternative]
*[http://www.eclipse.org/sirius/overview.html eclipse sirius - Free and GPL eclipse tool to build your own arbitrary complex military grade modeling tools on one hour]
*[https://www.eclipse.org/soa/sca/ eclipse SCA Tools - Gnu free composite tool]
*[http://www.obeodesigner.com/overview Free GPL obeodesigner made with eclipse sirius]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{citation
| first = Christudas
| last = Binildas A
| title = Service Oriented Java Business Integration
| author-link =
| publication-date =
| date = August 13, 2008
| edition = 1st
| volume =
| series =
| publication-place =
| place =
| publisher = [[Packt|Packt Publishers]]
| page = 436
| id =
| isbn = 978-1-84719-440-4
| doi =
| oclc =
| url = http://www.packtpub.com/service-oriented-java-business-integration
| accessdate =
}}
*{{citation
| first1 = Tijs
| last1 = Rademakers
| first2 = Jos
| last2 = Dirksen
| title = Open-Source ESBs in Action
| author-link =
| publication-date =
| date = October 28, 2008
| edition =
| volume =
| series =
| publication-place =
| place =
| publisher = [[Manning Publications]]
| page = 528
| id =
| isbn = 978-1-933988-21-4
| doi =
| oclc =
| url =
| accessdate =
}}
{{Refend}}

==External links==
*[https://servicemix.apache.org ServiceMix website]
**[https://servicemix.apache.org/users-guide.html Apache User Guide]
**[https://servicemix.apache.org/documentation.html Apache documentation]
**[https://servicemix.apache.org/discussion-forums.html Apache Forums]
*[https://github.com/cmoulliard/karafee karafee]
*[https://web.archive.org/web/20140819083100/https://parleys.com/display/PARLEYS/Home#slide=2;talk=14123079;title=ServiceMix Javapolis 2007 Online ServiceMix Session]
*[http://camelone.org CamelOne Conference]
*[https://jboss.org JBoss community website]

{{Apache}}

{{DEFAULTSORT:Apache Servicemix}}
[[Category:Apache Software Foundation|ServiceMix]]
[[Category:Enterprise application integration]]
[[Category:Java enterprise platform]]
[[Category:Java platform]]
[[Category:Service-oriented architecture-related products]]</text>
      <sha1>80osr85a46ph4e7k5ot89ayv6ff30a4</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Camel</title>
    <ns>0</ns>
    <id>11335363</id>
    <revision>
      <id>833943727</id>
      <parentid>820593553</parentid>
      <timestamp>2018-04-03T03:48:25Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Tooling */ *</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4639">{{No footnotes|date=April 2015}}
{{Infobox Software
| name                   = Apache Camel
| logo                   = [[File:Apache-camel-logo.png|150px|Apache Camel Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version =  2.19.3
| latest release date    = {{Start date|2017|09|14}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Enterprise Integration Patterns]] [[Enterprise Service Bus]] [[Service-oriented architecture|SOA]] [[Message Oriented Middleware]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://camel.apache.org}}
}}
'''Apache Camel''' is an [[open source]] [[Framework-oriented design|framework]] for [[message-oriented middleware]] with a rule-based routing and [[data mediation|mediation]] engine that provides a [[Plain Old Java Object|Java object]]-based implementation of the [[Enterprise Integration Patterns]] using an [[application programming interface]] (or declarative Java [[domain-specific language]]) to configure routing and mediation rules. The domain-specific language means that Apache Camel can support type-safe smart completion of routing rules in an [[integrated development environment]] using regular Java code without large amounts of [[XML]] configuration files, though XML configuration inside [[Spring Framework]] is also supported.

Camel is often used with [[Apache ServiceMix]], [[Apache ActiveMQ]] and [[Apache CXF]] in [[service-oriented architecture]] projects.

==Tooling==

* Several Maven-plugins are provided for validation and deployment.
* Graphical, [[Eclipse (software)|Eclipse]]-based tooling is freely available from [[Red Hat]]. It provides graphical editing and debugging and advanced validation.
* Eclipse based tooling from [[Talend]].

==See also==
* AdroitLogic [[UltraESB]]
* [[Apache ServiceMix]]
* [[FUSE ESB]] (enterprise ServiceMix)
* [[Guaraná DSL]]
* [[Mule (software)]] is a FREE/GPL lightweight enterprise service bus (ESB) and integration framework integrated and complete that has graphical tooling.
* [[Open ESB]]
* [[Service Component Architecture]] (SCA)
* [[Petals ESB]]
* [[IBM Integration Bus]]
* [[Akka (toolkit)]]  open-source toolkit and runtime for [[Reactive programming]], concurrent and distributed applications on the JVM with camel integration.
* [[Ibis Adapter Framework]] opensource integration framework

==Books==

{{refbegin}}
* {{citation
| first1     = Claus
| last1      = Ibsen
| first2     = Jonathan
| last2      = Anstey
| date       = 2010
| title      = Camel in Action
| edition    = 1st
| publisher  = [[Manning Publications]]
| page      = 552
| isbn       = 978-1-935182-36-8
| url        =
}}
* {{citation
| first1     = Scott
| last1      = Cranton
| first2     = Jakub
| last2      = Korab
| date       = 2013
| title      = Apache Camel Developer's Cookbook
| edition    = 1st
| publisher  = [[Packt Publishing]]
| page      = 424
| isbn       = 978-1-78217-030-3
| url        = http://www.packtpub.com/apache-camel-developers-cookbook/book
}}
* {{citation
| first     = Bilgin
| last      = Ibryam
| date       = 2013
| title      = Instant Apache Camel Message Routing
| edition    = 1st
| publisher  = [[Packt Publishing]]
| page      = 62
| isbn       = 978-1-78328-347-7
| url        = http://www.packtpub.com/apache-camel-message-routing/book
}}
{{refend}}

==External links==
* [http://camel.apache.org/ Apache Camel Home]
* [http://camelone.org CamelOne Conference]
* [https://www.jboss.org/products/fuse.html JBoss Fuse based on Apache Camel]
* Methods &amp; Tools article - [http://www.methodsandtools.com/tools/tools.php?camel Camel, an Open Source Routing Framework]
* JDJ article - [http://opensource.sys-con.com/read/504392.htm SOA Made Easy with Open Source Apache Camel]
* [https://web.archive.org/web/20080915035805/http://wiki.open-esb.java.net/Wiki.jsp?page=CamelSE Apache Camel JBI Service Engine]
* [http://cwiki.apache.org/qpid/ Apache [[Apache_Qpid]]]
* [http://talend.com/ Talend ESB based on Apache Camel]
* [http://www.javavillage.in/apache-camel.php/ Apache Camel Examples]
{{apache}}


[[Category:Apache Software Foundation|Camel]]

{{business-software-stub}}
[[Category:Enterprise application integration]]
{{free-software-stub}}
[[Category:Java enterprise platform]]
[[Category:Java platform]]
[[Category:Message-oriented middleware]]
[[Category:Service-oriented architecture-related products]]
[[Category:Web scraping]]</text>
      <sha1>sj1p4e6n0rsexhm84mcmia4jsxl2wy9</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Excalibur</title>
    <ns>0</ns>
    <id>12082169</id>
    <revision>
      <id>701816070</id>
      <parentid>695980366</parentid>
      <timestamp>2016-01-26T19:18:36Z</timestamp>
      <contributor>
        <ip>50.126.125.240</ip>
      </contributor>
      <comment>Official URL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1574">{{ Infobox software
| name                   = Apache Excalibur
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 2.2.3
| latest release date    = {{release date|2007|07|05}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Inversion of control]] framework
| license                = [[Apache License]] 2.0
| website                = {{Official URL}}
}}

'''Apache Excalibur''' project produces a set of libraries for component based programming in the Java language. Its main products include the [[inversion of control]] (IoC) framework Avalon, an Avalon-based container named Fortress, and a set of Avalon compatible software components.

Excalibur spun out of the original Apache Avalon project following Avalon's closure in 2004. Since that time Apache Excalibur has hosted the Avalon framework and related source code.

'''''Excalibur project has been retired by Apache Software foundation and it has been moved to [[Apache Attic]]'''''

==See also==
* [[Apache Software Foundation]]
* [[Apache Cocoon]]

==External links==
* {{Official website}}
* [http://wiki.apache.org/excalibur/ExcaliburHistory History of Apache Avalon and Excalibur]
{{apache}}

[[Category:Apache Software Foundation|Excalibur]]
[[Category:Free software programmed in Java (programming language)]]


{{programming-software-stub}}</text>
      <sha1>1xizuujy5zgvrkalgpbqsmuda1qok1c</sha1>
    </revision>
  </page>
  <page>
    <title>Apache HiveMind</title>
    <ns>0</ns>
    <id>1885573</id>
    <revision>
      <id>843454685</id>
      <parentid>799911923</parentid>
      <timestamp>2018-05-29T07:56:23Z</timestamp>
      <contributor>
        <username>Nodulation</username>
        <id>14180121</id>
      </contributor>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2551">{{For|the Will Wright game|HiveMind}}
{{multiple issues|
{{Notability|Products|date=September 2011}}
{{primary sources|date=September 2011}}
}}

{{ Infobox Software
| name                   = HiveMind
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version = 1.1.1
| latest release date    = {{release date|2007|12|19}}
| latest preview version =  2.0-alpha1
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Inversion of Control]] (IoC) Container
| license                = [[Apache License]] 2.0
| website                = http://hivemind.apache.org/
}}
'''Apache HiveMind''' is an [[inversion of control]] (IOC) software project of the [[Apache Software Foundation]] written in [[Java (programming language)|Java]]. It takes the form of a services and configuration [[microkernel]].

In HiveMind, a service is an implementation of a Java [[interface (computer science)|interface]]. Unlike other [[service-oriented architecture]]s (SOAs), HiveMind is explicit about combining Java code within a single [[JVM]].

The HiveMind project, formerly a top-level Apache project, was retired 22 April 2009,&lt;ref&gt;http://www.mail-archive.com/announce@apache.org/msg00674.html&lt;/ref&gt;&lt;ref&gt;http://attic.apache.org/projects/hivemind.html&lt;/ref&gt; ending its life in the Apache Attic repository.  Its successor is considered to be Tapestry IOC [[Apache Tapestry|Tapestry IOC]], a [[Guice]]-like [[Inversion of control|IoC]] container for [[Java EE]] [[Model-view-controller]] web applications.
(Tapestry's requirement for IoC was the reason HiveMind was conceived in the first place.)

==See also==
*[[Spring Framework]]

==References==
{{reflist|30em}}

== External links ==
*[http://hivemind.apache.org/ HiveMind home page]
*[http://hivetranse.sourceforge.net/ HiveMind Utilities]
*[https://web.archive.org/web/20060911043028/http://wiki.apache.org/jakarta-hivemind/ HiveMind Wiki]
*[https://web.archive.org/web/20060821192235/http://howardlewisship.com/blog/  HiveMind Blog]
*[http://paulreed.com/2006/07/27/hivemind-helloworld/ HiveMind Hello World]
*[https://springmodules.dev.java.net/  Spring HiveMind integration]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

{{Apache}}

[[Category:Java platform]]
[[Category:Apache Software Foundation|HiveMind]]


{{Compu-soft-stub}}</text>
      <sha1>cicjtt5j29g11cucmv5j8m4r13cxxsf</sha1>
    </revision>
  </page>
  <page>
    <title>SuEXEC</title>
    <ns>0</ns>
    <id>12974560</id>
    <revision>
      <id>828854732</id>
      <parentid>825410317</parentid>
      <timestamp>2018-03-05T04:04:47Z</timestamp>
      <contributor>
        <ip>203.7.155.101</ip>
      </contributor>
      <comment>revert junk edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2609">{{lowercase}}
Apache '''suEXEC''' is a feature of the [[Apache HTTP Server|Apache]] [[Web server]]. It allows users to run [[Common Gateway Interface|CGI]] and [[Server Side Includes|SSI]] applications as a different user - normally, all web server processes run as the default web server user (often wwwrun, www-data, Apache or [[nobody (username)|nobody]]). The '''suEXEC''' feature consists of a module for the web server and a [[Executable|binary]] executable which acts as a wrapper. suEXEC was introduced in Apache 1.2 and is often included in the default Apache package provided by most [[Linux distribution]]s.

If a client requests a CGI and suEXEC is activated, it will call the suEXEC binary which then wraps the CGI scripts and executes it under the user account of the server process (virtual host) defined in the virtual host directive.&lt;ref name=huff /&gt;

Additionally, suEXEC perform a multi-step check on the executed CGI to ensure security for the server (including path-checks, a limit of permitted commands, etc.)&lt;ref&gt;[http://httpd.apache.org/docs/2.4/suexec.html apache.org - suEXEC Support]&lt;/ref&gt;

==Example==
User "Alice" has a website including some CGI scripts in her own public_html folder, which can be accessed by &lt;nowiki&gt;http://server/~alice&lt;/nowiki&gt;.

Bob now views Alice's webpage, which requires Apache to run one of these CGI scripts.

Instead of running all scripts as "wwwrun" (which results in the need that all scripts have to be readable and executable for the "wwwrun" group if the file is owned by that group or for all users otherwise), the scripts in /home/alice/public_html will be wrapped using suEXEC and run with Alice's user ID resulting in higher security and eliminating the need to make the scripts readable and executable for all users or everyone in the "wwwrun" group (instead only alice herself needs to be able to run the script).

== References==
{{reflist|refs=
&lt;ref name=huff&gt;{{cite web|last1=Miller|first1=Chris|title=Deploying PHP: mod_php? CGI/suExec? FastCGI?|url=http://www.huffingtonpost.com/cmiller/deploying-php-mod_pp-cgis_b_1824688.html|publisher=Huffington Post|accessdate=26 February 2015}}&lt;/ref&gt;
}}
==External links==
* [http://httpd.apache.org/docs/2.4/suexec.html apache.org - suEXEC Support (Apache 2.4)]
* [http://httpd.apache.org/docs/1.3/suexec.html apache.org - suEXEC Support (Apache 1.3)]

[[Category:Unix network-related software]]
[[Category:Apache Software Foundation|HTTP Server]]
[[Category:Apache httpd modules]]
[[Category:Computer security software]]
[[Category:Unix security-related software]]


{{security-software-stub}}</text>
      <sha1>r410yl70e83lq4hdvzdrw5ho25fjwmy</sha1>
    </revision>
  </page>
  <page>
    <title>WebWork</title>
    <ns>0</ns>
    <id>2316975</id>
    <revision>
      <id>812305644</id>
      <parentid>770260997</parentid>
      <timestamp>2017-11-27T04:51:37Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* WebWork lifecycle */ simplify headin</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6508">{{About|a web application framework||Webwork (disambiguation){{!}}Webwork}}
{{distinguish|Webwork Ponzi scam}}
{{refimprove|date=September 2015}}

{{ Infobox Software
| name                   = Webwork
| developer              = [[OpenSymphony]]
| latest release version = 2.2.6
| latest release date    = {{release date|2007|07|21}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)]]
| status                 = Unmaintained
| genre                  = [[Web application framework|Web Framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://www.opensymphony.com}}
}}
'''WebWork''' was a Java-based [[web application framework]] developed by [[OpenSymphony]] that merged into the current [[Struts2]] framework. It was designed to improve developer productivity and simplify code. WebWork was built on top of [[XWork]], which provided a generic [[command pattern]] framework as well as an [[Inversion of Control]] container.

WebWork provided support for building reusable UI templates, such as form controls, UI themes, [[internationalization]], dynamic form parameter mapping to [[JavaBeans]], client and server side validation, etc.

==Design goals and overview==
In a standard [[Java Platform, Enterprise Edition|Java EE]] web application, the client will typically submit information to the server via a [[form (web)|web form]]. The information is then either handed over to a [[Java Servlet]] which processes it, interacts with a database and produces an [[HyperText Markup Language|HTML]]-formatted response, or it is given to a [[JavaServer Pages]] (JSP)  document which intermingles HTML and Java code to achieve the same result. Both approaches are often considered inadequate{{by whom|date=January 2015}} for large projects because they mix application logic with presentation and make maintenance difficult.

WebWork's designers attempted to understand what they regarded as existing frameworks' limitations, and to work to eliminate them. It supports type conversion, continuations, and interceptors. WebWork also supports multiple-view technologies like JSP, velocity, and FreeMarker.

WebWork was designed and implemented with a specific set of goals:

*Web Designer never had to touch Java code
*Create multiple "Web Skins" for an application
*Change Look and Feel
*Change Layout on a given Web Page
*Change Flow among Web Pages
*Move *existing* data elements from one page to another
*Integrate with various backend infrastructures
*Reuse components
*Perform [[Internationalization and localization|internationalization (i18n)]] of a web application
*Keep the API small and to the point
*Ability to learn WebWork fast, by making all the fancier features optional
*Allow the developer to choose how to implement as much as possible, while providing default implementations that work well in most cases.&lt;ref&gt;[http://www.opensymphony.com/webwork_old/src/docs/manual/faq.html] {{webarchive |url=https://web.archive.org/web/20080310020613/http://www.opensymphony.com/webwork_old/src/docs/manual/faq.html |date=March 10, 2008 }}&lt;/ref&gt;

== Lifecycle ==

The architecture of WebWork was based on the [[Model-view-controller|MVC]] Framework, Command, and Dispatcher patterns and the principle of Inversion of Control. The life cycle of a WebWork request begins when the servlet container receives a new request. The new request is passed through a set of filters called the filter chain and sent to the FilterDispatcher. The FilterDispatcher forwards the request to the ActionMapper to determine what needs to be done with the request. If the request requires an action, it sends an ActionMapping object back to the FilterDispatcher. If not, ActionMapper returns a null object, indicating that no action needs to be taken. The FilterDispatcher forwards the request and the ActionMapper object to the ActionProxy for further action. The ActionProxy invokes the Configuration File manager to get the attributes of the action, which is stored in the xwork.xml file and creates an ActionInvocation object. The ActionInvocation object contains attributes like the action, invocation context, result, result code, etc. The configuration file manager has access to these configuration files and is used by the ActionProxy as a gateway to the configuration files. The ActionInvocation object also has information about Interceptors that need to be invoked after or before an action is executed.

ActionInvocation invokes all the interceptors listed in the ActionInvocation object and then invokes the actual action. When the action is completed, ActionInvocation gets the action result code from the execution. It uses the action result code to look up the appropriate result, which is usually a JSP page, a velocity template or a freemarker template associated with the result code. ActionInvocation also executes the interceptors again in the reverse order and returns the response as a HttpServletResponse.&lt;ref&gt;{{cite web|url=http://javaboutique.internet.com/reviews/webworks/ |title=Java(TM) Boutique - Review - WebWork: The New Framework on the Block |publisher=Javaboutique.internet.com |date=2006-03-24 |accessdate=2012-03-06 |deadurl=yes |archiveurl=https://web.archive.org/web/20120210054553/http://javaboutique.internet.com/reviews/webworks/ |archivedate=February 10, 2012 }}&lt;/ref&gt;

== WebWork / Struts merger ==

On November 27, 2005, WebWork developer Patrick Lightbody announced that WebWork would be merging in to [[Apache Struts|Struts]]2. [http://blogs.opensymphony.com/webwork/2005/11/webwork_joining_struts.html] While the next major release (WebWork 2.2.x) was released under the WebWork name, all future major revisions (namely, 2.3.x and beyond) would be folded into Struts2.

Ted Husted, developer of Struts said in an email that WebWork was very similar to Struts 1.x and did certain things better than Struts. Both Husted and Lightbody's rationale was that combining WebWork's technology with Struts' community would benefit both projects.

== License ==
WebWork uses the [[OpenSymphony Software License]] which is a modified (and fully [[License compatibility|compatible]] with) [[Apache Software License]].

==See also==
* [[Apache Struts]]
* [[JavaServer Faces]]
* [[Tapestry (programming)|Tapestry]]
* [[Struts2]]

== References ==
{{reflist|30em}}

{{Application frameworks}}

[[Category:Apache Software Foundation]]
[[Category:Web frameworks]]
[[Category:Java enterprise platform]]</text>
      <sha1>o9ap8kz0u1wm76ti6kbwir42xx7bwk5</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Harmony</title>
    <ns>0</ns>
    <id>1894247</id>
    <revision>
      <id>795149927</id>
      <parentid>788871825</parentid>
      <timestamp>2017-08-12T10:26:18Z</timestamp>
      <contributor>
        <ip>70.184.214.35</ip>
      </contributor>
      <comment>Alphabetized the categories.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29553">{{Use mdy dates|date=June 2015}}
{{Update|inaccurate=yes|article|date=February 2013}}
{{Infobox software
| name                   = Apache Harmony
| logo                   = [[File:Apache Harmony Logo.png|300px]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Discontinued&lt;ref&gt;{{cite news |url=http://www.h-online.com/open/news/item/The-end-of-Apache-Harmony-1371819.html |title=The end of (Apache) Harmony |publisher=The H |date=November 4, 2011}}&lt;/ref&gt;
| latest release version = 5.0M15&lt;br/&gt;6.0M3
| latest release date    = {{Start date and age|2010|09|15}}
| latest preview version =
| latest preview date    =
| operating system       = [[Microsoft Windows|Windows]], [[Linux]]
| programming language   = [[C++]], [[Java (programming language)|Java]]
| genre                  = [[Java Virtual Machine]], Java [[Library (computer science)|Library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|harmony.apache.org}}
}}
'''Apache Harmony''' is a retired [[open source]], [[free Java implementations|free Java implementation]], developed by the [[Apache Software Foundation]].&lt;ref&gt;{{cite web
| url=http://harmony.apache.org/faq.html
| title=Original FAQ Questions from Project Launch
| publisher=harmony.apache.org
|quote=''We are starting with Java SE 5, as that is the first version of Java SE for which the licensing allows an open source implementation, but we'll continue with Java SE 6 and any subsequent versions that follow.''
| accessdate=2011-02-27}}&lt;/ref&gt; It was announced in early May 2005 and on October 25, 2006, the Board of Directors voted to make Apache Harmony a top-level project. The Harmony project achieved (as of February 2011) 99% completeness for [[J2SE 5.0]], and 97% for [[Java SE 6]].&lt;ref name="completeness"&gt;{{cite web
| url=http://harmony.apache.org/subcomponents/classlibrary/status.html
| title=Class Library Component Status
| publisher=harmony.apache.org
| accessdate=2011-02-27}}&lt;/ref&gt; The [[Android (operating system)|Android]] operating system has historically been a major user of Harmony, although since [[Android Nougat]] it increasingly relies on [[OpenJDK]] libraries.&lt;ref name=ars-njdk&gt;{{cite web|title=Android N switches to OpenJDK, Google tells Oracle it is protected by the GPL|url=https://arstechnica.com/tech-policy/2016/01/android-n-switches-to-openjdk-google-tells-oracle-it-is-protected-by-the-gpl/|first=Ron|last=Amadeo|date=January 7, 2016|website=[[Ars Technica]]|publisher=[[Condé Nast]]|accessdate=July 1, 2016}}&lt;/ref&gt;

On October 29, 2011 a vote was started by the project lead Tim Ellison whether to retire the project. The outcome was 20 to 2 in favor,&lt;ref name="atticproposal"&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/harmony-dev/201111.mbox/browser |archiveurl=http://old.nabble.com/-Result--Move-Apache-Harmony-to-the-Attic-%28updated%29-p32772228.html |title=Move Apache Harmony to the Attic (updated)|first=Tim |last=Ellison |publisher=org.apache.harmony.dev |archivedate=2011-11-03 |date=2011-11-03}}&lt;/ref&gt; and the project was retired on November 16, 2011.&lt;ref name="retired"&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/harmony-dev/201111.mbox/browser |archiveurl=http://old.nabble.com/Board-accepted-attic-resolution-td32862837.html |title=Board accepted attic resolution |first=Tim |last=Ellison |publisher=org.apache.harmony.dev |date=2011-11-16 |archivedate=2011-11-16 |accessdate=2011-11-27}}&lt;/ref&gt;

== History ==

=== Initiation ===
The Harmony project was initially conceived as an effort to unite all developers of the [[free Java implementations]]. Many [[software developer|developers]] expected that it would be the project above{{clarify|date=November 2014}} the [[GNU]], [[Apache Software Foundation|Apache]] and other communities. GNU developers were invited into and participated during the initial, preparatory planning.&lt;ref&gt;{{cite web| url=http://article.gmane.org/gmane.comp.java.classpath.devel/5521| title=Harmony!| author=Mark Wielaard| date=2005-05-09| quote=''Apache has set up a proposal for discussion around a full free j2se implementation. Which they call "Harmony". This is (at the moment) not about code, but about finding out a direction for getting to such a beast. Dalibor, Tom, Jeroen and I were asked to help them in that discussion and possibly show them how to set up a good architecture for it.''}}&lt;/ref&gt;

=== Incompatibility with GNU Classpath ===
Despite the impression given by the preparatory planning, it was decided not to use the code from [[GNU Classpath]], and that Harmony would use an incompatible license; therefore blocking the collaboration between Harmony and existing free Java projects.&lt;ref&gt;{{cite web| url=https://lwn.net/Articles/135111/| title=A proposal for a free Java implementation| author=Geir Magnusson Jr.| date=2006-05-24| publisher=[[Apache Software Foundation|Apache]]}}&lt;/ref&gt; Apache developers would then [[rewrite (programming)|write the needed classes from scratch]] and expect necessary large code donations from [[software company|software companies]]. Various misunderstandings at the start of the project, and the fact that major companies like [[IBM]] proposed to give large amount of existing code, created some confusion in the free Java community about the real objectives of the project.&lt;ref name="towardfree"&gt;{{cite web| url=https://lwn.net/Articles/184967/| title=Toward a free Java| author=Mark Wielaard| date=2006-05-24| publisher=[[LWN.net]]|quote=''All this means that, despite the fact that there is now some code available donated by Intel, there is no practical cooperation between the original free software projects backing Harmony and the project now known as Apache Harmony. All this made some people think of Harmony as a company consortium in the guise of an ASF project and not a full community project.''}}&lt;/ref&gt;

One major point of incompatibility between the GNU Classpath and Apache Harmony projects was their incompatible licenses: Classpath's [[GNU General Public License]] with the [[GPL linking exception|linking exception]] versus Harmony's [[Apache License]].&lt;ref name=towardfree/&gt;

=== Difficulties to obtain a TCK license from Sun ===
{{See also|Technology Compatibility Kit}}

On April 10, 2007, the [[Apache Software Foundation]] sent an [[open letter]] to [[Sun Microsystems]] [[Chief executive officer|CEO]], [[Jonathan I. Schwartz|Jonathan Schwartz]] regarding their inability to acquire an acceptable license for the Java SE 5 [[Technology Compatibility Kit]] (TCK), a test kit needed by the project to demonstrate compatibility with the Java SE 5 specification, as needed by the [[Sun Microsystems|Sun]] specification license for Java SE 5.&lt;ref&gt;{{cite web|url=http://www.apache.org/jcp/sunopenletter.html|title=Open Letter to Sun Microsystems - JCK|publisher=apache.org}}&lt;/ref&gt; What makes the license unacceptable for [[Apache Software Foundation|ASF]] is the fact that it imposes rights restrictions through limits on the "field of use" available to users of Harmony, not compliant with the [[Java Community Process]] rules.&lt;ref&gt;According to ASF, 1) a specification lead cannot ''impose any contractual condition or covenant that would limit or restrict the right of any licensee to create or distribute such Independent Implementations'' (section 5.C.III), and 2) ''a specification lead must license all necessary IP royalty-free to any compatible implementation of a specification'' (section 5.B).&lt;/ref&gt;

Sun answered on a company blog&lt;ref&gt;{{cite web|url=http://blogs.sun.com/ontherecord/ |title= On the Record |publisher= |deadurl=yes |archiveurl=https://web.archive.org/web/20070421103127/http://blogs.sun.com/ontherecord/ |archivedate=April 21, 2007 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://java.sys-con.com/read/360602.htm|title=Sun to Apache: "Open Source Process is a Journey" - Java IoT|publisher=}}&lt;/ref&gt; that it intended to create an open source implementation of the Java platform under [[GPL]], including the TCK, but that their current priority was to make the [[Java (software platform)|Java Platform]] accessible to the [[Linux|GNU/Linux]] community under [[GPL]] as quickly as possible.

This answer triggered some reactions, either criticizing [[Sun Microsystems|Sun]] for not responding "in a sufficiently open manner" to an open letter,&lt;ref&gt;{{cite web|url=http://ianskerrett.wordpress.com/2007/04/16/the-silence-from-an-open-sun/|title=The Silence from an Open Sun|work=Ian Skerrett}}&lt;/ref&gt; or rather [[Apache Software Foundation]]; some think that ASF acted unwisely to aggressively demand something they could have obtained with more diplomacy from Sun, especially considering the timescale of the opening class library.&lt;ref&gt;{{cite web|url=http://gnu.wildebeest.org/diary/2007/04/21/openjck/|title=Mark J. Wielaard|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://jroller.com/page/dgilbert?entry=five_reasons_why_apache_will
| title=Five Reasons Why Apache Will Regret That Open Letter
| last=Gilbert|first=Dave
| date=2007-04-16
| accessdate=2008-03-08}}&lt;/ref&gt;

Sun released the [[OpenJDK]] in May 2007, along with a specific license to allow to run the TCK in the OpenJDK context for any [[GNU General Public License|GPL]] implementation deriving substantially from OpenJDK.&lt;ref&gt;{{cite web
| url=http://openjdk.java.net/legal/openjdk-tck-license.pdf
| title=OpenJDK Community TCK License Agreement V 1.1
| publisher=[[Sun Microsystems]]
|quote=''Subject to and conditioned upon its Licensee Implementation being substantially derived from OpenJDK Code and, if such Implementation has or is to be distributed to a third party, its being distributed under the GPL License, Sun hereby grants to Licensee, to the extent of Sun's Intellectual Property Rights in the TCK, a worldwide, personal, non-exclusive, non-transferable, limited license to use the TCK internally and solely for the purpose of developing and testing Licensee Implementation.''
| accessdate=2008-03-08|format=PDF}}&lt;/ref&gt; This does not cover Apache Harmony, which is not GPL-licensed. On December 9, 2010, the Apache Software Foundation resigned from the Java Community Process Executive Committee,&lt;ref&gt;{{cite web | url=https://blogs.apache.org/foundation/entry/the_asf_resigns_from_the | title=The ASF Resigns From the JCP Executive Committee | publisher=Apache Software Foundation}}&lt;/ref&gt; in protest over the difficulty in obtaining a license acceptable to Apache for use with the Harmony project.&lt;ref&gt;{{Cite web | url=https://arstechnica.com/open-source/news/2010/12/apache-resigns-from-jcp-in-protest-of-oracle-governance-failures.ars | title=Apache quits Java governance group in protest of Oracle abuses | publisher=Ars Technica }}&lt;/ref&gt;

=== Use in Android SDK ===
The virtual machine that is used in [[Google]]'s [[Android (operating system)|Android]] platform ([[Dalvik virtual machine|Dalvik]] up to version 4.4, and its successor, [[Android Runtime]] (ART)) uses a subset of Harmony for the core of its [[Java Class Library|Class Library]].&lt;ref&gt;{{cite web
| url=http://www.infoq.com/news/2007/11/android-java
| title=Google's Android SDK Bypasses Java ME in Favor of Java Lite and Apache Harmon
| publisher=infoq.com
| date=2007-11-12
| quote=''Instead of providing a full version of the Java SE or Java ME Google has diverged on two fronts. First, a limited subset of the core Java packages is provided. (...) By going this route Android is following in the footsteps of another Google project GWT which uses Java as its development language but does not support the full JDK.''
| accessdate=2009-05-31}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://developer.android.com/reference/packages.html
| title=Package Index
| publisher=[[Open Handset Alliance]]
| accessdate=2009-05-31| archiveurl= https://web.archive.org/web/20090627025423/http://developer.android.com/reference/packages.html| archivedate= June 27, 2009 | deadurl= no}}&lt;/ref&gt; However, Dalvik does not align to [[Java Platform, Standard Edition|Java SE]] nor [[Java Platform, Micro Edition|Java ME]] [[Java Class Library|Class Library]] profiles (for example [[Java Platform, Micro Edition|J2ME]] classes, [[Abstract Window Toolkit|AWT]] and [[Swing (Java)|Swing]] are not supported).

[[Android Nougat|Android 7.0 "Nougat"]] replaced Harmony with [[OpenJDK]].&lt;ref name=ars-njdk/&gt;

===  End of the project ===
On October 11, 2010, [[IBM]], by far the biggest participant in the project, decided to join [[Oracle Corporation|Oracle]] on the [[OpenJDK]] project, effectively shifting its efforts from Harmony to the [[Oracle Corporation|Oracle]] reference implementation.&lt;ref&gt;{{cite web
| url=http://www.marketwire.com/press-release/Oracle-and-IBM-Collaborate-to-Accelerate-Java-Innovation-Through-OpenJDK-NASDAQ-ORCL-1332855.htm
| title=Oracle and IBM Collaborate to Accelerate Java Innovation Through OpenJDK
| publisher=[[Oracle Corporation]]
| accessdate=2010-10-22| archiveurl= https://web.archive.org/web/20101014150141/http://www.marketwire.com/press-release/Oracle-and-IBM-Collaborate-to-Accelerate-Java-Innovation-Through-OpenJDK-NASDAQ-ORCL-1332855.htm| archivedate= October 14, 2010 | deadurl= no}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=https://arstechnica.com/open-source/news/2010/10/ibm-joins-openjdk-as-oracle-shuns-apache-harmony.ars
| title=Java wars: IBM joins OpenJDK as Oracle shuns Apache Harmony
|author=Ryan Paul
| publisher=[[Ars Technica]]
| accessdate=2010-10-22| archiveurl= https://web.archive.org/web/20101019094951/http://arstechnica.com/open-source/news/2010/10/ibm-joins-openjdk-as-oracle-shuns-apache-harmony.ars| archivedate= October 19, 2010 | deadurl= no}}&lt;/ref&gt; Bob Sutor, IBM's head of Linux and open source, blogged that "IBM will be shifting its development effort from the Apache Project Harmony to OpenJDK".&lt;ref&gt;{{cite web
| url=http://www.sutor.com/c/2010/10/ibm-joins-the-openjdk-community/
| title=IBM joins the OpenJDK community, will help unify open source Java efforts
|author=Bob Sutor
|quote=''IBM will be shifting its development effort from the Apache Project Harmony to OpenJDK. For others who wish to do the same, we’ll work together to make the transition as easy as possible. IBM will still be vigorously involved in other Apache projects.''
| accessdate=2010-10-22| archiveurl= https://web.archive.org/web/20101018160132/http://www.sutor.com/c/2010/10/ibm-joins-the-openjdk-community/| archivedate= October 18, 2010 | deadurl= no}}&lt;/ref&gt; On March 2011, [[IBM]]'s Tim Ellison announced that he resigned as Project Management Chair for Harmony&lt;ref&gt;{{cite web
| url=http://harmony.markmail.org/message/ah5f42h4p2bhub6o
| title=Rebooting the Harmony project
| author=Tim Ellison
| date=2011-03-14
| accessdate=2011-03-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://www.infoq.com/news/2011/03/apache-harmony
| title=What is the Future of Apache Harmony?
| publisher=infoq.com
| date=2011-03-14
| accessdate=2011-03-20}}&lt;/ref&gt; After IBM's disengagement, the project's activity level greatly declined.&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/harmony-dev/|title=Mailing list archives: dev@harmony.apache.org|publisher=apache.org|accessdate=2011-09-11}}&lt;/ref&gt; 

On October 29, 2011, a poll was started on the harmony-dev mailing list by the project lead Tim Ellison whether to retire the project. The outcome on November 3, was 20 to 2 in favor of retirement.&lt;ref name="atticproposal" /&gt;  On November 16, 2011, the Apache Software Foundation board of directors passed a motion to terminate the project.&lt;ref name=ASFBODminutes&gt;{{cite web | url=http://www.apache.org/foundation/records/minutes/2011/board_minutes_2011_11_16.txt | title=Board of Directors Meeting Minutes | publisher=Apache Software Foundation | date=November 16, 2011 | accessdate=August 8, 2012 | quote=Larry wishes it to be noted that he is not against termination, he simply would have preferred to have more time to work on the messaging.}}&lt;/ref&gt;   One director,  [[Lawrence Rosen (attorney)|Larry Rosen]], cast a "no" vote, based on the timing rather than the merits of the proposal; it was otherwise unanimous.&lt;ref name=ASFBODminutes/&gt;  The project was retired on November 16, 2011.&lt;ref name="retired" /&gt;

== Development team ==
At the start, Apache Harmony received some large code contributions from several companies. Development discussions have taken place on open mailing lists.  Later, the Apache Software foundation mentors put a lot of effort
into bringing the development process more in line with "the Apache way,"&lt;ref&gt;{{cite web| url=http://article.gmane.org/gmane.comp.java.harmony.devel/10742| title=We would like all the development to happen right here|author=Leo Simons| date=2006-07-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://incubator.apache.org/learn/theapacheway.html| title=The Apache Way}}&lt;/ref&gt; and it seemed that their efforts were successful.

== Last development status ==
Apache Harmony was accepted among the official Apache projects on October 29, 2006.

=== Architecture ===
&lt;!--  Commented out because image was deleted: [[Image:DRL structure.gif|300px|thumb|Major Dynamic Runtime Layer Components and their interfaces]] --&gt;
The Dynamic Runtime Layer virtual machine consists of the following components:

#'''The VM core:''' with its subcomponents concentrates most of the JVM control functions.
#'''The porting layer''': hides platform-specific details from other VM components behind a single interface and is based on the [[Apache Portable Runtime]] layer.
#'''The [[garbage collection (computer science)|garbage collector]]''': allocates Java objects in the heap memory and reclaims unreachable objects using various algorithms
#'''Execution Manager''': selects the execution engine for compiling a method, handles profiles and the dynamic recompilation logic.
#'''Class Library''':  is a Java standard library.
#'''The thread manager''' that handle operating system threading
#'''The execution engine:''' consists of the following:
##The [[just-in-time compiler]] for compilation and execution of method code.
##The [[interpreter (computing)|interpreter]] for easier debugging.

=== Support platform and operating system ===
The project provided a portable implementation that ease development on many platforms and operating systems. The main focus was on [[Microsoft Windows|Windows]] and [[Linux]] operating systems on x86 and x86-64 architectures.&lt;ref&gt;{{cite web|url=http://harmony.apache.org/supported_platforms.html|title=Apache Harmony - Supported Platforms|author=Harmony Documentation Team|publisher=harmony.apache.org}}&lt;/ref&gt;
{| class="wikitable"
|-
!
! [[Windows 2000]]
! [[Microsoft Windows|Windows]] [[Windows XP|XP]], [[Windows Server 2003|Server 2003]], [[Windows Vista|Vista]]
! [[Linux]] [[RHEL]], [[SUSE Linux Enterprise Server|SLES]], [[Debian]], [[Gentoo Linux|Gentoo]], [[Fedora (operating system)|Fedora]]
! [[FreeBSD]]
! [[AIX]]
! [[Mac OS X]]
|-
| '''[[IA-32]] (Pentium III or better)'''
| {{No}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{n/a}}
| {{n/a}}
|-
| '''[[x86-64]] (Intel 64, AMD64)'''
| {{n/a}}
| {{Yes}}
| {{Yes}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| '''[[Itanium]] (IA64, IPF)'''
| {{n/a}}
| {{No}}
| {{Yes}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| '''[[PowerPC|PowerPC 32-bit]]'''
| {{n/a}}
| {{n/a}}
| {{No}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| '''[[ppc64|PowerPC 64-bit]]'''
| {{n/a}}
| {{n/a}}
| {{No}}
| {{n/a}}
| {{No}}
| {{n/a}}
|-
| '''[[IBM System z|zSeries]] 31-bit'''
| {{n/a}}
| {{n/a}}
| {{No}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|}

=== Class library coverage ===
The expected donations from software companies were actually received. The Apache Harmony now contains the working code, including the [[Swing (Java)|Swing]], [[Abstract Window Toolkit|AWT]] and [[Java 2D]] code which were contributed by [[Intel]].

The Harmony project currently achieve (as of February 2011)  99% completeness for [[J2SE 5.0|JDK 5.0]], and 97% for [[Java SE 6]].&lt;ref name=completeness/&gt;

The progress of the Apache Harmony project can be tracked against J2SE 1.4 and Java SE 5.0.&lt;ref&gt;{{cite web|url=http://people.apache.org/~chunrong/latest-harmony-japi.html|title=Results of comparison between JDK_5.0 and Harmony_5.0|publisher=}}&lt;/ref&gt; Also, there is a branch for Harmony v6.0 in development for Java SE 6.0.

Apache Harmony developers integrate several existing, field-tested open-source projects to meet their goal (not [[reinventing the wheel]]). Many of these projects are mature and well known and other parts of the library needed to be written from scratch.

This is a list of existing open source components that are used in the Apache Harmony project; some of them were in use before the project started.

{| class="wikitable"
! Component
! Description
|-
| [[International Components for Unicode|ICU]]
| Mature C/C++ and Java libraries for [[Unicode]] support and [[Internationalization and localization|software internationalization and globalization]]
|-
| [[Apache Xalan]]
| [[XSLT]] stylesheet processor for [[Java (programming language)|Java]], [[C++]] which implements [[XPath]] language
|-
| [[Apache Xerces]]
| [[XML parser]] library for Java, C++, [[Perl]]
|-
| [[Apache Portable Runtime]]
| [[Cross-platform]] abstraction library, provides platform independence
|-
| [[Apache CXF]]
| Robust, high performance [[Web service]]s framework work over protocols such as [[SOAP]], XML/HTTP, [[Representational State Transfer|RESTful]] HTTP, [[CORBA]]
|-
| [[Byte Code Engineering Library|BCEL]]
| Libraries to decompose, modify, and recompose binary Java classes, i.e., [[bytecode]]
|-
| [[MX4J]]
| [[Java Management Extensions]] (JMX) tools to manage and monitor applications, system objects, devices and service-oriented networks
|-
| [[Jikes RVM#VM Magic|VM Magic]]
| Set of extensions to Java language to facilitate systems programming in Java by adding direct memory operations, etc.
|-
| [[Bouncy Castle (cryptography)|Bouncy Castle]]
| Libraries collection of lightweight cryptography for Java and [[C Sharp (programming language)|C#]]
|-
| [[ANTLR]]
| Language tool, provides a framework to construct recognizers, interpreters, compilers, and translators from grammatical descriptions containing actions in many target languages
|}

=== Documentation ===
Harmony is currently less documented than the alternative free Java implementations. For instance, in GNU Classpath every method of the central [[CORBA]] class (ORB) has the explaining comment both in the standard abstract API class&lt;ref&gt;http://cvs.savannah.gnu.org/viewcvs/*checkout*/classpath/org/omg/CORBA/ORB.java?rev=1.2.2.12&amp;root=classpath&lt;/ref&gt; and implementation.&lt;ref&gt;http://cvs.savannah.gnu.org/viewcvs/*checkout*/classpath/gnu/CORBA/OrbFunctional.java?rev=1.6&amp;root=classpath&lt;/ref&gt; In the [http://incubator.apache.org/yoko/ Yoko] project, used by Harmony,&lt;ref&gt;{{cite web|url=http://www.mail-archive.com/yoko-dev@incubator.apache.org/msg01428.html|title=Harmony uses latest Yoko jars...|date=September 27, 2006|publisher=}}&lt;/ref&gt; most methods both in the standard declaration&lt;ref&gt;http://svn.apache.org/repos/asf/incubator/yoko/trunk/yoko-spec-corba/src/main/java/org/omg/CORBA/ORB.java&lt;/ref&gt; and implementing class&lt;ref&gt;http://svn.apache.org/repos/asf/incubator/yoko/trunk/core/src/main/java/org/apache/yoko/orb/OBCORBA/ORB_impl.java&lt;/ref&gt; were undocumented at the end of October 2006. Also, GNU Classpath supported both older and current CORBA features (same as Sun's implementation). Harmony, differently, left the central method of the older standard (&lt;code&gt;ORB.connect(Object)&lt;/code&gt;) fully unimplemented.

=== Tools ===
A complete implementation of the Java platform also needs a [[compiler]] that translates Java source code into [[bytecode]]s, a program that manages [[JAR (file format)|JAR files]], a [[debugger]], and an [[applet]] viewer and [[web browser]] [[Plug-in (computing)|plugin]], to name a few. Harmony currently has the [[Java compiler|compiler]], [[appletviewer]], jarsigner, javah, javap, [[keytool]], policytool, and [[unpack200]].&lt;ref&gt;{{cite web|url=http://incubator.apache.org/harmony/roadmap.html#General|title=Apache Harmony - Project Roadmap}}&lt;/ref&gt;

=== Virtual machine support ===
Harmony currently has seven [[virtual machine]] implementations that run Harmony Class Library, all of which were donations by external groups:

* JC Harmony Edition VM, "JCHEVM," based on the [[JC virtual machine|JCVM's]] [[interpreter (computing)|interpreter]], contributed by the author, Archie Cobbs.
* BootJVM, a simple [[bootstrapping (computing)|bootstrapping]] virtual machine, contributed by Daniel Lydick.
* [[SableVM]], an advanced, portable interpreter, contributed by authors from the [[Sable Research Group]]; and the Dynamic Runtime Layer Virtual Machine.
* [[DRLVM]], a [[just-in-time compiler]] contributed by [[Intel]].
* BEA announced the availability of an evaluation version of JRockit VM running Apache Harmony Class Library.&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/harmony-dev/200701.mbox/%3C1D7F0297-B0C5-4B05-AD27-B457B309C425@pobox.com%3E|title=[general] JRockit H27.2.1 for Harmony Class Library released now|publisher=}}&lt;/ref&gt;
* [[JikesRVM]], an open-source [[meta-circular evaluator|meta-circular]] JVM that use the Apache Harmony Class Library.&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/harmony-dev/200808.mbox/%3cb1e4cffb0808070820u2554f4faw4d98aa5059b5b425@mail.gmail.com%3e|title=Announcing Jikes RVM 3.0 + Apache Harmony!|publisher=}}&lt;/ref&gt;
* [[Ja.NET SE]], an open source project providing a Java 5 JDK (class libraries, tools, etc.) that run on the [[.NET Framework]] CLR. Ja.NET SE is based on the Apache Harmony Class Libraries.&lt;ref&gt;{{cite web|url=http://www.janetdev.org/|title=Janet Dev|work=Janet Dev}}&lt;/ref&gt;
In the end of November 2006, the language support provided by these virtual machine was still incomplete, and the build instructions recommended to use [[IBM]]'s [[proprietary software|proprietary]] J9 instead to run the class library test suite. However, this is not necessary anymore (as of July 2007).

As for the rest of the project, DRLVM [[virtual machine]] development has now stalled (as of May 2011).&lt;ref name="jiracommits"&gt;{{cite web
| url=https://issues.apache.org/jira/browse/HARMONY#selectedTab=com.atlassian.jira.plugin.ext.subversion%3Asubversion-project-tab
| title=Subversion Commits
| publisher=harmony.apache.org
| accessdate=2011-05-28}}&lt;/ref&gt;

==Application status==
Since its conception, Harmony grew in its ability to execute non-trivial Java applications.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/harmony/Application_Status|title=Application_Status|publisher=apache.org}}&lt;/ref&gt; {{as of|2007|7}}, supported applications include:

* [[Eclipse (software)|Eclipse]]: 99.3% of the 36000 [[reference implementation (computing)|reference implementation]] (RI) test pass on Harmony's DRLVM + class library.&lt;ref&gt;http://wiki.apache.org/harmony/Eclipse_Unit_Tests_Pass_on_DRLVM#PassRate_2007&lt;/ref&gt;
* [[Apache Tomcat]]: 100% of the RI tests pass.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/harmony/Apache_Tomcat|title=Apache_Tomcat|publisher=apache.org}}&lt;/ref&gt;
* [[JUnit]]: 100% of the RI tests pass.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/harmony/JUnit|title=JUnit|publisher=apache.org}}&lt;/ref&gt;
* [[Apache Ant]]: 97% of the RI tests pass.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/harmony/Apache_Ant|title=Apache_Ant|publisher=apache.org}}&lt;/ref&gt;
* Other applications pass with a high success rate, such as [[Apache Derby]], [[Apache Axis]], [[Log4j]], [[Apache Velocity]], [[Apache Cocoon]], [[jEdit]], and [[Apache Commons]].

However, Harmony's incomplete library prevented it from launching some other applications:

* [[ArgoUML]]: because it needs a [[Java applet]] implementation, which was still unavailable in Harmony.
* [[Apache Geronimo]] runs on Apache Harmony with some issues and workarounds.&lt;ref&gt;{{cite web|url=http://cwiki.apache.org/confluence/display/GMOxDOC20/Apache+Harmony|title=Apache Harmony|publisher=apache.org}}&lt;/ref&gt;
* [[Vuze]], formerly Azureus, because of unimplemented security classes.

== See also ==
{{Portal|Free software|Java}}

* [[GNU Classpath]]
* [[List of Java virtual machines]]
* [[Free Java implementations]]
* [[Java Class Library]]
* [[OpenJDK]]
* [[IcedTea]]

== References ==
{{Reflist|30em}}

== External links ==
* {{official website}}
* [http://mail-archives.apache.org/mod_mbox/incubator-general/200505.mbox/%3CE3603144-2C26-4C31-896D-6CC7445A63EB@apache.org%3E Apache Harmony FAQ]
* [http://svn.apache.org/viewvc/harmony/?root=Apache-SVN Apache Harmony source code repository]
* [http://developers.sun.com/learning/javaoneonline/2006/coreplatform/TS-3752.html JavaOne 2006 Online Harmony Session]
* [http://developers.sun.com/learning/javaoneonline/j1sessn.jsp?sessn=TS-7820&amp;yr=2007&amp;track=6 JavaOne 2007 Online Harmony Session]
* [http://parleys.com/display/PARLEYS/Apache+Harmony?showComments=true Apache Harmony] by Geir Magnusson Jr at JavaPolis 2006
* [http://www.osnews.com/story.php?news_id=10806 The Java open source debate - a good summary of the debate]
* [http://www.infoq.com/news/2011/03/apache-harmony What is the Future of Apache Harmony?]
* [http://www.h-online.com/open/news/item/Apache-Harmony-loses-project-manager-1210343.html Apache Harmony loses project manager]

{{Java Virtual Machine}}
{{Java (Sun)}}
{{apache}}

[[Category:Apache Software Foundation|Harmony]]
[[Category:Java (programming language)]]
[[Category:Java (programming language) libraries]]
[[Category:Java virtual machine]]
[[Category:Software using the Apache license]]</text>
      <sha1>fda1r9kt6rjnni2seam4hnz8erkzxfu</sha1>
    </revision>
  </page>
  <page>
    <title>Chainsaw (log file viewer)</title>
    <ns>0</ns>
    <id>14109962</id>
    <revision>
      <id>753497550</id>
      <parentid>737056692</parentid>
      <timestamp>2016-12-07T14:57:49Z</timestamp>
      <contributor>
        <ip>93.184.187.123</ip>
      </contributor>
      <comment>/* Open Source License */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2694">{{ Infobox Software
| name                   =  Apache Chainsaw
| logo                   = &lt;!-- Missing image removed: [[Image:Apache Chainsaw Logo.png|150px|Apache Chainsaw]]  --&gt;
| screenshot             = &lt;!-- Missing image removed: [[Image:Apache Chainsaw.png|250px]] --&gt;
| caption                = Demonstration of the Apache Chainsaw GUI
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Inactive
| latest release version = 2.0
| latest release date    = {{release date|2006|03|02}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = GUI tool to view and analyze log files
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = http://logging.apache.org/chainsaw/
}}
'''Chainsaw''' is a java-based GUI [[software]] tool to view and analyze [[computer]] log files - specifically logs generated by the [[Log4j]] logging system. Both Log4j and Chainsaw are [[Open source]] projects under [[Apache Software Foundation]]. The latest release is Chainsaw v2.&lt;ref&gt;{{cite web|url=http://logging.apache.org/chainsaw/|title=Apache Chainsaw v2 Download' |accessdate=2008-01-30}}&lt;/ref&gt; Chainsaw can read log files formatted in Log4j's &lt;code&gt;[http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/xml/XMLLayout.html XMLLayout]&lt;/code&gt;, receive events from remote locations, read events from a DB, or work with JDK 1.4 logging events.

==License==
The project is licensed under Apache License, V 2.0. Dependencies may not follow the same licensing.&lt;ref&gt;{{cite web|url=http://logging.apache.org/chainsaw/license.html|title=Apache Chainsaw Project License' |accessdate=2008-01-30}}&lt;/ref&gt;

==Alternatives==
===Open Source License===
* [http://glogg.bonnefon.org/ glogg]
* [http://sourceforge.net/projects/lilith/ Lilith]
* [http://logexpert.codeplex.com/ LogExpert]
* [http://code.google.com/p/otroslogviewer/ OtrosLogViewer]
* [https://github.com/couchcoding/Logbert/ Logbert]

===Commercial License===
* [https://www.baremetalsoft.com/baretail/index.php BareTail]
* [http://www.logmx.com/ LogMX]
* [http://www.logviewplus.com/ LogViewPlus]
* [http://insightextensions.codeplex.com/ ReflectInsight Log Viewer]

== References ==
{{reflist}} 

== External links ==
* [http://logging.apache.org/chainsaw/ Apache Logging Services Chainsaw v2 Home]
* [http://logging.apache.org/chainsaw/apidocs/ JavaDoc API reference for Chainsaw v2]

{{apache}}

[[Category:Apache Software Foundation]] 
[[Category:Java (programming language)]]</text>
      <sha1>jo9dlqi6e5dmpgdiyd4juuwgp1u2xgg</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Synapse</title>
    <ns>0</ns>
    <id>17911473</id>
    <revision>
      <id>814681334</id>
      <parentid>791339796</parentid>
      <timestamp>2017-12-10T07:44:18Z</timestamp>
      <contributor>
        <username>Vanjikumaran</username>
        <id>16346774</id>
      </contributor>
      <minor/>
      <comment>Changed the stable version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3882">{{expert|Software|reason=reads like an advertisement and not like an informative, educational article}}
{{ Infobox Software
| name                   = Apache Synapse
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest_release_version = 3.0.1
| latest_release_date    = {{release_date|2017|12|07}}
| latest_preview_version = 
| latest_preview_date    = 
| operating_system       = [[Cross-platform]]
| programming_language   = [[Java (programming language)|Java]]
| genre                  = [[enterprise service bus|Enterprise Service Bus]] 
| license                = [[Apache License]] 2.0
| website                = http://synapse.apache.org
}}
[[Apache Software Foundation|Apache]] '''Synapse''' is a simple, lightweight and high performance open source [[enterprise service bus]] (ESB) and mediation engine. It began incubation at the [[Apache Software Foundation]] on August 22, 2005,&lt;ref&gt;[http://incubator.apache.org/projects/index.html Apache Incubator]&lt;/ref&gt; and became a sub project of the Apache Web Services project on January 2, 2007. After implementing extensive support for [[Legacy system|legacy systems]] integration, it moved to a Top Level Project of the Apache Software Foundation on the February 5, 2008.&lt;ref&gt;[http://apache.sys-con.com/read/496070.htm TLP Graduation]&lt;/ref&gt; Apache Synapse is released under the [[Apache License]].

Synapse supports the creation of Proxy Services, which allows users to create virtual services on the ESB layer to front existing services. Existing services may be [[SOAP]], POX/REST services over [[HTTP]]/[[HTTPS|S]], as well as SOAP or legacy services over [[Java Message Service|JMS]], Apache VFS file systems (e.g. s/ftp, file, zip/tar/gz, webdav, SMB, etc.), Mail systems (e.g. pop3, imap, smtp), [[Financial Information eXchange]] (FIX), [[Hessian (web service protocol)|Hessian]], [[Advanced Message Queuing Protocol|AMQP]] etc. The proxy services allows switching of transport, interface (WSDL/Schema/Policy), message format (SOAP 1.1, 1.2/POX/REST, Text, Binary/Hessian etc.), QoS (WS-Addressing, WS-Security, WS-Reliable Messaging) and message optimization (MTOM/SwA) etc.

Synapse has implemented a non-blocking [[HTTP]]/[[HTTPS|S]] transport implementation over the Apache HttpComponents/NIO module to handle thousands of concurrent requests using little resources and threads. This implementation is capable of connection throttling to control the rate at which large messages are read and processed, and thus can handle heavy concurrent loads of large messages using constant memory. 

Synapse also supports clustered deployments, with support for load balancing, throttling and caching over clustered deployments. The integration with an external Registry/Repository allows Synapse to use externally defined resources for mediation, as well as store its configuration into an externally managed Registry/Repository for [[SOA Governance]]. Synapse can be extended with custom Java extensions or POJO classes, or via Apache BFS scripting languages such as Javascript, Ruby, Groovy etc. Synapse ships with over 50 samples.

==See also==
*[[Enterprise Service Bus]]
*[[Service-oriented architecture]]
*[[Apache Axis2]]
*[[Web service]]
*[[Apache License]]

==References==
&lt;references /&gt;

== External links ==
*[http://synapse.apache.org Synapse web site]
*[http://synapse.apache.org/Synapse_QuickStart.html Quick Start Guide]
*[http://synapse.apache.org/Synapse_Configuration_Language.html Configuration Language Syntax]
*[http://synapse.apache.org/Synapse_Samples.html Samples Guide]

{{apache}}

[[Category:Apache Software Foundation|Synapse]]
[[Category:Java platform]]
[[Category:Java enterprise platform]]
[[Category:Service-oriented architecture-related products]]
[[Category:Enterprise application integration]]</text>
      <sha1>28ev65yax3hosy79d58pebz5ur8ewvh</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Commons</title>
    <ns>0</ns>
    <id>4642733</id>
    <revision>
      <id>845006211</id>
      <parentid>838711271</parentid>
      <timestamp>2018-06-08T17:40:22Z</timestamp>
      <contributor>
        <ip>73.135.215.90</ip>
      </contributor>
      <comment>Updated Compress library release date to 1.17</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14298">{{Citation style|date=February 2011|details=Violates Wikipedia:External links: "Wikipedia articles may include links to web pages outside Wikipedia (external links), but they should not normally be used in the body of an article."}}

The '''Apache Commons''' is a project of the [[Apache Software Foundation]], formerly under the [[Jakarta Project]]. The purpose of the Commons is to provide reusable, [[open source]] Java software. The Commons is composed of three parts: proper, sandbox, and dormant.

== Commons Proper ==
The Commons Proper is dedicated to creating and maintaining reusable [[Java platform|Java]] components. The Commons Proper is a place for collaboration and sharing, where developers from throughout the Apache community can work together on projects to be shared by Apache projects and Apache users.
Commons developers will make an effort to ensure that their components have minimal dependencies on other [[Library (computer science)|software libraries]], so that these components can be [[Software deployment|deployed]] easily. In addition, Commons components will keep their [[Interface (computer science)|interfaces]] as stable as possible, so that Apache users, as well as other Apache projects, can implement these components without having to worry about changes in the future.&lt;ref&gt;[http://commons.apache.org/ The Apache Commons root page]&lt;/ref&gt;

{|class="wikitable sortable"
! Components
! Description
! Latest version
! Released
|-  
| [http://commons.apache.org/proper/commons-bcel/ BCEL]
| Byte Code Engineering Library - analyze, create, and manipulate Java class files
| 6.2
| 2017-11-08
|-  
| [http://commons.apache.org/proper/commons-beanutils/ BeanUtils]
| Easy-to-use wrappers around the Java reflection and introspection APIs.
| 1.9.3
| 2016-09-26
|-  
| [http://commons.apache.org/proper/commons-bsf/ BSF]
| Bean Scripting Framework - interface to scripting languages, including JSR-223
| 3.1
| 2010-24-06
|-  
| [http://commons.apache.org/proper/commons-chain/ Chain]
| ''Chain of Responsibility'' pattern implementation.
| 1.2
| 2008-06-02
|-  
| [http://commons.apache.org/proper/commons-cli/ CLI]
|  Command Line arguments parser.
| 1.4
| 2017-03-09
|-  
| [http://commons.apache.org/proper/commons-codec/ Codec]
| General encoding/decoding algorithms (for example phonetic, base64, URL).
| 1.11
| 2017-10-20
|-  
| [http://commons.apache.org/proper/commons-collections/ Collections]
| Extends or augments the Java Collections Framework.
| 4.1
| 2015-11-27
|-  
| [http://commons.apache.org/proper/commons-compress/ Compress]
| Defines an API for working with tar, zip and bzip2 files.
| 1.17
| 2018-05-31
|-  
| [http://commons.apache.org/proper/commons-configuration/ Configuration]
| Reading of configuration/preferences files in various formats.
| 2.2
| 2017-10-12
|-  
| [http://commons.apache.org/proper/commons-csv/ CSV]
| Component for reading and writing comma separated value files.
| 1.5
| 2017-09-03
|-  
| [http://commons.apache.org/proper/commons-daemon/ Daemon]
| Alternative invocation mechanism for unix-daemon-like java code.
| 1.0.15
| 2013-03-04
|-  
| [http://commons.apache.org/proper/commons-dbcp/ DBCP]
| Database connection pooling services.
| 2.2.0
| 2017-12-24
|-  
| [http://commons.apache.org/proper/commons-dbutils/ DbUtils]
| JDBC helper library.
| 1.7
| 2017-07-20
|-  
| [http://commons.apache.org/proper/commons-digester/ Digester]
| XML-to-Java-object mapping utility.
| 3.2
| 2011-12-13
|-  
| [http://commons.apache.org/proper/commons-email/ Email]
| Library for sending e-mail from Java.
| 1.5
| 2017-08-01
|-  
| [http://commons.apache.org/proper/commons-exec/ Exec]
| API for dealing with external process execution and environment management in Java.
| 1.3
| 2014-11-06
|-  
| [http://commons.apache.org/proper/commons-fileupload/ FileUpload]
| File upload capability for your servlets and web applications.
| 1.3.3
| 2017-06-13
|-  
| [http://commons.apache.org/proper/commons-functor/ Functor]
| A functor is a function that can be manipulated as an object, or an object representing a single, generic function.
| 1.0
| 2011-??-??
|-  
| [http://commons.apache.org/proper/commons-imaging/ Imaging]
|  A pure-Java image library.
| 0.97-incubator
| 2009-02-20
|-  
| [http://commons.apache.org/proper/commons-io/ IO]
| Collection of I/O utilities.
| 2.6
| 2017-10-15
|-  
| [http://commons.apache.org/proper/commons-jci/ JCI]
| Java Compiler Interface
| 1.1
| 2013-10-14
|-  
| [http://commons.apache.org/proper/commons-jcs/ JCS]
| Java Caching System
| 2.2
| 2017-08-02
|-  
| [http://commons.apache.org/proper/commons-jelly/ Jelly]
| XML based scripting and processing engine.
| 1.0.1
| 2017-09-27
|-  
| [http://commons.apache.org/proper/commons-jexl/ Jexl]
|  Expression language which extends the Expression Language of the JSTL.
| 3.1
| 2017-04-14
|-  
| [http://commons.apache.org/proper/commons-jxpath/ JXPath]
|  Utilities for manipulating Java Beans using the XPath syntax.
| 1.3
| 2008-08-14
|-  
| [http://commons.apache.org/proper/commons-lang/ Lang]
| Provides extra functionality for classes in java.lang.
| 3.7
| 2017-10-04
|-  
| [[Apache Commons Logging|Logging]]
| Wrapper around a variety of logging API implementations.
| 1.2
| 2014-07-11
|-  
| [http://commons.apache.org/proper/commons-math/ Math]
| Lightweight, self-contained mathematics and statistics components.
| 3.6.1
| 2016
|-  
| [http://commons.apache.org/proper/commons-net/ Net]
| Collection of network utilities and protocol implementations.
| 3.6
| 2017-02-15
|-  
| [http://commons.apache.org/proper/commons-ognl/ OGNL]
| An Object-Graph Navigation Language
| 4.0
| 2013-??-??
|-  
| [http://commons.apache.org/proper/commons-pool/ Pool]
| Generic object pooling component.
| 2.5.0
| 2017-12-19
|-  
| [http://commons.apache.org/proper/commons-proxy/ Proxy]
|  Library for creating dynamic proxies.
| 1.0
| 2008-02-28
|-
|[https://commons.apache.org/proper/commons-rdf/ RDF]
|Common implementation of RDF 1.1 that could be implemented by systems on the JVM.
|0.3.0-incubating
|2016-11-15
|-    
| [https://commons.apache.org/proper/commons-rng/ RNG]
|  Commons Rng provides implementations of pseudo-random numbers generators.
| 1.0
| 2016-12-13
|- 
| [http://commons.apache.org/proper/commons-scxml/ SCXML]
| An implementation of the State Chart XML specification aimed at creating and maintaining a Java SCXML engine.
| 0.9
| 2008-12-01
|-  
| [http://commons.apache.org/proper/commons-validator/ Validator]
| Framework to define validators and validation rules in an xml file.
| 1.6
| 2017-02-21
|-  
| [http://commons.apache.org/proper/commons-vfs/ VFS]
| Virtual File System component for treating files, FTP, SMB, ZIP and such like as a single logical file system.
| 2.2
| 2017-10-06
|-  
| [http://commons.apache.org/proper/commons-weaver/ Weaver]
| Provides an easy way to enhance (weave) compiled bytecode.
| 1.3
| 2016-10-18
|}

== Commons Sandbox ==
The Commons Sandbox provides a workspace where Commons contributors collaborate and experiment on projects not included in the Commons Proper. Commons members champion projects in the Sandbox for promotion to the Commons Proper, and groups of developers work to enhance Sandbox projects until they meet the standards for promotion.

{|class="wikitable"
! Components
! Description
|-  
| [http://commons.apache.org/sandbox/commons-beanutils2/ BeanUtils2]
| Redesign of Commons BeanUtils.
|-  
| [http://commons.apache.org/sandbox/commons-classscan/ ClassScan]
| Find Class interfaces, methods, fields, and annotations without loading.
|-  
| [http://commons.apache.org/sandbox/commons-cli2 CLI2]
| Redesign of Commons CLI.
|-  
| [http://commons.apache.org/sandbox/commons-convert/ Convert]
| Commons-Convert aims to provide a single library dedicated to the task of converting an object of one type to another.
|-  
| [http://commons.apache.org/sandbox/commons-finder/ Finder]
| Java library inspired by the UNIX find command.
|-  
| [http://commons.apache.org/sandbox/commons-flatfile/ Flatfile]
| Java library for working with flat data structures.
|-  
| [http://commons.apache.org/sandbox/commons-graph/ Graph]
| A general purpose Graph APIs and algorithms.
|-  
| [http://commons.apache.org/sandbox/commons-i18n/ I18n]
| Adds the feature of localized message bundles that consist of one or many localized texts that belong together.
|-  
| [http://commons.apache.org/sandbox/commons-id/ Id]
| Id is a component used to generate identifiers.
|-  
| [http://commons.apache.org/sandbox/commons-javaflow/ Javaflow]
| Continuation implementation to capture the state of the application.
|-  
| [http://commons.apache.org/sandbox/commons-jnet/ JNet]
| JNet allows using dynamically registered url stream handlers through the java.net API.
|-
|[http://commons.apache.org/sandbox/commons-monitoring/ Monitoring]
|Monitoring aims to provide a simple but extensible monitoring solution for Java applications.
|-  
| [http://commons.apache.org/sandbox/commons-nabla/ Nabla]
| Nabla provides automatic differentiation classes that can generate derivative of any function implemented in the Java language.
|-  
| [http://commons.apache.org/sandbox/commons-openpgp/ OpenPGP]
| Interface to signing and verifying data using OpenPGP.
|-  
| [http://commons.apache.org/sandbox/commons-performance/ Performance]
| A small framework for microbenchmark clients, with implementations for Commons DBCP and Pool.
|-  
| [http://commons.apache.org/sandbox/commons-pipeline/ Pipeline]
| Provides a set of pipeline utilities designed around work queues that run in parallel to sequentially process data objects.
|}

== Commons Dormant ==
The Commons Dormant is a collection of components that have been declared inactive due to little recent development activity. These components may be used, but must be built yourself. It is best to assume that these components will not be released in the near future.

{|class="wikitable"  
! Components
! Description
|-  
| [http://commons.apache.org/./dormant/commons-attributes/ Attributes]
| Runtime API to metadata attributes such as doclet tags.
|-  
| [http://commons.apache.org/./dormant/commons-betwixt/ Betwixt]
| Services for mapping JavaBeans to XML documents, and vice versa.
|-  
| [http://commons.apache.org/./dormant/cache/ Cache]
| Cache provides object caching services.
|-  
| [http://commons.apache.org/./dormant/clazz/ Clazz]
| Clazz focuses on introspection and class manipulation.
|-  
| [http://commons.apache.org/./dormant/contract/ Contract]
| This component makes all the nice features available to the java programming language that come along with contract based programming.
|-  
| [http://commons.apache.org/./dormant/convert/ Convert]
| Commons-Convert aims to provide a single library dedicated to the task of converting an object of one type to another.
|-
|[http://commons.apache.org/dormant/discovery/ Discovery]
|Tools for locating resources by mapping service/reference names to resource names.
|-
|[http://commons.apache.org/dormant/commons-el/ EL]
|Interpreter for the Expression Language defined by the JSP 2.0 specification.
|-  
| [http://commons.apache.org/./dormant/events/ Events]
| Commons-Events provides additional classes for firing and handling events. It focusses on the Java Collections Framework, providing decorators to other collections that fire events.
|-  
| [http://commons.apache.org/./dormant/feedparser/ Feedparser]
| A Java RSS/Atom parser designed to elegantly support all major versions of RSS and Atom, as well as easy ad hoc extension and RSS 1.0 modules capability.
|-  
| [http://commons.apache.org/./dormant/jjar/ JJar]
| Jakarta JAR Archive Repository
|-  
| [http://commons.apache.org/./dormant/latka/ Latka]
| Commons-Latka is an HTTP functional testing suite for automated QA, acceptance and regression testing.
|-
|[http://commons.apache.org/dormant/launcher/ Launcher]
|Cross platform Java application launcher.
|-  
| [http://commons.apache.org/./dormant/mapper/ Mapper]
| Mapper is a thin abstraction layer around a project's chosen data mapping technology (a.k.a. DAO pattern).
|-  
| [http://commons.apache.org/./dormant/messenger/ Messenger]
| Messenger is an easy to use and lightweight framework for working with JMS in the web tier.
|-
|[http://commons.apache.org/dormant/commons-modeler/ Modeler]
|Mechanisms to create Model MBeans compatible with JMX specification.
|-
|[http://commons.apache.org/dormant/commons-primitives/ Primitives]
|Smaller, faster and easier to work with types supporting Java primitive types.
|-  
| [http://commons.apache.org/./dormant/resources/ Resources]
| Resources provides a lightweight framework for defining and looking up internationalized message strings keyed by a java.util.Locale and a message key.
|-  
| [http://commons.apache.org/./dormant/scaffold/ Scaffold]
| Scaffold is a toolkit for building web applications.
|-  
| [http://commons.apache.org/./dormant/threadpool/ ThreadPool]
| ThreadPool is a simple component for asynchronously dispatching work to another thread in a pool for simple multi threaded programming.
|-  
| [http://commons.apache.org/proper/commons-transaction/ Transaction]
| Implementations for multi level locks, transactional collections and transactional file access.
|-  
| [http://commons.apache.org/./dormant/workflow/ Workflow]
| Workflow provides a framework for building workflow management systems.
|-  
| [http://commons.apache.org/./dormant/xmlio/ XMLIO]
| Simple and fast importer for XML configuration or import files.
|}

== See also ==
* [[Google Guava]]

== References ==
{{Reflist}}

{{Citation|last=Goyal|first=Vikram|url=http://www.onjava.com/pub/a/onjava/2003/06/25/commons.html|title=Using the Jakarta Commons, Part I|accessdate=August 13, 2006|year=2003}}

== External links ==
* [http://commons.apache.org/ Apache Commons]
* [http://commons.apache.org/components.html Components page]
* [http://commons.apache.org/sandbox.html Sandbox page]
* [http://commons.apache.org/dormant/index.html Dormant page]
* [http://apiwave.com/java/api/org.apache.commons Apache Commons popular APIs in GitHub]

{{Apache}}

&lt;!--Categories--&gt;
[[Category:Java (programming language) libraries]]
[[Category:Apache Software Foundation|Commons]]
[[Category:Software using the Apache license]]</text>
      <sha1>m3b4pxn4cdp1uat1chn9yrcbr96p850</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Apache Software Foundation projects</title>
    <ns>14</ns>
    <id>1972685</id>
    <revision>
      <id>835869716</id>
      <parentid>835807756</parentid>
      <timestamp>2018-04-11T08:02:10Z</timestamp>
      <contributor>
        <username>NicoScribe</username>
        <id>25958220</id>
      </contributor>
      <comment>Undid revision 835807756 by [[Special:Contributions/89.15.237.155|89.15.237.155]] ([[User talk:89.15.237.155|talk]]) ?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="332">Please note that this listing is incomplete: another one is being maintained at [http://projects.apache.org projects.apache.org]

{{Portal|Free software}}
{{Commons category|Apache Software Foundation}}

[[Category:Free software projects]]
[[Category:Apache Software Foundation]]

[[nl:Categorie:Apache Software Foundation project]]</text>
      <sha1>0yi0ikalwfpagp8mcy3zvkxhq8i00vf</sha1>
    </revision>
  </page>
  <page>
    <title>Log4j</title>
    <ns>0</ns>
    <id>3291868</id>
    <revision>
      <id>844215302</id>
      <parentid>831279180</parentid>
      <timestamp>2018-06-03T12:29:20Z</timestamp>
      <contributor>
        <username>اقرأ</username>
        <id>16412120</id>
      </contributor>
      <minor/>
      <comment>update</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24070">{{Infobox software
| name                   = Apache Log4j
| logo                   = [[File:Apache Log4j Logo.png|250px]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| released              = {{Start date and age|2001|01|08}}&lt;ref&gt;{{cite web |url=https://logging.apache.org/log4j/1.2/changes-report.html |title=Apache Log4j 1.2 Release History |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-09-02}}&lt;/ref&gt;
| latest release version = 2.11.0
| latest release date    = {{release date and age|2018|03|11}}&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/2.x/changes-report.html|title=Log4j – Changes - Apache Log4j 2|website=apache.org|publisher=Apache Software Foundation|accessdate=21 April 2017}}&lt;/ref&gt;
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[computer data logging|Logging Tool]]
| license                = [[Apache License]] 2.0
| website                = {{url|https://logging.apache.org/log4j/2.x/}}
}}
Apache '''Log4j''' is a [[Java platform|Java]]-based [[computer data logging|logging]] utility. It was originally written by [[Ceki Gülcü]] and is part of the [[Apache Logging Services]] project of the [[Apache Software Foundation]].  Log4j is one of several [[Java logging frameworks]].

Gülcü has since started the [[SLF4J]] and Logback&lt;ref&gt;{{cite web|url=http://logback.qos.ch/ |title=Logback Home |publisher=Logback.qos.ch |date= |accessdate=2014-07-24}}&lt;/ref&gt; projects, with the intention of offering a successor to Log4j.

The Apache Log4j team has created a successor to Log4j 1 with version number 2.&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/2.x/ |title=Log4j 2 Guide - Apache Log4j 2 |publisher=Logging.apache.org |date=2014-07-12 |accessdate=2014-07-24}}&lt;/ref&gt; Log4j 2 was developed with a focus on the problems of Log4j 1.2, 1.3, java.util.logging and Logback, and addresses issues which appeared in those frameworks. In addition, Log4j 2 offers a plugin architecture which makes it more extensible than its predecessor.  Log4j 2 is not backwards compatible with 1.x versions,&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/2.x/index.html#News |title=Log4j 2 Guide - Apache Log4j 2: News |publisher=Logging.apache.org |date=2014-07-12 |accessdate=2014-07-24}}&lt;/ref&gt; although an "adapter" is available.

On August 5, 2015 the Apache Logging Services Project Management Committee announced&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/apache_logging_services_project_announces |title=Apache™ Logging Services™ Project Announces Log4j™ 1 End-Of-Life; Recommends Upgrade to Log4j 2 |publisher=blogs.apache.org |date=2015-08-05 |accessdate=2016-07-03}}&lt;/ref&gt; that Log4j 1 had reached end of life and that users of Log4j 1 are recommended to upgrade to Apache Log4j 2.

==Apache Log4j 2==
Apache Log4j 2 is the successor of Log4j 1 which was released as [[Software release life cycle#General availability .28GA.29|GA version]] in July 2014. The framework was rewritten from scratch and has been inspired by existing logging solutions, including Log4j 1 and java.util.logging. The main differences&lt;ref&gt;{{cite web|url=http://www.grobmeier.de/the-new-log4j-2-0-05122012.html |title=The new log4j 2.0 |publisher=Grobmeier.de |date=2012-12-05 |accessdate=2014-07-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/2.x/manual/index.html |title=Log4j – Overview - Apache Log4j 2 |publisher=logging.apache.org |date=2016-06-05 |accessdate=2016-07-03}}&lt;/ref&gt; from Log4j 1 are:
* Improved reliability. Messages are not lost while reconfiguring the framework like in Log4j 1 or Logback
* Extensibility: Log4j 2 supports a plugin system to let users define and configure custom components
* Simplified configuration syntax
* Support for xml, json, yaml and properties configurations
* Improved filters
* Property lookup support for values defined in the configuration file, system properties, environment variables, the ThreadContext Map, and data present in the event
* Support for multiple APIs: Log4j 2 can be used with applications using the Log4j 2, Log4j 1.2, SLF4J, Commons Logging and java.util.logging (JUL) APIs.
* Custom log levels
* Java 8-style lambda support for "lazy logging"
* Markers
* Support for user-defined Message objects
* "Garbage-free or low garbage" in common configurations
* Improved speed

One of the most recognized features of Log4j 2 is the performance of the "Asynchronous Loggers".&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/2.x/manual/async.html#Performance |title=Log4j 2 Asynchronous Loggers for Low-Latency Logging - Apache Log4j 2 |publisher=Logging.apache.org |date=2014-07-12 |accessdate=2014-07-24}}&lt;/ref&gt; Log4j 2 makes use of the LMAX Disruptor.&lt;ref&gt;{{cite web|url=https://lmax-exchange.github.io/disruptor/ |title=Disruptor by LMAX-Exchange |publisher=Lmax-exchange.github.io |date= |accessdate=2014-07-24}}&lt;/ref&gt; The library reduces the need for kernel locking and increases the logging performance by a factor of 12. For example, in the same environment Log4j 2 can write more than 18,000,000 messages per second, whereas other frameworks like Logback and Log4j 1 just write &lt; 2,000,000 messages per second.

== Log4j log levels ==
The following table defines the built-in log levels and messages in Log4j, in decreasing order of severity. The left column lists the log level designation in Log4j and the right column provides a brief description of each log level.

{| class="wikitable"
|-
!'''Level'''
!'''Description'''
|-
| '''OFF'''
| The highest possible rank and is intended to turn off logging.
|-
| '''FATAL'''
| Severe errors that cause premature termination. Expect these to be immediately visible on a status console.
|-
| '''ERROR'''
| Other runtime errors or unexpected conditions. Expect these to be immediately visible on a status console.
|-
| '''WARN'''
| Use of deprecated APIs, poor use of API, 'almost' errors, other runtime situations that are undesirable or unexpected, but not necessarily "wrong". Expect these to be immediately visible on a status console.
|-
| '''INFO'''
| Interesting runtime events (startup/shutdown). Expect these to be immediately visible on a console, so be conservative and keep to a minimum.
|-
| '''DEBUG'''
| Detailed information on the flow through the system. Expect these to be written to logs only. Generally speaking, most lines logged by your application should be written as DEBUG.
|-
| '''TRACE'''
| Most detailed information. Expect these to be written to logs only. Since version 1.2.12.&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/Level.html#TRACE |title=Level (Apache Log4j 1.2.17 API) |publisher=Logging.apache.org |date=2012-06-09 |accessdate=2014-07-24}}&lt;/ref&gt;
|}

===Custom log levels===
Log4j 2 allows users to define their own log levels.&lt;ref&gt;{{cite web|url=https://logging.apache.org/log4j/2.x/manual/customloglevels.html |title=Custom Log Levels |publisher=Logging.apache.org |date=2014-07-12 |accessdate=2016-07-16}}&lt;/ref&gt; A source code generator tool is provided to create Loggers that support custom log levels identically to the built-in log levels. Custom log levels can either complement or replace the built-in log levels.

==Log4j configuration==
Log4j can be configured&lt;ref&gt;{{cite web|url=https://logging.apache.org/log4j/2.x/manual/configuration.html |title=Configuration |publisher=Logging.apache.org |date=2016-07-05 |accessdate=2016-07-16}}&lt;/ref&gt; through a configuration file or through Java code. Configuration files can be written in [[XML]], [[JSON]], [[YAML]], or [[.properties|properties file]] format. Within a configuration you can define three main components: Loggers, Appenders and Layouts. Configuring logging via a file has the advantage that logging can be turned on or off without modifying the application that uses Log4j. The application can be allowed to run with logging off until there's a problem, for example, and then logging can be turned back on simply by modifying the configuration file.

'''Loggers'''&lt;ref&gt;{{cite web|url=https://logging.apache.org/log4j/2.x/manual/architecture.html |title=Architecture|publisher=Logging.apache.org |date=2016-07-05 |accessdate=2016-07-16}}&lt;/ref&gt; are named log message destinations. They are the names that are known to the Java application. Each logger is independently configurable as to what level of logging (FATAL, ERROR, etc.) it currently logs. In early versions of Log4j, these were called category and priority, but now they're called logger and level, respectively. A Logger can send log messages to multiple Appenders.

The actual outputs are done by '''Appenders'''.&lt;ref&gt;{{cite web|url=https://logging.apache.org/log4j/2.x/manual/appenders.html |title=Appenders|publisher=Logging.apache.org |date=2016-07-05 |accessdate=2016-07-16}}&lt;/ref&gt; There are numerous Appenders available, with descriptive names, such as FileAppender, RollingFileAppender, ConsoleAppender, SocketAppender, SyslogAppender, and SMTPAppender. Log4j 2 added Appenders that write to [[Apache Flume]], the [[Java Persistence API]], [[Apache Kafka]], [[NoSQL]] databases, [[Memory-mapped file]]s, Random Access files&lt;ref&gt;{{cite web|url=https://docs.oracle.com/javase/7/docs/api/java/io/RandomAccessFile.html |title=RandomAccessFile|publisher=docs.oracle.com |date=2011-07-28 |accessdate=2016-07-16}}&lt;/ref&gt; and [[ZeroMQ]] endpoints. Multiple Appenders can be attached to any Logger, so it's possible to log the same information to multiple outputs; for example to a file locally and to a [[Internet socket|socket]] listener on another computer.

Appenders use '''Layouts'''&lt;ref&gt;{{cite web|url=https://logging.apache.org/log4j/2.x/manual/layouts.html |title=Layouts|publisher=Logging.apache.org |date=2016-07-05 |accessdate=2016-07-16}}&lt;/ref&gt; to format log entries. A popular way to format one-line-at-a-time log files is PatternLayout, which uses a pattern string, much like the [[C (programming language)|C]] / [[C++]] function [[printf]]. There are also HTMLLayout and XMLLayout formatters for use when [[HTML]] or XML formats are more convenient, respectively. Log4j 2 added Layouts for [[Comma-separated values|CSV]], Graylog Extended Log Format (GELF),&lt;ref&gt;{{cite web|url=http://docs.graylog.org/en/2.0/pages/gelf.html |title=GELF|publisher=docs.graylog.org |date=2016-06-08 |accessdate=2016-07-16}}&lt;/ref&gt; [[JSON]], [[YAML]] and RFC-5424.&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5424 |title=RFC 5424 - The Syslog Protocol|publisher=tools.ietf.org |date=2009-03-01 |accessdate=2016-07-16}}&lt;/ref&gt;

In Log4j 2, '''Filters'''&lt;ref&gt;{{cite web|url=https://logging.apache.org/log4j/2.x/manual/filters.html |title=Filters|publisher=Logging.apache.org |date=2016-07-05 |accessdate=2016-07-16}}&lt;/ref&gt; can be defined on configuration elements to give more fine-grained control over which log entries should be processed by which Loggers and Appenders. In addition to filtering by log level and regular expression matching on the message string, Log4j 2 added burst filters, time filters, filtering by other log event attributes like Markers or Thread Context Map and [[JSR 223]] script filters.

To debug a misbehaving configuration:
* In Log4j 2 configurations set the &lt;code&gt;status&lt;/code&gt; attribute to TRACE to send internal status logging output to [[Standard streams|standard out]]. To enable status logging before the configuration is found, use the Java VM property &lt;code&gt;-Dorg.apache.logging.log4j.simplelog.StatusLogger.level=trace&lt;/code&gt;.
* In Log4j 1, use the Java VM property &lt;code&gt;-Dlog4j.debug&lt;/code&gt;. 
To find out where a log4j2.xml configuration file was loaded from inspect &lt;code&gt;getClass().getResource("/log4j2.xml")&lt;/code&gt;.

There is also an implicit "unconfigured" or "default" configuration of Log4j, that of a Log4j-instrumented Java application which lacks any Log4j configuration. This prints to stdout a warning that the program is unconfigured, and the URL to the Log4j web site where details on the warning and configuration may be found. As well as printing this warning, an unconfigured Log4j application will only print ERROR or FATAL log entries to standard out.

===Example for Log4j 2===
&lt;source lang="xml" enclose="div"&gt;
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;Configuration status="trace" monitorInterval="60"&gt;
  &lt;Properties&gt;
    &lt;Property name="filename"&gt;target/test.log&lt;/Property&gt;
  &lt;/Properties&gt;
 
  &lt;Appenders&gt;
    &lt;Console name="STDOUT"&gt;
      &lt;PatternLayout pattern="%d %p %c{1.} [%t] %m%n"/&gt;
    &lt;/Console&gt;

    &lt;File name="file" fileName="${filename}"&gt;
      &lt;PatternLayout&gt;
        &lt;pattern&gt;%d %p %c{1.} [%t] %m%n&lt;/pattern&gt;
      &lt;/PatternLayout&gt;
    &lt;/File&gt;
  &lt;/Appenders&gt;
 
  &lt;Loggers&gt; 
    &lt;!-- 
         loggers whose name starts with 'org.springframework' will only log messages of level "info" or higher;
         if you retrieve Loggers by using the class name (e.g. Logger.getLogger(AClass.class))
         and if AClass is part of the org.springframework package, it will belong to this category
    --&gt;
    &lt;Logger name="org.springframework" level="info" additivity="false" /&gt;

    &lt;!--
        Filter example: for loggers whose name starts with 'com.mycompany.myproduct',
        log entries of level "debug" or higher whose ThreadContextMap data contains
        the key-value pair "test=123", also send these log entries to the "STDOUT" appender.
    --&gt;
    &lt;Logger name="com.mycompany.myproduct" level="debug" additivity="true"&gt;
      &lt;ThreadContextMapFilter&gt;
        &lt;KeyValuePair key="test" value="123"/&gt;
      &lt;/ThreadContextMapFilter&gt;
      &lt;AppenderRef ref="STDOUT"/&gt;
    &lt;/Logger&gt;
 
    &lt;!--
        By default, all log messages of level "trace" or higher will be logged.
        Log messages are sent to the "file" appender and 
        log messages of level "error" and higher will be sent to the "STDOUT" appender.
    --&gt;
    &lt;Root level="trace"&gt;
      &lt;AppenderRef ref="file"/&gt;
      &lt;AppenderRef ref="STDOUT" level="error"/&gt;
    &lt;/Root&gt;
  &lt;/Loggers&gt;
 
&lt;/Configuration&gt;
&lt;/source&gt;

===Example for Log4j 1.2 ===
&lt;source lang="xml" enclose="div"&gt;
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE log4j:configuration PUBLIC "-//LOGGER" "http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/xml/doc-files/log4j.dtd"&gt;
&lt;log4j:configuration&gt;
    &lt;!-- 
         an appender is an output destination, such as the console or a file;
         names of appenders are arbitrarily chosen.
    --&gt;
    &lt;appender name="stdout" class="org.apache.log4j.ConsoleAppender"&gt;
        &lt;layout class="org.apache.log4j.PatternLayout"&gt;
            &lt;param name="ConversionPattern"
                value="%d{ABSOLUTE} %5p %c{1}:%L - %m%n" /&gt;
        &lt;/layout&gt;
    &lt;/appender&gt;
 
    &lt;!-- 
         loggers of category 'org.springframework' will only log messages of level "info" or higher;
         if you retrieve Loggers by using the class name (e.g. Logger.getLogger(AClass.class))
         and if AClass is part of the org.springframework package, it will belong to this category
    --&gt;
    &lt;logger name="org.springframework"&gt;
        &lt;level value="info"/&gt;
    &lt;/logger&gt;

    &lt;!-- 
         everything of spring was set to "info" but for class 
         PropertyEditorRegistrySupport we want "debug" logging 
    --&gt;
    &lt;logger name="org.springframework.beans.PropertyEditorRegistrySupport"&gt;
        &lt;level value="debug"/&gt;
    &lt;/logger&gt;
 
    &lt;logger name="org.acegisecurity"&gt;
        &lt;level value="info"/&gt;
    &lt;/logger&gt;
    
    
    &lt;root&gt;
        &lt;!-- 
            all log messages of level "debug" or higher will be logged, unless defined otherwise 
            all log messages will be logged to the appender "stdout", unless defined otherwise 
        --&gt;
        &lt;level value="debug" /&gt;
        &lt;appender-ref ref="stdout" /&gt;
    &lt;/root&gt;
&lt;/log4j:configuration&gt;
&lt;/source&gt;

== TTCC ==

TTCC is a message format used by log4j.&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/TTCCLayout.html |title=TTCCLayout (Apache Log4j 1.2.17 API) |publisher=Logging.apache.org |date=2012-06-09 |accessdate=2014-07-24}}&lt;/ref&gt;  TTCC is an acronym for ''Time Thread Category Component''.  It uses the following pattern:

  %r [%t] %-5p %c %x - %m%n

Where
{| class="wikitable"
|-
! Mnemonic
! Description
|-
| %r
| Used to output the number of milliseconds elapsed from the construction of the layout until the creation of the logging event.
|-
| %t
| Used to output the name of the thread that generated the logging event.
|-
| %p
| Used to output the priority of the logging event.
|-
| %c
| Used to output the category of the logging event.
|-
| %x
| Used to output the NDC (nested diagnostic context) associated with the thread that generated the logging event.&lt;ref&gt;{{cite web |url=http://logging.apache.org/log4j/docs/api/org/apache/log4j/NDC.html |title=Class NDC
|archiveurl=https://web.archive.org/web/20070820182511/http://logging.apache.org/log4j/docs/api/org/apache/log4j/NDC.html |archivedate=2007-08-20 |deadurl=yes |accessdate=2014-07-24}}&lt;/ref&gt;
|-
| %X{key}
| Used to output the MDC (mapped diagnostic context) associated with the thread that generated the logging event for specified key.&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/MDC.html |title=MDC (Apache Log4j 1.2.17 API) |publisher=Logging.apache.org |date=2012-06-09 |accessdate=2014-07-24}}&lt;/ref&gt;
|-
| %m
| Used to output the application supplied message associated with the logging event.
|-
| %n
| Used to output the platform-specific [[newline]] character or characters.
|}

'''Example output'''&lt;br /&gt;
467 [main] INFO  org.apache.log4j.examples.Sort - Exiting main method.

==Ports==
*'''log4c''' - A port for C. ''Log4C'' is a C-based [[computer data logging|logging]] library, released on [[SourceForge]] under the [[LGPL]] license.  For various [[Unix]] operating systems the [[autoconf]] and [[automake]] files are provided. On [[Microsoft Windows|Windows]] a [[Makefile]] is provided for use with [[MSVC]]. [[Software developer|Developers]] may also choose to use their own make system to compile the source, depending on their build engineering requirements. An instance of the ''log4c'' library may be configured via three methods: using [[environment variable]]s, programmatically, or via [[XML]] configuration file. Last version is 1.2.4, released in 2013, and the project is no longer actively developed.&lt;ref&gt;{{cite web|url=http://sourceforge.net/projects/log4c/ |title=Logging Framework for C &amp;#124; Free System Administration software downloads at |publisher=Sourceforge.net |date= |accessdate=2014-07-24}}&lt;/ref&gt;
*'''log4js''' - A port for [[JavaScript]]. Log4js is available under the licence of [[Apache Software Foundation]]. One special feature of Log4js is the ability to log the events of the browser remotely on the server. Using [[Ajax (programming)|Ajax]] it is possible to send the logging events in several formats ([[XML]], [[JSON]], plain [[ASCII]], etc.) to the server to be evaluated there. The following appenders are implemented for ''log4js'': AjaxAppender, ConsoleAppender, FileAppender, JSConsoleAppender, MetatagAppender, and WindowsEventsAppender. The following Layout classes are provided: BasicLayout, HtmlLayout, JSONLayout, and XMLLayout. Last version is 1.1, released in 2008.&lt;ref&gt;{{cite web|author= |url=https://stritti.github.io/log4js/ |title=Log4js |publisher=Log4js |date= |accessdate=2017-03-29}}&lt;/ref&gt;
*'''log4javascript''' - Another port for JavaScript. log4javascript is a JavaScript logging framework based on the ''log4j''. The latest version is 1.4.9, released in May 2014.&lt;ref&gt;{{cite web|url=http://log4javascript.org/ |title=a JavaScript logging framework |publisher=log4javascript |date= |accessdate=2014-07-24}}&lt;/ref&gt;
*'''JSNLog''' - A port for [[JavaScript]]. Automatically places messages from JavaScript loggers in server side logs using a .NET server side component that interfaces with Log4Net, NLog, Elmah or Common.Logging. This to provide an integrated log for client and server side events. Request ids correlate events related to a specific user. Configuration is via a server side web.config file. Supports exception logging including stack traces. In July 2014 the latest version was 2.7.1 and updates were made regularly.&lt;ref&gt;{{cite web |url=http://jsnlog.com/ |title=Logging JavaScript errors to your server side log |publisher=JSNLog |date= |accessdate=2014-07-24}}&lt;/ref&gt;
*'''Apache Log4net''' - A port to the Microsoft [[.NET Framework]]. The initial work was done by [[Neoworks]] and was donated to the [[Apache Software Foundation]] in February 2004. The framework is similar to the original log4j while taking advantage of new features in the .NET runtime. Provides Nested Diagnostic Context (NDC) and Mapped Diagnostic Context (MDC). Last version is 1.2.15, released in 2015.&lt;ref&gt;{{cite web|url=http://logging.apache.org/log4net/ |title=Apache log4net: Home |publisher=Logging.apache.org |date=2015-12-05 |accessdate=2016-04-08}}&lt;/ref&gt;
*'''log4perl''' -  A [[Perl]] port of the widely popular log4j logging package. Last version is 1.49, released in February 2017.&lt;ref&gt;{{cite web|url=https://mschilli.github.com/log4perl |title=log4perl - log4j for Perl |publisher=Mschilli.github.com |date= |accessdate=2014-07-24}}&lt;/ref&gt;
*'''Apache log4php''' - "A versatile logging framework for [[PHP]]. Originally a port of Apache log4j to PHP, it has grown to include various PHP specific features."&lt;ref&gt;{{cite web|url=http://logging.apache.org/|title=Apache Logging Services |publisher=Apache.org |date= |accessdate=2015-03-11}}&lt;/ref&gt;
*'''PL-SQL-Logging-Utility''' is an adaptation of log4j in PL/SQL.&lt;ref&gt;{{cite web|url=https://github.com/tmuth/Logger---A-PL-SQL-Logging-Utility |title=tmuth/Logger-A-PL-SQL-Logging-Utility — GitHub |publisher=Github.com |date= |accessdate=2014-07-24}}&lt;/ref&gt;
*'''Log4db2''' is a logging utility for DB2 for LUW that uses SQL instructions with SQL PL code.&lt;ref&gt;{{cite web|url=https://angoca.github.io/log4db2/ |title=Log4db2 by angoca |publisher=Angoca.github.io |date= |accessdate=2014-07-24}}&lt;/ref&gt;
*'''Apache Log4cxx''' - A logging framework for C++ patterned after Apache log4j, which uses [[Apache Portable Runtime]] for most platform-specific code and should be usable on any platform supported by APR. It is currently undergoing Incubation, The latest version is 0.10.0, released in 2008.&lt;ref&gt;{{cite web|title=log4cxx - Changes|url=https://logging.apache.org/log4cxx/latest_stable/changes-report.html|website=logging.apache.org}}&lt;/ref&gt;
*'''Log4r''' - A comprehensive and flexible logging library written in Ruby for use in Ruby programs. It was inspired by and provides much of the features of the Apache Log4j project.&lt;ref&gt;{{cite web|title=Log4r Manual|url=http://log4r.rubyforge.org/|publisher=log4r.rubyforge.org|accessdate=2017-04-13|deadurl=yes|archiveurl=https://archive.is/20121225133348/http://log4r.rubyforge.org/|archivedate=2012-12-25|df=}}&lt;/ref&gt;

== See also ==
{{Portal|Java|Free software}}
*[[Chainsaw (log file viewer)]]
* [https://github.com/rdiachenko/jlv JLV - Java logging viewer which is currently available as a plugin for Eclipse IDE]

==References==
{{Reflist|30em}}

==Further reading==
{{Refbegin}}
*{{Citation
| first1     = Ceki
| last1      = Gülcü
| date       = February 2010
| title      = The Complete Log4j Manual
| edition    = 2nd
| publisher  = QOS.ch
| page      = 204
| isbn       = 978-2-9700369-0-6
}}
*{{Citation
| first1     = Samudra
| last1      = Gupta
| date       = June 22, 2005
| title      = Pro Apache Log4j
| edition    = 2nd
| publisher  = [[Apress]]
| page      = 224
| isbn       = 978-1-59059-499-5
}}
{{Refend}}

==External links==
*{{Official website}}

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Log file formats]]
[[Category:Software using the Apache license]]</text>
      <sha1>53f7ih2561fj549o54nd3rmyrhe1w1m</sha1>
    </revision>
  </page>
  <page>
    <title>Apache MyFaces Trinidad</title>
    <ns>0</ns>
    <id>23229103</id>
    <revision>
      <id>842676456</id>
      <parentid>840099380</parentid>
      <timestamp>2018-05-23T23:19:09Z</timestamp>
      <contributor>
        <username>Googol30</username>
        <id>16053044</id>
      </contributor>
      <minor/>
      <comment>fixing infobox error</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1989">{{Infobox Software
| name                   = Apache MyFaces Trinidad
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 1.2.11
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[web framework|Web Framework]]
| license                = 
| website                = http://myfaces.apache.org/trinidad/
}}
Apache '''MyFaces Trinidad''' is a [[JavaServer Faces|JSF]] framework including a large, enterprise quality component library, supporting critical features such as accessibility (e.g. Section 508), right-to-left languages, etc. It also includes a set of framework features, including :

* Partial-page rendering support for the entire component set
* Integrated client-side validation
* A dialog framework
* pageFlowScope, for communicating between pages&lt;ref name="Apache MyFaces Trinidad Project Page"&gt;http://myfaces.apache.org/trinidad/&lt;/ref&gt;

Trinidad is a subproject of [[Apache MyFaces]] project and was donated by [[Oracle Corporation|Oracle]], where it was known as [[Oracle Application Development Framework|ADF]] Faces. It was renamed Trinidad after a long voting process. Trinidad is more than just a component library because it also contains a lot of goodies which solve common development challenges.&lt;ref&gt;{{cite book | last = Wadia, Marinschek, Saleh, Byrne | first = | authorlink = | coauthors = | title = The Definitive Guide to Apache MyFaces and Facelets | publisher = Apress | date = 2008 | location = New York | pages = 111 | url = | doi = | id = | isbn = 9781590597378}}&lt;/ref&gt;

==References==
&lt;references/&gt;

{{apache}}

[[Category:JavaServer Faces|MyFaces]]
[[Category:Apache Software Foundation|MyFaces]]
[[Category:Java enterprise platform|MyFaces]]


{{web-software-stub}}</text>
      <sha1>82h1zho7bwaal5hcteyy5zjq53ykgo8</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Wink</title>
    <ns>0</ns>
    <id>23973422</id>
    <revision>
      <id>777164710</id>
      <parentid>693744998</parentid>
      <timestamp>2017-04-25T15:56:19Z</timestamp>
      <contributor>
        <ip>195.212.29.70</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3702">{{ Infobox Software
| name                   = Apache Wink
| logo                   = [[Image:Apache Wink Logo.png|Apache Wink Logo]]
| screenshot             = 
| caption                = 
| collapsible            = 
| developer              = [[Apache Software Foundation]]
| latest release version = 1.4.0
| latest release date    = {{Release date|2013|09|15}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[JAX-RS]]&lt;br /&gt;[[Representational State Transfer|RESTful Web services]]
| size                   = 11.4 MB (archived)
| license                = [[Apache License]] 2.0
| website                = {{url|http://wink.apache.org/}}
}}

'''Apache Wink''' is an [[open source]] framework that enables development and consumption of [[Representational State Transfer|REST]] style web services.

== History ==
The project was initiated in 2007 as an [[Hewlett-Packard|HP]] internal REST SDK called Symphony. At the beginning of 2009, HP and [[IBM]] joined forces to deliver the Apache Wink project,&lt;ref&gt;[http://wiki.apache.org/incubator/WinkProposal Apache Wink Proposal], retrieved August 14, 2009&lt;/ref&gt; HP contributed the SDK and IBM contributed a lot of integration tests.

The Apache Wink is currently part of the [[Apache Incubator]] project.

The Apache Wink has been retired from [[Apache Incubator]] 2017, April 25 and put in [[Apache Attic]].

==Features==
Apache Wink is composed of a Server module and a Client module. The Server module is an implementation of the [[JAX-RS]] (JSR-311) specification along with additional features to facilitate the development of RESTful Web services. The Client module is a Java-based framework that provides functionality for communicating with [[RESTful]] Web services.

* JAX-RS 1.0 compliant &lt;ref&gt;[http://incubator.apache.org/wink/0.1/Apache_Wink_User_Guide.htm Apache Wink User Guide], retrieved August 14, 2009&lt;/ref&gt;
* Has support for [[Atom (standard)|Atom Syndication Format]], [http://tools.ietf.org/html/rfc4287 Atom Publishing Protocol] and [[RSS]].
* Has support for [[JSON]], [[Comma-separated values|CSV]] and [[OpenSearch]].

== Releases ==
{| class="wikitable sortable"
! Release
! Date
! Notes
|-
|pre 0.1
|2009-05-27
|Wink enters incubation.
|-
|pre 0.1
|2009-07-01
|Wink is JAX-RS 1.0 compliant
|-
|0.1
|2009-08-26 
|Wink 0.1 released.
|-
|1.1
|2010-05-14
|Wink 1.1 released.
|-
|1.1.1
|2010-07-07
|Wink 1.1.1 released.
|-
|1.1.2
|2010-11-15
|Wink 1.1.2 released.
|-
|1.1.3
|2011-05-09
|Wink-1.1.3 released.
|-
|1.2.0
|2012-05-21
|Wink-1.2.0 released.
|-
|1.3.0
|2013-04-12
|Wink-1.3.0 released.
|-
|1.4.0
|2013-09-15
|Wink-1.4.0 released.
|}

==See also==
*[[Apache CXF]], a web services framework with JAX-RS support

==References==
{{Reflist}}

==External links==
*{{Official website}}
*[http://cwiki.apache.org/confluence/display/WINK/Index Apache Wink Wiki]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink1/index.html IBM DeveloperWorks Apache Wink Article]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink1/ RESTful Web services with Apache Wink, Part 1: Build an Apache Wink REST service]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink2/ RESTful Web services with Apache Wink, Part 2: Advanced topics in Apache Wink REST development]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink3/ RESTful Web services with Apache Wink, Part 3: Apache Wink and the REST]

{{Java API for RESTful Web Services}}
{{Apache}}

[[Category:Apache Software Foundation|Wink]]
[[Category:Java (programming language) libraries]]


{{web-software-stub}}</text>
      <sha1>n41tac5v8q87yzlorkk5yrjmgc410pm</sha1>
    </revision>
  </page>
  <page>
    <title>List of Apache Software Foundation projects</title>
    <ns>0</ns>
    <id>24299663</id>
    <revision>
      <id>844595594</id>
      <parentid>844595147</parentid>
      <timestamp>2018-06-05T21:47:31Z</timestamp>
      <contributor>
        <ip>2001:558:1418:48:4C94:5EF2:F016:8A16</ip>
      </contributor>
      <comment>/* Active Projects */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29229">This '''list of Apache Software Foundation projects''' contains the [[software development]] projects of the [[Apache Software Foundation]] (ASF).

Besides the projects, there are a few other distinct areas of Apache:

*[[Apache Incubator|Incubator]]: for aspiring ASF projects
*[http://attic.apache.org/ Attic]: for retired ASF projects
*[http://labs.apache.org/ Labs]: a place for innovation where committees of the foundation can experiment with new ideas
*[http://www.apache.org/dev/infrastructure.html INFRA - Apache Infrastructure Team]: provides and manages all infrastructure and services for the Apache Software Foundation, and for each project at the Foundation

== Active Projects ==

*[[Apache Accumulo|Accumulo]]: secure implementation of [[Bigtable]]
*[[Apache ActiveMQ|ActiveMQ]]: [[message broker]] supporting different communication protocols and clients, including a full [[Java Message Service]] (JMS) 1.1 client.
*[[Apache Airavata|Airavata]]: Apache Airavata is a distributed system software framework to manage simple to composite applications with complex execution and workflow patterns on diverse computational resources.
*[[Apache Allura|Allura]]: [[Python (programming language)|Python]]-based an open source implementation of a software forge
*[[Apache Ambari|Ambari]]: Apache Ambari makes Hadoop cluster provisioning, managing, and monitoring dead simple.
*[[Apache Ant|Ant]]: [[Java (programming language)|Java]]-based build tool
**[[Apache AntUnit|AntUnit]]: The Ant Library provides Ant tasks for testing Ant task, it can also be used to drive functional and integration tests of arbitrary applications with Ant
**[[Apache Ivy|Ivy]]: Apache Ivy is a very powerful dependency manager oriented toward Java dependency management, even though it could be used to manage dependencies of any kind
**[[Apache IvyDE|IvyDE]]: Integrate Ivy in Eclipse with the IvyDE plugin
*[[Apache Any23|Any23]]: Anything To Triples (Any23) is a library, a web service and a command line tool that extracts structured data in RDF format from a variety of Web documents.
*[[Apache Apex|Apex]]: Enterprise-grade unified stream and batch processing engine.
*[[Apache Portable Runtime|APR]]: Apache Portable Runtime, a portability library written in [[C (programming language)|C]]
*[[Apache Archiva|Archiva]]: Build Artifact Repository Manager
*[[Apache Aries|Aries]]: OSGi Enterprise Programming Model
*[[Apache Arrow|Arrow]]: "A high-performance cross-system data layer for columnar in-memory analytics".&lt;ref&gt;{{cite web|title=Apache Arrow|url=https://arrow.apache.org/|publisher=Apache Software Foundation|accessdate=12 May 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The Apache Software Foundation Announces Apache Arrow as a Top-Level Project|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces87|publisher=Apache Software Foundation|accessdate=12 May 2016}}&lt;/ref&gt;
*[[Apache AsterixDB|AsterixDB]]: Open source Big Data Management System
*[[Apache Atlas|Atlas]]: Scalable and extensible set of core foundational governance services
*[[Apache Aurora|Aurora]]: Mesos framework for long-running services and cron jobs
*[[Apache Avro|Avro]]: Apache Avro is a data serialization system.
*Apache Axis Committee
**[[Apache Axis|Axis]]: open source, XML based Web service framework
**[[Apache Axis2|Axis2]]: Apache Axis2 is a service hosting and consumption framework that makes it easy to use SOAP and Web Services
**[[Apache Rampart module|Rampart]]: implementation of the WS-Security standard for the Axis2 Web services engine
**[[Apache Sandesha2|Sandesha2]]: Apache Sandesha2 is an Axis2 module implementing WS-RM.
*[[Apache Bahir|Bahir]]: Extensions to distributed analytic platforms such as Apache Spark
*[[Apache Beam|Beam]], an uber-API for big data
*[[Apache Bigtop|Bigtop]]: Bigtop is a project for the development of packaging and tests of the Apache Hadoop ecosystem. 
*[[Apache Bloodhound|Bloodhound]]: defect tracker based on [[Trac]]&lt;ref&gt;{{cite web|title=Bloodhound Project Incubation Status|url=http://incubator.apache.org/projects/bloodhound.html|publisher=Apache Software Foundation|accessdate=21 March 2013}}&lt;/ref&gt;
*[[Apache BookKeeper|BookKeeper]]: A reliable replicated log service
*[[Apache Brooklyn|Brooklyn]]: Apache Brooklyn is a framework for modeling, monitoring, and managing applications through autonomic blueprints.
*[[Apache Buildr|Buildr]]: Apache Buildr is a build system for Java-based applications, including support for Scala, Groovy and a growing number of JVM languages and tools.
*[[Apache BVal|BVal]]: Bean Validation API Implementation
*[[Apache Calcite|Calcite]]: dynamic data management framework
*[[Apache Camel|Camel]]: declarative routing and mediation rules engine which implements the [[Enterprise Integration Patterns]] using a Java-based domain specific language
*[[Apache CarbonData|CarbonData]]: Apache CarbonData is an indexed columnar data format for fast analytics on big data platform, e.g. Apache Hadoop, Apache Spark, etc.
*[[Apache Cassandra|Cassandra]]: highly scalable second-generation distributed database
*[[Apache Cayenne|Cayenne]]: Java [[Object-relational mapping|ORM]] framework
*[[Apache Celix|Celix]]: Implementation of the [[OSGi]] specification adapted to C and C++.
*[[Apache Chemistry|Chemistry]]: Apache Chemistry provides open source implementations of the Content Management Interoperability Services (CMIS) specification.
*[[Apache Chukwa|Chukwa]]:  Chukwa is an open source data collection system for monitoring large distributed systems.
*[[Apache Clerezza|Clerezza]]: Clerezza is a service platform which provides a set of functionality for management of semantically linked data accessible through RESTful Web Services and in a secured way
*[[Apache CloudStack|CloudStack]]: software to deploy and manage cloud infrastructure
*[[Apache Cocoon|Cocoon]]: [[XML]] publishing framework
*[[Apache Commons|Commons]]: Reusable Java libraries and utilities too small to merit their own project
**[[Byte Code Engineering Library|BCEL]]: Bytecode Engineering Library
**[[Commons Daemon|Daemon]]: Commons Daemon
**[[Apache Jelly|Jelly]]: Jelly is a Java and XML based scripting engine. Jelly combines the best ideas from JSTL, Velocity, DVSL, Ant and Cocoon all together in a simple yet powerful scripting engine.
**[[Apache Commons Logging|Logging]]: Commons Logging is a thin adapter allowing configurable bridging to other, well known logging systems
**[[OGNL|OGNL]]: Object Graph Navigation Library
*[[Apache Cordova|Cordova]]: mobile development framework
*[[Apache CouchDB|CouchDB]]: [[Document-oriented database]]
*Apache Creadur Committee
**[[Apache Rat|Rat]]: Apache Rat improves accuracy and efficiency when reviewing and auditing releases.
**[[Apache Tentacles|Tentacles]]: Apache Tentacles simplifies the job of reviewing repository releases consisting of large numbers of artifacts.
**[[Apache Whisker|Whisker]]: Apache Whisker assists assembled applications to maintain correct legal documentation. 
*[[Apache Crunch|Crunch]]: Provides a framework for writing, testing, and running MapReduce pipelines
*[[Apache cTAKES|cTAKES]]: clinical "Text Analysis Knowledge Extraction Software" to extract information from electronic medical record clinical free-text
*[[Apache Curator|Curator]]: Apache Curator builds on [[ZooKeeper]] and handles the complexity of managing connections to the ZooKeeper cluster and retrying operations.
*[[Apache CXF|CXF]]: web services framework
*[[Apache DataFu|DataFu]]: Apache DataFu is a collection of libraries for working with large-scale data in Hadoop. 
*Apache DB Committee
**[[Apache Derby|Derby]]: pure [[Java (programming language)|Java]] [[relational database management system]]
**[[Java Data Objects|JDO]]: Java Data Objects, persistence for [[Java (programming language)|Java]] objects
**[[Apache Torque|Torque]]: [[Object-relational mapping|ORM]] for [[Java (programming language)|Java]]
*[[Apache DeltaSpike|DeltaSpike]]: DeltaSpike is a collection of portable Extensions for CDI Containers
*Apache Directory Committee
**[[Apache Directory|Directory]]: LDAP and Kerberos, entirely in Java
**[[Apache Directory Server|Directory Server]]: An extensible, embeddable LDAP and Kerberos server, entirely in Java
**[[Apache Directory Studio|Directory Studio]]: The Eclipse based LDAP browser and directory client
**[[Apache Fortress|Fortress]]: A standards-based access management system, written in Java.
**[[Apache Kerby|Kerby]]: Kerberos binding in Java
**[[Apache LDAP API|LDAP API]]: An SDK for directory access in Java.
*[[Apache DRAT|DRAT]]: large scale code license analysis, auditing and reporting
*[[Apache Drill|Drill]]: software framework that supports data-intensive distributed applications for interactive analysis of large-scale datasets
*[[Apache Eagle|Eagle]]: open source analytics solution for identifying security and performance issues instantly on big data platforms
*[[Apache Empire-db|Empire-db]]: Apache Empire-db is a lightweight relational database abstraction layer and data persistence component
*[[Apache Falcon|Falcon]]: Data governance engine
*[[Apache Felix|Felix]]: Implementation of the [[OSGi]] Release 5 core framework specification
*[[Apache Fineract|Fineract]]: Platform for Digital Financial Services
*[[Apache Flex|Flex]]: cross-platform SDK for developing and deploying rich Internet applications.
*[[Apache Flink|Flink]]: Fast and reliable large-scale data processing engine.
*[[Apache Flume|Flume]]: large scale log aggregation framework
*Apache Fluo Committee
**[[Apache Fluo|Fluo]]: Apache Fluo is a distributed processing system that lets users make incremental updates to large data sets.
**[[Apache Fluo Recipes|Fluo Recipes]]: Apache Fluo Recipes build on the Fluo API to offer additional functionality to developers. 
**[[Apache Fluo YARN|Fluo YARN]]: Apache Fluo YARN is a tool for running Apache Fluo applications in Apache Hadoop YARN.
*[[Apache Forrest|Forrest]]: documentation framework based upon Cocoon
*[[Apache FreeMarker|FreeMarker]]: FreeMarker is a template engine, i.e. a generic tool to generate text output based on templates. FreeMarker is implemented in Java as a class library for programmers.
*[[Apache Geode|Geode]]: Low latency, high concurrency data management solutions
*[[Apache Geronimo|Geronimo]]: [[Java Platform, Enterprise Edition|Java EE]] server
*[[Apache Giraph|Giraph]]: scalable Graph Processing System
*[[Apache Gora|Gora]]: The Apache Gora open source framework provides an in-memory data model and persistence for big data.
*[[Apache Groovy|Groovy]]: an object-oriented, dynamic programming language for the Java platform
*[[Apache Guacamole|Guacamole]]: HTML5 web application for accessing remote desktops
*[[Apache Gump|Gump]]: [[digital integration|integration]], [[coupling (computer science)|dependencies]], and [[Version control|versioning]] management
*[[Apache Hadoop|Hadoop]]: [[Java (programming language)|Java]] [[software framework]] that supports data intensive distributed applications
*[[Apache Hama|Hama]]: Hama is an efficient and scalable general-purpose BSP computing engine.
*[[Apache HBase|HBase]]: Apache HBase software is the Hadoop database. Think of it as a distributed, scalable, big data store.
*[[Apache Helix|Helix]]: A cluster management framework for partitioned and replicated distributed resources
*[[Apache Hive|Hive]]: The Apache Hive data warehouse software facilitates querying and managing large datasets residing in distributed storage.
*[[Apache HTTP Server|HTTP Server]]:  The Apache HTTP Server application 'httpd'
**[[mod_python]]: module that integrates the [[Python (programming language)|Python]] interpreter into Apache server. Deprecated in favour of [[mod_wsgi]].
*[[Apache HttpComponents]]: Low-level Java libraries for [[HTTP]]
*[[Apache Ignite|Ignite]]: Apache Ignite is an In-Memory Data Fabric providing in-memory data caching, partitioning, processing, and querying components.
*[[Apache Impala|Impala]]: A high-performance distributed SQL engine
*[[Apache Isis|Isis]]: Apache Isis is a framework for rapidly developing domain-driven apps in Java
*[[Apache Jackrabbit|Jackrabbit]]: implementation of the [[content repository API for Java|Java Content Repository API]]
*[[Apache James|JAMES]]: [[Java (programming language)|Java]] [[email]] and [[usenet|news]] server
*[[Apache jclouds|jclouds]]: open source multi-cloud toolkit for the Java platform
*[[Apache Jena|Jena]] is an [[open source]] [[Semantic Web]] framework for [[Java (programming language)|Java]]
*[[Apache JMeter|JMeter]]: Pure Java application for load and functional testing
*[[Apache Johnzon|Johnzon]]: JSR-353 compliant JSON parsing; modules to help with JSR-353 as well as JSR-374 and JSR-367
*[[Apache JSPWiki|JSPWiki]]: A feature-rich and extensible WikiWiki engine built around the standard J2EE components (Java, servlets, JSP).
*Apache jUDDI Committee
**[[Apache Scout|Scout]]: Apache Scout is an implementation of the JSR 93 (JAXR).
*[[Apache Juneau|Juneau]]: A toolkit for marshalling POJOs to a wide variety of content types using a common framework
*[[Apache Kafka|Kafka]]: Message broker software
*[[Apache Karaf|Karaf]]: Apache Karaf is an OSGi distribution for server-side applications.
*[[Apache Kibble|Kibble]]: Apache Kibble is a suite of tools for collecting, aggregating and visualizing activity in software projects.
*[[Apache Knox|Knox]]: A REST API Gateway for Hadoop Services
*[[Apache Kudu|Kudu]]: A distributed columnar storage engine built for the Apache Hadoop ecosystem
*[[Apache Kylin|Kylin]]: distributed analytics engine
*[[Apache Lens|Lens]]: Unified Analytics Interface
*[[Apache Libcloud|Libcloud]]: Apache Libcloud is a standard Python library that abstracts away differences among multiple cloud provider APIs.
*Apache Logging Services Committee
**[[Chainsaw_(log_file_viewer)|Chainsaw]]: Apache Chainsaw is a GUI log viewer.
**[[Apache Log4cxx|Log4cxx]]: Apache log4cxx provides logging services for C++.
**[[Log4j|Log4j]]: Apache Log4j
**[[Apache Log4net|Log4net]]: Apache log4net provides logging services for .NET.
**[[Apache Log4php|Log4php]]: Apache log4php is a logging framework for PHP.
*Apache Lucene Committee
**[[Apache Lucene|Lucene Core]]: A high-performance, full-featured text search engine library
**[[Apache Solr|Solr]]: enterprise search server based on the Lucene Java search library
*[[Apache Lucene.Net|Lucene.Net]]: Lucene.Net is a port of the Lucene search engine library, written in C# and targeted at .NET runtime users.
*[[Apache Lucy|Lucy]]: The Apache Lucy search engine library provides full-text search for dynamic programming languages.
*[[Apache MADlib|MADlib]]: Scalable, Big Data, SQL-driven machine learning framework for Data Scientists
*[[Apache Mahout|Mahout]]: machine learning and data mining solution. [http://mahout.apache.org/ Mahout]
*[[Apache ManifoldCF|ManifoldCF]]: Open-source software for transferring content between repositories or search indexes
*[[Apache Marmotta|Marmotta]]: An Open Platform for Linked Data
*[[Apache Maven|Maven]]: [[Java (programming language)|Java]] [[project management]] and comprehension tool
**[[Doxia|Doxia]]: Doxia is a content generation framework, which supports many markup languages.
*[[Apache Mesos|Mesos]]:  open-source cluster manager
*[[Apache MetaModel|MetaModel]]: Providing a common interface for discovery, exploration of metadata and querying of different types of data sources.
*[[Apache Metron|Metron]]: Real-time big data security
*Apache MINA Committee
**[[Apache FtpServer|FtpServer]]: [[FTP server]] written entirely in Java
**[[Apache MINA|MINA]]: Multipurpose Infrastructure for Network Application, a framework to develop high performance and high scalability network applications. [http://mina.apache.org/ MINA]
**[[Apache SSHD|SSHD]]: Apache SSHD is a 100% pure java library to support the SSH protocols on both the client and server side.
**[[Apache Vysper|Vysper]]: Apache Vysper aims to be a modular, full featured XMPP (Jabber) server. Vysper is implemented in Java.
*[[Apache Mnemonic|Mnemonic]]: a transparent nonvolatile hybrid memory oriented library for Big data, High-performance computing, and Analytics
*Apache MyFaces Committee
**[[Apache MyFaces|MyFaces]]: [[JavaServer Faces]] implementation
**[[Apache Tobago|Tobago]]: Set of user interface components based on JSF
*[[Apache Mynewt|Mynewt]]: embedded OS optimized for networking and built for remote management of constrained devices
*[[Apache NiFi|NiFi]]: Easy to use, powerful, and reliable system to process and distribute data
*[[Apache Nutch|Nutch]]: a highly extensible and scalable open source [[web crawler]]
*[[Apache ODE|ODE]]: Apache ODE is a WS-BPEL implementation that supports web services orchestration using flexible process definitions.
*[[Apache OFBiz|OFBiz]]: Open for Business: enterprise automation software
*[[Apache Olingo|Olingo]]: Client and Server for OData
*[[Apache OODT|OODT]]: Object Oriented Data Technology, a data management framework for capturing and sharing data
*[[Apache Oozie|Oozie]]: Oozie is a workflow scheduler system to manage Apache Hadoop jobs.
*[[Apache Open Climate Workbench|Open Climate Workbench]]: A comprehensive suite of algorithms, libraries, and interfaces designed to standardize and streamline the process of interacting with large quantities of observational data and conducting regional climate model evaluations.
*[[Apache OpenJPA|OpenJPA]]: Java Persistence API Implementation
*[[Apache OpenMeetings|OpenMeetings]]:  Video conferencing, instant messaging, white board and collaborative document editing application.
*[[Apache OpenNLP|OpenNLP]]: [[natural language processing]] toolkit.
*[[Apache OpenOffice|OpenOffice]]: An open-source, office-document productivity suite
*[[Apache OpenWebBeans|OpenWebBeans]]: Dependency Injection Platform
*[[Apache ORC|ORC]]: A columnar file format for big data workloads
*[[Apache Parquet|Parquet]]: A general-purpose columnar storage format
*[[Apache PDFBox|PDFBox]]: Java based PDF library (reading, text extraction, manipulation, viewer)
*[[Mod_perl|Mod_perl]]: module that integrates the [[Perl]] interpreter into Apache server
*[[Apache Phoenix|Phoenix]]: [[SQL]] layer on [[Apache HBase|HBase]]
*[[Apache Pig|Pig]]: A platform for analyzing large data sets on Hadoop
*[[Apache Pivot|Pivot]]: A platform for building rich internet applications in Java
*[[Apache POI|POI]]: Poor Obfuscation Implementation, a library for reading and writing [[Microsoft Office]] formats
*[[Apache Polygene|Polygene]]: Exploration of Composite Oriented Programming for domain-centric application development
*[[Apache Portals|Portals]]: [[web portal]] related software
*[[Apache PredictionIO|PredictionIO]]: PredictionIO is an open source Machine Learning Server built on top of state-of-the-art open source stack, that enables developers to manage and deploy production-ready predictive services for various kinds of machine learning tasks.
*[[Apache Qpid|Qpid]]: [[Advanced Message Queuing Protocol|AMQP]] messaging system in [[Java (programming language)|Java]] and [[C++]]
*[[Apache Ranger|Ranger]]: A framework to enable, monitor and manage comprehensive data security across the Hadoop platform.
*[[Apache REEF|REEF]]: A scale-out computing fabric that eases the development of Big Data applications on top of resource managers such as Apache YARN and Mesos. 
*[[Apache River|River]]: Apache River software provides a standards-compilani JINI service.
*[[Apache RocketMQ|RocketMQ]]: A fast, low latency, reliable, scalable, distributed, easy to use message-oriented middleware, especially for processing large amounts of streaming data
*[[Apache Roller|Roller]]: Apache Roller is a full-featured, multi-user and group blog server suitable for both small and large blog sites.
*[[Apache Royale|Royale]]: Improving developer productivity in creating applications for wherever Javascript runs (and other runtimes)
*[[Apache Samza|Samza]]: Stream Processing Framework
*[[Apache Santuario|Santuario]]: XML Security in Java and C++
*[[Apache Sentry|Sentry]]: Fine grained authorization to data and metadata in Apache Hadoop
*[[Apache Serf|Serf]]: High performance C-based HTTP client library built upon the Apache Portable Runtime (APR) library.
*[[Apache ServiceMix|ServiceMix]]: [[enterprise service bus]] that supports [[Java Business Integration|JBI]] and [[OSGi]]
*[[Apache Shiro|Shiro]]: A simple to use Java Security Framework
*Apache SIS Committee
**[[Apache Spatial Information System|Spatial Information System]]: A library for developing geospatial applications
*[[Apache Sling|Sling]]: Innovative Web framework based on JCR and OSGi
*[[Apache SpamAssassin|SpamAssassin]]: email filter used to identify [[e-mail spam|spam]]
*[[Apache Spark|Spark]]: open source cluster computing framework
*[[Apache Sqoop|Sqoop]]: A tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.
*[[Apache Stanbol|Stanbol]]: Software components for semantic content management
*[[Apache Steve|Steve]]: Apache STeVe is a collection of online voting tools, used by the ASF, to handle STV and other voting methods.
*[[Apache Storm|Storm]]: Apache Storm is a distributed real-time computation system.
*[[Apache Streams|Streams]]: Interoperability of online profiles and activity feeds
*[[Apache Struts|Struts]]: [[Java (programming language)|Java]] web applications framework
*[[Apache Subversion|Subversion]]: open source [[Revision control|version control]] (client/server) system
*[[Apache Synapse|Synapse]]: A lightweight and high-performance Enterprise Service Bus (ESB)
*[[Apache Syncope|Syncope]]: Apache Syncope is an Open Source system for managing digital identities in enterprise environments.
*[[Apache SystemML|SystemML]]: Scalable machine learning
*[[Apache Tajo|Tajo]]: Relational data warehousing system. It using the hadoop file system as distributed storage.
*[[Apache Tapestry|Tapestry]]: Component-based Java web framework
*Apache Tcl Committee
**[[Tcl]] integration for Apache httpd
**[[Apache Rivet|Rivet]]: Server-side Tcl programming system combining ease of use and power
**[[Apache Websh|Websh]]: Websh is a rapid development environment for building powerful, fast, and reliable web applications in Tcl
*[[Apache Tez|Tez]]: Apache Tez is an effort to develop a generic application framework which can be used to process arbitrarily complex directed-acyclic graphs (DAGs) of data-processing tasks and also a re-usable set of data-processing primitives which can be used by other projects.
*[[Apache Thrift|Thrift]] : Interface definition language and binary communication protocol that is used to define and create services for numerous languages.
*[[Apache Tika|Tika]]: content analysis toolkit for extracting metadata and text from digital documents of various types, e.g., audio, video, image, office suite, web, mail, and binary
*[[Apache Tiles|Tiles]]: Apache Tiles is a templating framework built to simplify the development of web application user interfaces.
*[[Apache TinkerPop|TinkerPop]]: A graph computing framework for both graph databases (OLTP) and graph analytic systems (OLAP)
*[[Apache Tomcat|Tomcat]]: [[web container]] for serving servlets and JSP
**[[Apache RDC Taglib|Reusable Dialog Components (RDC) Taglib]]: A framework for creating JSP taglibs that aid in rapid development of voice and multimodal applications. 
*[[Apache TomEE|TomEE]]: Apache TomEE is an all-Apache Java EE 6 Web Profile stack for Apache Tomcat
*[[Apache Traffic Control|Traffic Control]]: Built around Apache Traffic Server as the caching software, Traffic Control implements all the core functions of a modern CDN. [http://trafficcontrol.apache.org/ Traffic Control]
*[[Apache Traffic Server|Traffic Server]]: HTTP/1.1 compliant caching proxy server. [http://trafficserver.apache.org/ Traffic Server]
*[[Apache Trafodion|Trafodion]]: Webscale SQL-on-Hadoop solution enabling transactional or operational workloads on Apache Hadoop.
*[[Apache Turbine|Turbine]]: Turbine is a servlet based framework that allows Java developers to quickly build web applications.
*[[Apache Twill|Twill]]: Use Apache Hadoop YARN's distributed capabilities with a programming model that is similar to running threads
*[[Apache UIMA|UIMA]]: unstructured content analytics framework
*[[Apache Usergrid|Usergrid]]: Usergrid is an open-source Backend-as-a-Service ("BaaS" or "mBaaS") composed of an integrated distributed NoSQL database, application layer and client tier with SDKs for developers looking to rapidly build web and/or mobile applications.
*[[Apache VCL|VCL]]: A cloud computing platform for provisioning and brokering access to dedicated remote compute resources.
*Apache Velocity Committee:
**[[Apache Anakia|Anakia]]: Anakia is an XML transformation tool which uses JDOM and Velocity to transform XML documents into the format of your choice.
**[[Apache Texen|Texen]]: Texen is a general purpose text generating utility based on Apache Velocity and Apache Ant.
**[[Apache Velocity|Velocity]]: Java template creation engine
**[[Apache Velocity DVSL]]: A tool modeled after XSLT and intended for general XML transformations using the Velocity Template Language.
**[[Apache Velocity Tools]]: Tools and infrastructure for the template engine
*[[Apache VXQuery|VXQuery]]: Apache VXQuery implements a parallel XML Query processor.
*Apache Web Services Committee
**[[Apache Axiom|Axiom]]: Apache Axiom is an XML object model supporting deferred parsing.
**[[Apache Woden|Woden]]: Apache Woden is used to develop a Java class library for reading, manipulating, creating and writing WSDL documents.
*[[Apache Whimsy|Whimsy]]: Tools that display and visualize various bits of data related to ASF organizations and processes.
*[[Apache Wicket|Wicket]]: Component-based Java web framework
*[[Apache Xalan|Xalan]]: XSLT processors in Java and C++
*[[Apache Xerces|Xerces]]: validating XML parser
*Apache XML Graphics Committee
**[[Apache Batik|Batik]]: pure Java library for [[Scalable Vector Graphics|SVG]] content manipulation
**[[Apache FOP|FOP]]: Java print formatter driven by XSL formatting objects (XSL-FO); supported output formats include PDF, PS, PCL, AFP, XML (area tree representation), Print, AWT and PNG, and to a lesser extent, RTF and TXT
**[[Apache XML Graphics Commons|XML Graphics Commons]]: Common components for Apache Batik and Apache FOP
*[[Apache Yetus|Yetus]]: A collection of libraries and tools that enable contribution and release processes for software projects.
*[[Apache Zeppelin|Zeppelin]]: Zeppelin is a collaborative data analytics and visualization tool for distributed, general-purpose data processing systems
*[[Apache ZooKeeper|ZooKeeper]]: Coordination service for distributed applications

==Incubating projects==

*[[Apache NetBeans|NetBeans (incubating)]]: [[Integrated development environment]] and software development [[Platform (computing)|platform]] for development in [[Java (programming language)|Java]] and other languages
*[[Apache Singa|Singa (incubating)]]: distributed [[deep learning]] platform.

==Retired Projects==

A retired project is one which has been closed down on the initiative of the board, the project its PMC, the PPMC or the IPMC for various reasons. It is no longer developed at the Apache Software Foundation and does not have any other duties.

*[[Apache Abdera|Abdera]]: implementation of the [[Atom (standard)|Atom Syndication Format and Atom Publishing Protocol]]
*[[Apache Beehive|Beehive]]: Java visual object model
*[[Apache Continuum|Continuum]]: continuous integration server
*[[Apache Excalibur|Excalibur]]: [[Inversion of Control]] container named [[Fortress]] and related components
*[[Apache Harmony|Harmony]]: [[Java (programming language)|Java]] implementation (Retired)
*[[Apache HiveMind|HiveMind]]: Services and configuration microkernel
*[[Apache iBATIS|iBATIS]]: [[Persistence framework]] which enables mapping [[sql]] queries to [[POJO]]s
*[[Apache Lenya|Lenya]]: [[content management system]] (CMS) based on [[Apache Cocoon]]
*[[Apache Muse|Muse]]: implementation of the WS-ResourceFramework ([[Web Services Resource Framework|WSRF]]), WS-BaseNotification ([[WSN]]), and WS-DistributedManagement ([[Web Services Distributed Management|WSDM]]) specifications
*[[Apache Shale|Shale]]: Web application framework based on JavaServer Faces
*[[Apache Stratos|Stratos]]: Platform-as-a-Service (PaaS) framework
*[[Apache Tuscany|Tuscany]]: [[service component architecture|SCA]] implementation, also providing other [[service-oriented architecture|SOA]] implementations
*[[Apache Wave|Wave]]: online real-time collaborative editing
*[[Apache XMLBeans (retired)]]: [[XML]]–[[Java (programming language)|Java]] binding tool
*[[Jakarta Project|Jakarta]]: server side [[Java (programming language)|Java]], including its own set of subprojects

==References==

{{reflist}}
{{cite web
|url=https://projects.apache.org/projects.html?name
|title=Apache Project List
|publisher=The Apache Software Foundation
|year=2018
|accessdate=2018-05-19
}}

[[Category:Apache Software Foundation| ]]
[[Category:Apache Software Foundation projects| ]]</text>
      <sha1>bnky8ezagqv5ja5on39ip4kwxd3r027</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Tomcat</title>
    <ns>0</ns>
    <id>200698</id>
    <revision>
      <id>847863525</id>
      <parentid>847717837</parentid>
      <timestamp>2018-06-28T09:01:49Z</timestamp>
      <contributor>
        <ip>42.150.126.194</ip>
      </contributor>
      <comment>/* Releases */ Version Update</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18248">{{multiple issues|
{{lead too short|date=October 2013}}
{{primary sources|date=February 2013}}
}}
{{Infobox software
| name                   = Apache Tomcat
| logo                   = Tomcat-logo.svg
| screenshot             = Apache-tomcat-frontpage-epiphany-browser.jpg
| caption                = Apache Tomcat Default Page
| developer              = [[Apache Software Foundation]]
| released               = {{Start date and age|1999}}
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Java Servlet#Servlet containers|Servlet container]]&lt;br /&gt;[[Hypertext Transfer Protocol|HTTP]] [[web server]]
| license                = [[Apache License]] 2.0
| website                = {{URL|tomcat.apache.org}}

}}
'''Apache Tomcat''', often referred to as '''Tomcat Server''', is an open-source [[Servlet container|Java Servlet Container]] developed by the [[Apache Software Foundation]] (ASF). Tomcat implements several [[Java Platform, Enterprise Edition|Java EE]] specifications including [[Java Servlet]], [[JavaServer Pages]] (JSP), [[Unified Expression Language|Java EL]], and [[WebSocket]], and provides a "pure [[Java (programming language)|Java]]" [[Hypertext Transfer Protocol|HTTP]] [[web server]] environment in which [[Java (programming language)|Java]] code can run.

Tomcat is developed and maintained by an open community of developers under the auspices of the Apache Software Foundation, released under the [[Apache License]] 2.0 license, and is [[open-source software]].

==Components==
Tomcat 4.x was released with Catalina (a servlet container), Coyote (an HTTP connector) and Jasper (a [[JSP engine]]).

===Catalina===
Catalina is Tomcat's  [[Web container|servlet container]]. Catalina implements [[Sun Microsystems]]'s specifications for [[Java servlet|servlet]] and [[JavaServer Pages]] (JSP). In Tomcat, a Realm element represents a "database" of usernames, passwords, and roles (similar to Unix groups) assigned to those users. Different implementations of Realm allow Catalina to be integrated into environments where such authentication information is already being created and maintained, and then use that information to implement Container Managed Security as described in the Servlet Specification.&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/tomcat-5.5-doc/config/realm.html |title=Apache Tomcat Configuration Reference - The Realm Component |publisher=Tomcat.apache.org |date= |accessdate=2013-11-01}}&lt;/ref&gt;

===Coyote===
Coyote is a Connector component for Tomcat that supports the HTTP 1.1 protocol as a web server. This allows Catalina, nominally a Java Servlet or JSP container, to also act as a plain web server that serves local files as HTTP documents.&lt;ref&gt;[http://tomcat.apache.org/tomcat-4.1-doc/config/coyote.html The Coyote HTTP/1.1 Connector], Apache&lt;/ref&gt;

Coyote listens for incoming connections to the server on a specific [[Transmission Control Protocol|TCP]] port and forwards the request to the Tomcat Engine to process the request and send back a response to the requesting client. Another Coyote Connector, Coyote JK, listens similarly but instead forwards its requests to another web server, such as Apache, using the [[Apache JServ Protocol|JK protocol]].&lt;ref&gt;[http://tomcat.apache.org/tomcat-4.1-doc/config/coyotejk.html The Coyote JK Connector, Apache]&lt;/ref&gt; This usually offers better performance. {{citation needed|date=September 2016}}

===Jasper===
Jasper is Tomcat's JSP Engine. Jasper [[Parsing|parse]]&lt;nowiki/&gt;s [[JSP files]] to compile them into Java code as servlets (that can be handled by Catalina). At runtime, Jasper detects changes to JSP files and recompiles them.

As of version 5, Tomcat uses Jasper 2, which is an implementation of the [[Sun Microsystems]]'s [[JavaServer Pages|JSP]] 2.0 specification. From Jasper to Jasper 2, important features were added:
* JSP Tag library pooling - Each tag markup in JSP file is handled by a tag handler class. Tag handler class objects can be pooled and reused in the whole JSP servlet.
* Background JSP compilation - While recompiling modified JSP Java code, the older version is still available for server requests. The older JSP servlet is deleted once the new JSP servlet has finished being recompiled.
* Recompile JSP when included page changes - Pages can be inserted and included into a JSP at runtime. The JSP will not only be recompiled with JSP file changes but also with included page changes.
* JDT Java compiler - Jasper 2 can use the Eclipse JDT (Java Development Tools) Java compiler instead of [[Apache Ant|Ant]] and &lt;code&gt;[[javac]]&lt;/code&gt;.

Three new components were added with the release of Tomcat 7:

===Cluster===
This component has been added to manage large applications. It is used for [[Load balancing (computing)|load balancing]] that can be achieved through many techniques. Clustering support currently requires the JDK version 1.5 or higher.

===High availability===
A high-availability feature has been added to facilitate the scheduling of system upgrades (e.g. new releases, change requests) without affecting the live environment. This is done by dispatching live traffic requests to a temporary server on a different port while the main server is upgraded on the main port. It is very useful in handling user requests on high-traffic web applications.&lt;ref&gt;{{cite web |url=http://www.javaworld.com/javaworld/jw-12-2004/jw-1220-tomcat.html |title=High availability Tomcat - Connect Tomcat servers to Apache and to each other to keep your site running |last1=King |first1=Graham |date=2004-12-20 |publisher=[[JavaWorld]] |accessdate=2013-02-13}}&lt;/ref&gt;

===Web application===
It has also added user- as well as system-based web applications enhancement to add support for deployment across the variety of environments. It also tries to manage sessions as well as applications across the network.

Tomcat is building additional components.  A number of additional components may be used with Apache Tomcat. These components may be built by users should they need them or they can be downloaded from one of the mirrors.&lt;ref&gt;{{cite web|author=Remy Maucherat |url=https://tomcat.apache.org/tomcat-7.0-doc/extras.html |title=Apache Tomcat 7 (7.0.47) - Additional Components |publisher=Tomcat.apache.org |date=2013-10-18 |accessdate=2013-11-01}}&lt;/ref&gt;

==Features==
Tomcat 7.x implements the Servlet 3.0 and JSP 2.2 specifications.&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/whichversion.html|title=Apache Tomcat Versions|accessdate=2011-11-12}}&lt;/ref&gt; It requires Java version 1.6, although previous versions have run on Java 1.1 through 1.5. Versions 5 through 6 saw improvements in [[Garbage collection (computer science)|garbage collection]], JSP parsing, performance and scalability. Native wrappers, known as "Tomcat Native", are available for [[Microsoft Windows]] and Unix for platform integration.

Tomcat 8.x implements the Servlet 3.1 and JSP 2.4 Specifications. &lt;ref&gt;[https://tomcat.apache.org/download-80.cgi  The Apache Software Foundation Apache Tomcat®]&lt;/ref&gt;Apache Tomcat 8.5.x is intended to replace 8.0.x and includes new features pulled forward from Tomcat 9.0.x. The minimum Java version and implemented specification versions remain unchanged.&lt;ref&gt;[http://tomcat.apache.org/tomcat-8.5-doc/index.html v8.5.x Online Manual]&lt;/ref&gt;

==History==
Tomcat started off as a servlet [[reference implementation (computing)|reference implementation]] by [[James Duncan Davidson]], a software architect at [[Sun Microsystems]]. He later helped make the project [[open source]] and played a key role in its donation by [[Sun Microsystems]] to the Apache Software Foundation. The [[Apache Ant]] software build automation tool was developed as a side-effect of the creation of Tomcat as an open source project.

Davidson had initially hoped that the project would become open sourced and, since many open source projects had [[O'Reilly Media|O'Reilly]] books associated with them featuring an animal on the cover, he wanted to name the project after an animal. He came up with ''[[Cat|Tomcat]]'' since he reasoned the animal represented something that could fend for itself. Although the tomcat was already in use for another O'Reilly title,&lt;ref&gt;{{Citation |title=UML in a Nutshell |url=http://shop.oreilly.com/product/9781565924482.do |author=Sinan Si Alhir  |publisher=O'Reilly Books |isbn=978-1-56592-448-2 |pages=296 | year = 1998 |postscript=&lt;!--none--&gt;}}&lt;/ref&gt; his wish to see an animal cover eventually came true when O'Reilly published their Tomcat book with a [[snow leopard]] on the cover in 2003.&lt;ref&gt;{{citation 
| first1     = Jason
| last1      = Brittain
| first2     = Ian F.
| last2      = Darwin
| title      = Tomcat: The Definitive Guide 
| edition    = 1st
| publisher  = [[O'Reilly Media]]
| isbn       = 978-0596003180 
| url        = http://shop.oreilly.com/product/9780596003180.do 
| page       = 320 
| date       = 
| accessdate = 2018-03-01
| postscript =&lt;!--none--&gt;
}}&lt;/ref&gt;

===Releases===
{| class="wikitable sortable"  style="margin:auto; margin:0 0 0 2em;"
|+ Apache Tomcat versions
|-
! Series
! Declared stable
! Description
! Latest release
! Latest release date
|-
|2.0
|&lt;!-- reference implementation hence no stable version, was not distributed --&gt;
|Tomcat started off in November 1998&lt;ref&gt;{{cite web|url=http://technotif.com/the-origin-story-of-tomcat/|title=The Origin Story of Tomcat|publisher=TechNotif|accessdate=23 July 2017}}&lt;/ref&gt; as a servlet [[reference implementation (computing)|reference implementation]] by [[James Duncan Davidson]], a software architect at [[Sun Microsystems]].
|&lt;!-- same as "stable" date, ref implementation was not deployed was: 2.5.0--&gt;
|
|-
| {{Version|o|3.0}}
| 1999
| Initial release. Merger of donated Sun Java Web Server code and ASF and Implements [[Java Servlet|Servlet 2.2]], and [[JavaServer Pages|JSP 1.1]] specifications.
| 3.3.2
| 2004-03-09
|-
| {{Version|o|4.1}}
| 2002-09-06&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/jakarta-announcements/200209.mbox/%3C3D789475.60405@apache.org%3E|title=[ANNOUNCEMENT] Tomcat 4.1.10 Stable|publisher=|accessdate=21 September 2015}}&lt;/ref&gt;
| First Apache Tomcat release to support the [[Java Servlet|Servlet 2.3]] and [[JavaServer Pages|JSP 1.2]]&lt;ref name=WhichVersion&gt;https://tomcat.apache.org/whichversion.html&lt;/ref&gt; specifications.
| 4.1.40
| 2009-06-25
|-
| {{Version|o|5.0}}
| 2003-12-03 &lt;!-- http://mail-archives.apache.org/mod_mbox/jakarta-announcements/200312.mbox/%3C3FCE6283.4010801%40apache.org%3E http://jakarta.apache.org/site/news/news-2003.html#20031203.1 --&gt;
| First Apache Tomcat release to support the [[Java Servlet|Servlet 2.4]], [[JavaServer Pages|JSP 2.0]], and [[Unified Expression Language|EL 1.1]]&lt;ref name=WhichVersion/&gt; specifications.
| 5.0.30
| 2004-08-30
|-
| {{Version|o|5.5}}&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/tomcat-55-eol.html|title=Apache Tomcat|author=Apache Tomcat Project|publisher=|accessdate=21 September 2015}}&lt;/ref&gt;
| 2004-11-10&lt;ref&gt;{{cite web|url=http://jakarta.apache.org/site/news/news-2004-2ndHalf.html#20041110.1|title=The Jakarta Site - News and Status|publisher=|accessdate=21 September 2015}}&lt;/ref&gt;
| Designed for J2SE 5.0. Inclusion of Eclipse JDT allows Tomcat to run without a full Java Development Kit being installed.
| 5.5.36
| 2012-10-10
|-
| {{Version|o|6.0}}
| 2007-02-28&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/www-announce/200703.mbox/%3C45E61194.5050003%40apache.org%3E|title=[ANN] Apache Tomcat 6.0.10 released|publisher=|accessdate=21 September 2015}}&lt;/ref&gt;
| First Apache Tomcat release to support the [[Java Servlet|Servlet 2.5]], [[JavaServer Pages|JSP 2.1]],&lt;ref name=WhichVersion/&gt; and [[Unified Expression Language|EL 2.1]] specifications.
| 6.0.53
| 2017-04-07
|-
| {{Version|co|7.0}}
| 2011-01-14&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/oldnews-2011.html#Tomcat_7.0.6_Released|title=Apache Tomcat|author=Apache Tomcat Project|publisher=|accessdate=21 September 2015}}&lt;/ref&gt;
| First Apache Tomcat release to support the [[Java Servlet|Servlet 3.0]], [[JavaServer Pages|JSP 2.2]], [[Unified Expression Language|EL 2.2]], and WebSocket&lt;ref name=WhichVersion/&gt; specifications.
| 7.0.88
| 2018-05-11
|-
| {{Version|co|8.0}}
| 2014-06-25&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/oldnews-2014.html#Tomcat_8.0.9_Released|title=Apache Tomcat|author=Apache Tomcat Project|publisher=|accessdate=5 October 2015}}&lt;/ref&gt;
| First Apache Tomcat release to support the [[Java Servlet|Servlet 3.1]], [[JavaServer Pages|JSP 2.3]], and [[Unified Expression Language|EL 3.0]]&lt;ref name=WhichVersion/&gt; specifications.
| 8.0.52
| 2018-05-08
|-
| {{Version|co|8.5}}
| 2016-06-13
| Adds support for HTTP/2, OpenSSL for JSSE, TLS virtual hosting and JASPIC 1.1. Created from Tomcat 9, following delays to Java EE 8.
| 8.5.32
| 2018-06-25
|-
| {{Version|c|9.0}}
| 2018-01-18
| First Apache Tomcat release to support the [[Java Servlet|Servlet 4.0]], [[JavaServer Pages|JSP 2.4 (TBD)]], and [[Unified Expression Language|EL 3.1 (TBD)]]&lt;ref name=WhichVersion/&gt; specifications.
| 9.0.10
| 2018-06-25
|-
|colspan="5"|&lt;small&gt;{{Version|l|show=011110}}&lt;/small&gt;
|}

==Communities==
Apache software is built as part of a community process that involves both user and developer [http://tomcat.apache.org/lists.html mailing lists]. The developer list is where discussion on building and testing the next release takes place, while the user list is where users can discuss their problems with the developers and other users.

Some of the free Apache Tomcat resources and communities include [http://www.tomcatexpert.com/ Tomcatexpert.com] (a [[SpringSource]] sponsored community for developers and operators who are running Apache Tomcat in large-scale production environments) and MuleSoft's [http://www.mulesoft.com/understanding-apache-tomcat/ Apache Tomcat Resource Center] (which has instructional guides on installing, updating, configuring, monitoring, troubleshooting and securing various versions of Tomcat).

==Apache TomEE==
[[Apache TomEE]] (pronounced "Tommy") is the [[Java Enterprise Edition]] of Apache Tomcat (Tomcat + Java EE = TomEE) that combines several Java enterprise projects including [[Apache OpenEJB]], Apache OpenWebBeans, [[Apache OpenJPA]], [[Apache MyFaces]] and others.&lt;ref&gt;{{cite web|url=http://openejb.apache.org/apache-tomee.html|title=Apache TomEE|publisher=[[Apache OpenEJB]]}}&lt;/ref&gt; In October 2011, the project obtained certification by [[Oracle Corporation]] as a compatible implementation of the Java EE 6 Web Profile.&lt;ref&gt;{{cite web|url=http://www.marketwatch.com/story/the-apache-software-foundation-announces-apache-tomee-certified-as-java-ee-6-web-profile-compatible-2011-10-04|title=The Apache Software Foundation Announces Apache TomEE Certified as Java EE 6 Web Profile Compatible|publisher=[[MarketWatch]]|date=4 Oct 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.infoworld.com/d/application-development/apache-tomee-web-stack-gains-approval-175341|title=Apache TomEE Web stack gains approval|publisher=[[InfoWorld]]|date=7 Oct 2011}}&lt;/ref&gt;

==See also==
* [[MuleSoft]], producer of Tcat, an enterprise Tomcat server
* [[Apache Geronimo]], an application server that can use Tomcat as its web container
* [[Resin Server]] Application Server from [[Caucho Technology]]
* [[WildFly]], formerly known as JBoss Application Server
* [[Jetty (web server)]]
* [[JOnAS]], application server that can use Tomcat as its web container
* [[Apache OpenEJB]], can be added to Tomcat to turn it into a JavaEE server
* [[GlassFish]], the [[reference implementation]] of Java EE, supporting [[Enterprise JavaBeans|EJB]], [[Java Persistence API|JPA]], [[JavaServer Faces|JSF]], [[Java Message Service|JMS]], [[Java remote method invocation|Java RMI]], [[JavaServer Pages|JSP]], servlets etc.
* [[Comparison of web server software|Comparison of web servers]].

== References ==
{{reflist}}

==Bibliography==
{{refbegin}}
*{{citation 
| first1     = Jason
| last1      = Brittain
| first2     = Ian F.
| last2      = Darwin
| title      = Tomcat: The Definitive Guide 
| edition    = 1st
| publisher  = [[O'Reilly Media]]
| isbn       = 978-0596003180 
| url        = http://shop.oreilly.com/product/9780596003180.do 
| page       = 320 
| date       = 
| accessdate = 2018-03-01
| postscript =&lt;!--none--&gt;
}}
*{{citation
| first1     = Jason
| last1      = Brittain
| first2     = Ian F.
| last2      = Darwin
| title      = Tomcat: The Definitive Guide
| edition    = 2nd
| publisher  = [[O'Reilly Media]]
| isbn       = 978-0596101060
| url        = http://shop.oreilly.com/product/9780596101060.do
| page       = 496
| date       = 
| accessdate = 2018-03-01
| postscript = &lt;!--none--&gt;
}}
*{{citation
| first1     = Vivek
| last1      = Chopra
| first2     = Sing
| last2      = Li
| first3     = Jeff
| last3      = Genender
| date       = August 13, 2007
| title      = Professional Apache Tomcat 6
| edition    = 1st
| publisher  = [[Wrox Press|Wrox]]
| page      = 629
| isbn       = 978-0-471-75361-2
| url        = http://www.wrox.com/WileyCDA/WroxTitle/productCd-0471753610.html
| accessdate        = 2009-10-08
| postscript     = &lt;!--none--&gt;
}}
*{{citation
| first1     = Matthew
| last1      = Moodie
| first2     = Kunal
| last2      = Mittal (Ed.)
| date       = March 22, 2007
| title      = Pro Apache Tomcat 6
| edition    = 1st
| publisher  = [[Apress]]
| page      = 325ে
| isbn       = 978-1-59059-785-9
| url        = http://www.apress.com/book/view/9781590597859
| accessdate        = 2009-10-08
| postscript     = &lt;!--none--&gt;
}}
{{refend}}

==External links==
{{Commons category|Apache Tomcat}}
*{{Official website}}
*[http://wiki.apache.org/tomcat/ Project Wiki]
*[http://www.tomcatexpert.com/ Enterprise Tomcat Community site]
*[http://www.coreservlets.com/Apache-Tomcat-Tutorial/ Tutorial - Configuring &amp; Using Tomcat 6 and Tomcat 7]

{{Apache}}
{{Web server software}}

[[Category:Apache Software Foundation|Tomcat]]
[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]
[[Category:Software using the Apache license]]
[[Category:Web server software programmed in Java]]</text>
      <sha1>qsh9t65lng5dm1bjurbmaqieyzozfgl</sha1>
    </revision>
  </page>
  <page>
    <title>Apache MyFaces</title>
    <ns>0</ns>
    <id>7449978</id>
    <revision>
      <id>845650825</id>
      <parentid>837938321</parentid>
      <timestamp>2018-06-13T06:47:36Z</timestamp>
      <contributor>
        <ip>62.77.227.98</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5386">{{ Infobox software
| name                   = Apache MyFaces
| logo                   = MyFaces logo.jpg
| logo size              = frameless
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version =  2.3.1
| latest release date    = {{release date|2018|5|3}}&lt;ref&gt;{{cite web|url=https://myfaces.apache.org/|title=Welcome to the Apache MyFaces Project|accessdate=15 June 2017|deadurl=no|archiveurl=https://web.archive.org/web/20170622113711/http://myfaces.apache.org/|archivedate=22 June 2017|df=}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web framework]]
| license                = [[Apache License]] 2.0
| website                = {{Official URL}}
}}
'''Apache MyFaces''' is an [[Apache Software Foundation]] project that creates and maintains an [[open-source]] [[JavaServer Faces]] implementation, along with several libraries of JSF components that can be deployed on the core implementation. The project is divided into several sub-projects:

* '''Core''': an implementation of the JSF 1.1, JSF 1.2, JSF 2.0, JSF 2.1 and JSF 2.2 specification and components as specified by [http://www.jcp.org/en/jsr/detail?id=127 JSR 127], [http://www.jcp.org/en/jsr/detail?id=252 JSR 252], [http://www.jcp.org/en/jsr/detail?id=314 JSR 314] and [http://www.jcp.org/en/jsr/detail?id=344 JSR 344] respectively
* '''Portlet Bridge''': an implementation of the JSF Portlet Bridge specification as specified by [http://www.jcp.org/en/jsr/detail?id=301 JSR 301]
* '''Tomahawk''': a set of JSF components created by the MyFaces development team prior to its donation to Apache
* '''[[MyFaces Trinidad|Trinidad]]''': a set of JSF components contributed to MyFaces by [[Oracle Corporation|Oracle]], where it was known as [[Oracle Application Development Framework|ADF]] Faces
* '''Tobago''': a set of JSF components contributed to MyFaces by [http://www.atanion.com Atanion GmbH]
* '''Orchestra''': a framework used to manage persistence sessions across various scopes
* '''Extensions Validator''': a JSF centric validation framework, which is based on annotations
* '''CODI''': a toolbox which will support you on solving your daily needs of software development with CDI, JSF, BV and JPA
* '''Others''': As well as these subprojects, MyFaces has a number of smaller subprojects, for example '''MyFaces Commons''' or '''MyFaces Shared'''. These subprojects can be found [http://myfaces.apache.org/otherProjects.html here].

The standard is based on the [[Model–view–controller|Model View Controller]] paradigm, but is also component-based and event-oriented. Different templating languages can be used. In the standard, JSP is used, other options include XML based templating languages like Shale Clay or Facelets.&lt;ref&gt;{{cite web|url=http://myfaces.apache.org/jsfintro.html|title=MyFaces – Introduction to JSF|author=|date=|website=myfaces.apache.org|accessdate=23 April 2018|deadurl=no|archiveurl=https://web.archive.org/web/20170922054541/http://myfaces.apache.org/jsfintro.html|archivedate=22 September 2017|df=}}&lt;/ref&gt;

==Core==

The MyFaces Core subproject is an [[open-source]] implementation of the [[JavaServer Faces]] specification. Each major release of MyFaces Core is certified against the Sun [[Technology Compatibility Kit]] to ensure compliance.&lt;ref&gt;{{cite web|url=http://myfaces.apache.org/core12/index.html|title=Apache MyFaces Core 1.2 Description|accessdate=December 7, 2009| archiveurl= https://web.archive.org/web/20091213150811/http://myfaces.apache.org/core12/index.html| archivedate= 13 December 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

The core subproject is divided into two submodules:

* '''MyFaces API''' implements all of the classes that are defined in the specification.
* '''MyFaces Impl''' provides "invisible" support classes that user code does not directly invoke, but which are needed for a working JSF framework. Examples are the renderer classes for the standard JSF components.

These two submodules are distributed in two libraries, myfaces-api.jar and myfaces-impl.jar. Both of them are needed to be able to deploy a JSF based web application.

The latest release of MyFaces Core is 2.2.9. It requires [[Java (programming language)|Java]] 1.6 or later, [[JavaServer Pages|JSP]] 2.1, [[JavaServer Pages Standard Tag Library|JSTL]] 1.2 and a Java [[Servlet]] 2.5 implementation.&lt;ref&gt;{{cite web|url=http://myfaces.apache.org/core22/index.html|title=Apache MyFaces Core 2.2 Description|accessdate=2016-01-29|deadurl=no|archiveurl=https://web.archive.org/web/20160103134152/http://myfaces.apache.org/core22/index.html|archivedate=2016-01-03|df=}}&lt;/ref&gt;

==See also==
{{Portal|Free software}}
* [[JavaServer Faces]]
* [[Facelets]]

==References==
{{reflist}}

==External links==
* {{Official website}}
* [//myfaces.apache.org/trinidad/index.html Apache Trinidad (former Oracle ADF Faces)]
* [//myfaces.apache.org/tobago/index.html Apache Tobago]
* [//myfaces.apache.org/extensions/validator/ Apache MyFaces Extensions Validator]

{{apache}}

{{DEFAULTSORT:Apache Myfaces}}
[[Category:JavaServer Faces]]
[[Category:Apache Software Foundation|Myfaces]]
[[Category:Java enterprise platform]]
[[Category:2017 software]]</text>
      <sha1>376c0216q2axs5vwf8dowh92yqagugl</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Sling</title>
    <ns>0</ns>
    <id>19060939</id>
    <revision>
      <id>845801636</id>
      <parentid>834446544</parentid>
      <timestamp>2018-06-14T07:11:00Z</timestamp>
      <contributor>
        <username>Rombertw</username>
        <id>22853849</id>
      </contributor>
      <comment>Update canonical repository, the old one is not active anymore</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4486">{{Infobox Software
| name                   = Apache Sling
| logo                   = Apache Sling Logo.png
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 10&lt;ref&gt;https://sling.apache.org/news.html&lt;/ref&gt;
| latest release date    = {{Release date|2018|02|06}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web application framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://sling.apache.org}}
|repo={{URL|https://github.com/apache/sling-aggregator}}}}

'''Apache Sling''' is an [[open source]] [[Web framework]] for the [[Java platform]] designed to create content-centric applications on top of a  [[JSR-170]]-compliant (a.k.a. JCR) [[content repository]] such as [[Apache Jackrabbit]].&lt;ref&gt;{{cite web|url=http://www.infoworld.com/article/08/06/27/Apache-looks-to-bring-fun-back-to-Java_1.html |title=Archived copy |accessdate=2008-08-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20080801175249/http://www.infoworld.com/article/08/06/27/Apache-looks-to-bring-fun-back-to-Java_1.html |archivedate=2008-08-01 |df= }}&lt;/ref&gt; Apache Sling allows developers to deploy their application components as [[OSGi]] bundles or as scripts and templates in the content repository. Supported scripting languages are [[JavaServer Pages|JSP]], server-side [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Apache Velocity|Velocity]]. The goal of Apache Sling is to expose content in the content repository as [[Http|HTTP]] resources, fostering a [[Representational State Transfer|RESTful]] style of application architecture.

Sling is different from many other [[Web application framework]]s in the
sense that it truly focuses on the web aspect of the "web application" development and 
through its development paradigm suggests an intuitive [[Representational State Transfer|RESTful]] 
development of a true web application.
Other frameworks focus more on the application development and therefore are ideal extensions to Sling.&lt;ref&gt;{{cite web|url=http://dev.day.com/microsling/content/blogs/main/slingspringspling.html |title=Archived copy |accessdate=2008-08-28 |deadurl=yes |archiveurl=https://web.archive.org/web/20080917070629/http://dev.day.com/microsling/content/blogs/main/slingspringspling.html |archivedate=2008-09-17 |df= }}&lt;/ref&gt;

The Sling project was started on August 27, 2007,&lt;ref&gt;http://markmail.org/message/67zkwcxzwgnbfjjz&lt;/ref&gt; when [[Day Software]] proposed to donate the source base of its internal web framework powering the Day Communiqué WCM to the [[Apache Software Foundation]]. The project was accepted to the [[Apache Incubator]] with [[Apache Jackrabbit]] being the sponsoring project.  On June 18, 2009 &lt;ref&gt;http://markmail.org/message/elioenbv2wid55c5&lt;/ref&gt; the project graduated as Apache top-level project.

==Features==
*Content resolution that maps a request [[Uniform Resource Locator|URL]] to a content node in the content repository
*Servlet resolution that maps a content node and a request method to a Servlet handling the request
*Default servlets supporting [[WebDAV]], content creation from web forms and [[JSON]] representation
*A Javascript client library, allowing access to the content repository through [[AJAX]]
*Support for server-side scripting with [[Javascript]], [[JavaServer Pages|JSP]], [[Ruby (programming language)|Ruby]], [[Apache Velocity|Velocity]] and [[Scala (programming language)|Scala]]
*[[OSGi]]-based extensibility through [[Apache Felix]] – the Felix Web Console was originally developed by the Apache Sling project

==See also==
*[[Apache Jackrabbit]]
*[[Apache Felix]]

==References==
{{Reflist|1}}

==External links==
*{{Official website|https://sling.apache.org}}
*[https://web.archive.org/web/20110629051514/http://video.google.com/videoplay?docid=4185433226971588276 Apache Sling] at [[Google Videos]] by [[David Nüscheler]]
**[http://www.slideshare.net/uncled/webtuesday-zurich?src=embed David Nüscheler's presentation]
*[http://dev.day.com/content/docs/en/cq/current/developing/sling_cheatsheet/_jcr_content/par/image.img.png Apache Sling Cheat Sheet]

{{Apache}}

[[Category:Apache Software Foundation|Sling]]
[[Category:Java enterprise platform]]
[[Category:Web frameworks]]</text>
      <sha1>9rn7u6m56mdrtolfquhhfny48lkeyte</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Continuum</title>
    <ns>0</ns>
    <id>9605075</id>
    <revision>
      <id>726878024</id>
      <parentid>697262247</parentid>
      <timestamp>2016-06-25T01:20:35Z</timestamp>
      <contributor>
        <username>Azbarcea</username>
        <id>1923543</id>
      </contributor>
      <comment>Apache Continuum moved to attic.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1909">{{multiple issues|
{{notability|Products|date=February 2014}}
{{primary sources|date=February 2014}}
{{ref improve|date=February 2014}}
}}
{{Infobox software
| name                   = Apache Continuum
| logo                   =
| screenshot             =
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 1.4.2
| status                 = [[Apache_Attic|Attic]] (end-of-life)
| discontinued           = May 2016
| latest release date    = {{release date|2014|06|13}}
| operating system       = [[Cross-platform]] 
| programming language   = [[Java (programming language)|Java]] 
| genre                  = [[Continuous integration]] 
| license                = [[Apache License|Apache 2.0 licence]] 
| website                = {{URL|http://continuum.apache.org/}}
}}
'''Apache Continuum''', a partner to [[Apache Maven]], is a [[continuous integration]] server, which runs builds on a configurable schedule.&lt;ref name=Ching2009&gt;{{cite web|last=Brett Porter, Maria Odea Ching|title=Apache Continuum: Ensuring the Health of your Source Code|url=http://www.packtpub.com/article/apache-continuum-ensuring-health-of-source-code-part2|publisher=Packt|accessdate=13 February 2014}}&lt;/ref&gt;&lt;ref name=Smart2006&gt;{{cite web|last=Smart|first=John Ferguson|title=Continuous Integration with Continuum|url=https://today.java.net/pub/a/today/2006/05/30/continuous-integration-with-continuum.html|publisher=Java.net|accessdate=13 February 2014}}&lt;/ref&gt;  Much like [[CruiseControl]], Continuum emails developers when the build is broken, requesting that the culprit fix the problem. 

==References==
{{reflist}}
{{Software-stub}}

{{apache}}
{{network-software-stub}}

[[Category:Compiling tools]]
[[Category:Java development tools]]
[[Category:Continuous integration]]
[[Category:Apache Software Foundation|Continuum]]
[[Category:Software using the Apache license]]</text>
      <sha1>4yhnmfu00uitvezmecv725js5idkunq</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Empire-db</title>
    <ns>0</ns>
    <id>23503030</id>
    <revision>
      <id>830759061</id>
      <parentid>785204775</parentid>
      <timestamp>2018-03-16T19:33:48Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Fix [[:Category:Pages using deprecated image syntax]]; [[WP:GenFixes]] on, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7948">{{Unreferenced|date=May 2011}}

{{ Infobox Software
| name                   = Apache Empire-db
| logo                   = Empire-db.png
| logo size              = 250px
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = incubation
| latest release version = 2.4.3
| latest release date    = {{release date|2014|08|20}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Persistence Framework]]
| license                = [[Apache License]] 2.0
| website                = {{url|//empire-db.apache.org/}}
}}

'''Apache Empire-db''' is a Java library that provides a high level object-oriented API for accessing [[relational database management system]]s (RDBMS) through [[JDBC]]. Apache Empire-db is open source and provided under the Apache 2.0 license from the [[Apache Software Foundation]].

Compared to [[Object-relational mapping]] (ORM) or other data persistence solutions such as [[Hibernate (Java)|Hibernate]], [[iBATIS]] or [[TopLink]] Empire-db does not use XML files or Java annotations to provide a mapping of plain (old) Java object ([[POJO]]'s) to database tables, views and columns. Instead Empire-db uses a Java object model to describe the underlying data model and an API that works almost solely with object references rather than [[string literal]]s.

Empire-db's aim is to provide better software quality and improved maintainability through increased compile-time safety and reduced redundancy of metadata. Additionally applications may benefit from better performance due to full control over [[SQL]] statements and their execution by the developer compared to most OR-mapping solutions.

== Major benefits ==

Empire-db's key strength is its API for dynamic SQL generation for arbitrary select, update, insert or delete statements, purely by using Java methods which reference the model objects. This provides type-safety and almost entirely eliminates the use of string literals for names or expressions in code. Additionally DBMS independence is achieved through a pluggable driver model.

Using references to table and column objects significantly improves compile-time safety and thus reduces the amount of testing. As a positive side effect the IDE's code completion can be used to browse the data model, increases productivity and eliminates the need for other external tools or IDE-plugins.

Further the object model also provides safe and easy access to meta-information of the data model such as field data type, maximum field length, whether a field is mandatory and a finite choice of options for a field’s values. Metadata is user-extensible and not limited to DBMS related metadata. Availability of meta-information encourages more generic code and eliminates redundancies throughout application layers.

== Features at a glance ==

* Data model definition through a Java object model omits the need to learn XML schemas or annotations and easily allows user interceptions and extensions.
* Portable RDBMS independent record handling and command definition with support for a variety of relational databases such as [[Oracle Database|Oracle]], [[Microsoft SQL Server]], [[MySQL]], [[Apache Derby|Derby]], [[H2 (DBMS)|H2]] and [[HSQLDB]] (as of version 2.0.5)
* DDL generation for target DBMS from object definition, either for the entire database or for individual objects such as tables, views, columns and relations.
* Type-safe API for dynamic SQL command generation allows dynamic building of SQL statements using API methods and object references only instead of string literals. This provides a high degree of type-safety which simplifies testing and maintenance.
* Reduced amount of Java code and powerful interception of field and metadata access through dynamic beans as an alternative to POJOs. This even allows data model changes (DDL) at runtime.
* Automatic tracking of record state and field modification (aka "dirty checking") to only insert/ update modified fields.
* Support for optimistic locking through timestamp column.
* No need to always work with full database entities. Build queries to provide you with the data exactly as you need it, and obtain the result for example as a list of any type of POJO with matching property setters or constructor.
* Lightweight and passive library with zero configuration footprint that allows simple integration with any architecture or framework.

== Example ==

As an example consider a database with two tables called ''Employees'' and ''Departments'' for which a list of employees in a particular format, with certain constraints and a given order should be retrieved.

The corresponding Oracle syntax SQL statement is assumed to be as follows:

&lt;source lang="sql"&gt;
    SELECT t1.EMPLOYEE_ID, 
           t1.LASTNAME || ', ' || t1.FIRSTNAME AS NAME, 
           t2.DEPARTMENT
    FROM (EMPLOYEES t1 
          INNER JOIN DEPARTMENTS t2 ON t1.DEPARTMENT_ID = t2.DEPARTMENT_ID)
    WHERE upper(t1.LASTNAME) LIKE upper('Foo%') 
      AND t1.RETIRED=0
    ORDER BY t1.LASTNAME, t1.FIRSTNAME
&lt;/source&gt;

This SQL statement can be created using Empire-db's command API using object model references like this:

&lt;source lang="java"&gt;
    SampleDB db = getDatabase();
    // Declare shortcuts (not necessary but convenient)
    SampleDB.Employees EMP = db.EMPLOYEES;
    SampleDB.Departments DEP = db.DEPARTMENTS;
    // Create a command object
    DBCommand cmd = db.createCommand();
    // Select columns
    cmd.select(EMP.EMPLOYEE_ID);
    cmd.select(EMP.LASTNAME.append(", ").append(EMP.FIRSTNAME).as("NAME"));
    cmd.select(DEP.DEPARTMENT);
    // Join tables
    cmd.join  (EMP.DEPARTMENT_ID, DEP.DEPARTMENT_ID);
    // Set constraints
    cmd.where(EMP.LASTNAME.likeUpper("Foo%"));
    cmd.where(EMP.RETIRED.is(false));
    // Set order
    cmd.orderBy(EMP.LASTNAME);
    cmd.orderBy(EMP.FIRSTNAME);
&lt;/source&gt;

In order to execute the query and retrieve a list of POJO's holding the query result the following code may be used:

&lt;source lang="java"&gt;
    // Class definition for target objects
    public class EmployeeInfo {
        private int employeeId;
        private String name;
        private String department;
        // Getter's and Setters for all properties
        // or a public Constructor using fields
        public get...
        public set...
    }

    // Retrieve employee list using the cmd object created above
    DBReader reader = new DBReader();
    try {
        reader.open(cmd, getConnection());
        List&lt;EmployeeInfo&gt; empList = reader.getBeanList(EmployeeInfo.class);
    } finally {
        reader.close()
    }
&lt;/source&gt;

Empire-db also supports field access through object references or obtaining query results as XML.

== History ==

Empire-db was originally developed at ESTEAM Software a German software development company which used Empire-db to develop various applications for a variety of different branches.

In January 2008 Empire-db was made officially open dource and first published though SourceForge.net.

In June 2008 a proposal was submitted to the Apache Software Foundation for Empire-db to become an Apache Incubator project. In July 2008 Empire-db got accepted for incubation and all rights over the Software were transferred to the Apache Foundation.

In October 2008 Empire-db 2.0.4 was the first official Apache incubator release with all package names changed to begin with org.apache.empire.

== See also ==
{{Portal|Java}}
* [[Java Database Connectivity]] (JDBC)
* [[Object-relational mapping]]
* [[Hibernate (Java)|Hibernate]]
* [[iBATIS]]
* [[TopLink]]
* [[Apache Struts]]

== External links ==
* {{official website|//empire-db.apache.org/}}
{{apache}}

[[Category:Apache Software Foundation|Empire-db]]
[[Category:Java (programming language)]]</text>
      <sha1>kk5dqz0jwi04rrv6y5vn6f3kpcun05q</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Jelly</title>
    <ns>0</ns>
    <id>20905300</id>
    <revision>
      <id>830041906</id>
      <parentid>830041867</parentid>
      <timestamp>2018-03-12T11:25:51Z</timestamp>
      <contributor>
        <username>Renamed user 2560613081</username>
        <id>16847332</id>
      </contributor>
      <comment>Oops! I meant to hit preview but I hit save instead.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2739">{{notability|Products|date=October 2015}}
{{ Infobox software
| name                   = Apache Jelly
| developer              = [[Apache Software Foundation]]
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]] 2.0
| website                = {{URL|apache.org/jelly}}
}}
'''Apache Jelly''' is a [[Java (programming language)|Java]] and [[XML]] based scripting and processing engine for turning [[XML]] into executable code.&lt;ref&gt;http://commons.apache.org/jelly/&lt;/ref&gt; Jelly is a component of [[Apache Commons]].

Custom XML languages are commonly created to perform some kind of processing action. Jelly is intended to provide a simple XML based processing engine that can be extended to support various custom actions.&lt;ref&gt;http://commons.apache.org/jelly/overview.html&lt;/ref&gt;

== Usage ==
CA Project and Portfolio Management, or CA PPM&lt;ref&gt;[https://www.ca.com/us/products/ca-project-portfolio-management.html CA PPM Product Page at ca.com]&lt;/ref&gt;, formerly known as CA Clarity PPM or just Clarity, is a product of [[CA Technologies]]. CA PPM uses an extended version of the Jelly tag-language as an automation and integration scripting language in its Process Management engine. CA PPM implementation of Jelly is called GEL (Generic Execution Language) and encompasses a new custom tag library that allows easier connection to CA PPM via its XML Open Gateway (XOG). The following example shows how CA PPM implements the classical "[[Hello World]]" application.&lt;ref&gt;[https://docops.ca.com/ca-ppm/15-2/en/reference/xml-open-gateway-xog-development/xog-gel-scripting CA PPM 15.2 XOG GEL Scripting]&lt;/ref&gt;&lt;source lang="xml"&gt;
&lt;gel:script xmlns:j="jelly:core" xmlns:gel="jelly:com.niku.union.gel.GELTagLibrary"&gt;
  &lt;j:forEach indexVar="i" begin="1" end="3"&gt;
    &lt;gel:out&gt;Hello World ${i}!&lt;/gel:out&gt;
  &lt;/j:forEach&gt;
&lt;/gel:script&gt;
&lt;/source&gt;

Jelly is also used by the [[ServiceNow]] platform, which uses Jelly tag-language for scripting,&lt;ref&gt;[https://docs.servicenow.com/bundle/jakarta-servicenow-platform/page/script/general-scripting/reference/r_JellyTags.html Now Platform Scripting - Jelly tags]&lt;/ref&gt; and by the [[Jenkins (software)|Jenkins]] continuous integration server, which uses Jelly to allow plugins to extend its UI.&lt;ref&gt;{{Cite web|url=https://cleantestcode.wordpress.com/2013/11/28/how-to-write-a-jenkins-plugin-part-2/|title=How to write a Jenkins Plugin – Part 2|date=2013-11-28|website=Clean Test Code|access-date=2016-12-30}}&lt;/ref&gt;

== References ==
&lt;references/&gt;

== External links ==
*[http://commons.apache.org/jelly/ Apache Jelly]
*[http://commons.apache.org/ Apache Commons Homepage]
{{Apache}}

[[Category:Java platform]]
[[Category:Apache Software Foundation|Jelly]]</text>
      <sha1>t5higgefxkw1mmekq98du6d3s6uwazo</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Mahout</title>
    <ns>0</ns>
    <id>18706674</id>
    <revision>
      <id>801376197</id>
      <parentid>785517279</parentid>
      <timestamp>2017-09-19T09:02:36Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v456)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4994">{{Infobox Software
| name                   = Apache Mahout
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Active
| latest release version = 0.13.0
| latest release date    = {{release date|2017|04|17|df=yes}}&lt;ref&gt;{{Cite web|url=https://mahout.apache.org/|title=Apache Mahout: Scalable machine learning and data mining|accessdate=13 June 2017}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]], [[Scala (programming language)|Scala]]
| genre                  = [[Machine Learning]]
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = {{url|//mahout.apache.org}}
}}

'''Apache Mahout''' is a project of the [[Apache Software Foundation]] to produce [[free software|free]] implementations of [[distributed computing|distributed]] or otherwise [[Scalability|scalable]] [[machine learning]] algorithms focused primarily in the areas of [[collaborative filtering]], clustering and classification. Many of the implementations use the [[Apache Hadoop]] platform.&lt;ref&gt;{{cite web |url= http://www.ibm.com/developerworks/java/library/j-mahout/ |title=Introducing Apache Mahout |work=ibm.com |year=2011  |accessdate=13 September 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.infoq.com/news/2009/04/mahout |title=InfoQ: Apache Mahout: Highly Scalable Machine Learning Algorithms |work=infoq.com |year=2011 |accessdate=13 September 2011}}&lt;/ref&gt; Mahout also provides Java libraries for common maths operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; the number of implemented algorithms has grown quickly,&lt;ref&gt;{{cite web |url= http://mahout.apache.org/users/basics/algorithms.html |title=Algorithms - Apache Mahout - Apache Software Foundation|work=cwiki.apache.org |year=2011 |accessdate=13 September 2011}}&lt;/ref&gt; but various algorithms are still missing.

While Mahout's core algorithms for [[Cluster analysis|clustering]], classification and batch based collaborative filtering are implemented on top of Apache Hadoop using the [[MapReduce|map/reduce]] paradigm, it does not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster are also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop.

Starting with the release 0.10.0, the project shifts its focus to building backend-independent programming environment, code named "Samsara".&lt;ref&gt;{{Cite web
| url = http://mahout.apache.org/users/environment/in-core-reference.html
| title = Mahout-Samsara's In-Core Linear Algebra DSL Reference
| last = 
| first = 
| date = 
| website = 
| publisher = 
| access-date = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
| url = http://mahout.apache.org/users/environment/out-of-core-reference.html
| title = Mahout-Samsara's Distributed Linear Algebra DSL Reference
| last = 
| first = 
| date = 
| website = 
| publisher = 
| access-date = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
| url = http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html
| title = Mahout 0.10.x: first Mahout release as a programming environment
| website = www.weatheringthroughtechdays.com
| access-date = 2016-02-29
}}&lt;/ref&gt; The environment consists of an algebraic backend-independent optimizer and an algebraic Scala DSL unifying in-memory and distributed algebraic operators. At the time of this writing supported algebraic platforms are [[Apache Spark]] and [[H2O_(software)|H2O]], and [[Apache Flink]]. Support for [[MapReduce]] algorithms is being gradually phased out.&lt;ref&gt;{{Cite web
| url = https://issues.apache.org/jira/browse/MAHOUT-1510
| title = MAHOUT-1510 ("Good-bye MapReduce")
| last = 
| first = 
| date = 
| website = 
| publisher = 
| access-date = 
}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{Official website}}
*[//giraph.apache.org/ Giraph] - a Graph processing infrastructure that runs on Hadoop (see Pregel).
*[http://portal.acm.org/citation.cfm?id=1582723 Pregel] - Google's internal graph processing platform, released details in ACM paper.
*[https://mahout.apache.org/general/mailing-lists,-irc-and-archives.html#archives Mahout Mailing List Archives]

===Demonstrations===
*[https://bhagyas.github.io/spring-mahout-demo/ A Spring based Java demo application that demonstrates a simple recommender using Apache Mahout]
*[http://test-triprecommender.rhcloud.com Demo of travel recommendations using anonymous user-based recommender of Mahout]

{{Apache}}

[[Category:Apache Software Foundation|Mahout]]
[[Category:Hadoop]]
[[Category:Data mining and machine learning software]]
[[Category:Big data products]]</text>
      <sha1>9m6jij1t2569cfiadtyr41xkhm7ax80</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Solr</title>
    <ns>0</ns>
    <id>10443665</id>
    <revision>
      <id>847857535</id>
      <parentid>842816568</parentid>
      <timestamp>2018-06-28T07:50:21Z</timestamp>
      <contributor>
        <ip>61.30.51.61</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16419">{{Buzzword|date=September 2017}}
{{Infobox software
| name                   = Solr
| logo                   = [[File:Solr.png|130px|Solr logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 7.4.0
| latest release date    = {{release date and age|2018|6|27}}&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/news.html|title=News|publisher=[[Apache Foundation]]|accessdate=27 Jun 2018}}&lt;/ref&gt;
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]] [[Application programming interface|API]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://lucene.apache.org/solr/}}
}}

'''Solr''' (pronounced "solar") is an [[open source]] [[enterprise search]] platform, written in [[Java (programming language)|Java]], from the '''Apache [[Lucene]]''' project. Its major features include [[Full text search|full-text search]], hit highlighting, [[faceted search]], real-time indexing, dynamic clustering, database integration, [[NoSQL]] features&lt;ref&gt;{{cite web|url=http://searchhub.org/2012/05/21/solr-4-preview/ |title=Archived copy |accessdate=2014-07-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20140706165443/http://searchhub.org/2012/05/21/solr-4-preview/ |archivedate=2014-07-06 }}&lt;/ref&gt; and rich document (e.g., Word, PDF) handling. Providing distributed search and index replication, Solr is designed for scalability and [[fault tolerance]].&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/#intro|title=Apache Solr -|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt; Solr is widely used for enterprise search and analytics use cases and has an active development community and regular releases.

Solr runs as a standalone full-text search server. It uses the [[Lucene]] Java search library at its core for full-text indexing and search, and has [[REST]]-like [[HTTP]]/[[XML]] and [[JSON]] APIs that make it usable from most popular programming languages. Solr's external configuration allows it to be tailored to many types of application without Java coding, and it has a plugin architecture to support more advanced customization.

Apache [[Lucene]] and Apache Solr are both produced by the same [[Apache Software Foundation]] development team since the two projects were merged in 2010.  It is common to refer to the technology or products as Lucene/Solr or Solr/Lucene.

== History ==
In 2004, Solr was created by Yonik Seeley at [[CNET Networks]] as an in-house project to add search capability for the company website.

In January 2006, CNET Networks decided to openly publish the source code by donating it to the [[Apache Software Foundation]].&lt;ref&gt;{{cite web|url=http://issues.apache.org/jira/browse/SOLR-1|title=[SOLR-1] CNET code contribution - ASF JIRA|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt; Like any new Apache project, it entered an incubation period which helped solve organizational, legal, and financial issues.

In January 2007, Solr graduated from incubation status into a standalone top-level project (TLP) and grew steadily with accumulated features, thereby attracting users, contributors, and committers. Although quite new as a public project, it powered several high-traffic websites.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/solr/PublicServers|title=PublicServers - Solr Wiki|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt;

In September 2008, Solr 1.3 was released including distributed search capabilities and performance enhancements among many others.&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/#15%20September%202008%20-%20Solr%201.3.0%20Available|title=Apache Solr -|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt;

In January 2009, solr  and enterprise search Yonik Seeley along with Grant Ingersoll and Erik Hatcher joined [[Lucidworks]] (formerly Lucid Imagination), the first company providing commercial support and training for Apache Solr search technologies{{Citation needed|reason=date=February 2015|date=February 2015}}. Since then, support offerings around Solr have been abundant.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/solr/Support|title=Support - Solr Wiki|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt;

November 2009 saw the release of Solr 1.4. This version introduced enhancements in indexing, searching and faceting along with many other improvements such as rich document processing ([[PDF]], [[Microsoft Word|Word]], [[HTML]]), Search Results clustering based on [[Carrot2]] and also improved database integration. The release also features many additional plug-ins.&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/#10%20November%202009%20-%20Solr%201.4%20Released|title=Apache Solr -|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt;

In March 2010, the [[Lucene]] and Solr projects merged.&lt;ref&gt;{{cite web|url=http://lucene.markmail.org/thread/cmbek34vfycb6sfc|title=[VOTE] merge lucene/solr development (take 3) - Yonik Seeley - org.apache.lucene.general - MarkMail|work=markmail.org|accessdate=16 January 2017}}&lt;/ref&gt; Solr became a Lucene sub project. Separate downloads continued, but the products were now jointly developed by a single set of committers.

In 2011 the Solr version number scheme was changed in order to match that of Lucene. After Solr 1.4, the next release of Solr was labeled 3.1, in order to keep Solr and Lucene on the same version number.&lt;ref&gt;[http://wiki.apache.org/solr/Solr3.1 Solr3.1 - Solr Wiki]. Wiki.apache.org (2013-05-16). Retrieved on 2013-07-21.&lt;/ref&gt;

In October 2012 Solr version 4.0 was released, including the new SolrCloud feature.&lt;ref&gt;[http://lucene.apache.org/solr/solrnews.html Apache Lucene]. Lucene.apache.org. Retrieved on 2013-07-21.&lt;/ref&gt; 2013 and 2014 saw a number of Solr releases in the 4.x line, steadily growing the feature set and improving reliability.

In February 2015, Solr 5.0 was released,&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/news.html#20-february-2015-apache-solr-500-and-reference-guide-for-50-available|title=Apache Solr - News|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt; the first release where Solr is packaged as a standalone application,&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/browse/SOLR-6733|title=[SOLR-6733] Umbrella issue - Solr as a standalone application - ASF JIRA|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt; ending official support for deploying Solr as a [[WAR (file format)|war]]. Solr 5.3 featured a built-in pluggable Authentication and Authorization framework.&lt;ref&gt;{{Cite web|title = Solr 5.3 Release announcement|url = http://lucene.apache.org/solr/news.html#24-august-2015-apache-solr-530-and-reference-guide-for-53-available|website = lucene.apache.org|accessdate = 2015-09-24}}&lt;/ref&gt;

In April 2016, Solr 6.0 was released.&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/news.html#8-april-2016-apache-solr-600-available|title=Apache Solr - News|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt; Added support for executing Parallel SQL queries across SolrCloud collections. Includes StreamExpression support and a new JDBC Driver for the SQL Interface.

In September 2017, Solr 7.0 was released.&lt;ref&gt;{{cite web|url=http://lucene.apache.org/solr/news.html#20-september-2017-apache-solr-700-available|title=Apache Solr - News}}&lt;/ref&gt; This release among other things, added support multiple replica types, auto-scaling, and a Math engine.

== Features ==
Excerpt:
* Uses the Lucene library for full-text search
* [[Faceted search|Faceted navigation]]
* Hit highlighting
* Query language supports structured as well as textual search
* Schema-less mode and Schema [[REST]] API
* [[JSON]], [[XML]], [[PHP]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[XSLT]], [[Apache Velocity|Velocity]] and custom Java binary output formats over HTTP
* HTML administration interface
* Built-in [[Computer security|security]]: [[Authentication]], [[Authorization (computer access control)|Authorization]], [[Transport Layer Security|SSL]]
* Replication to other Solr servers - enables scaling [[Queries per second|QPS]] and [[High availability]]
* Distributed Search through [[Sharding]] - enables scaling content volume
* Search results clustering based on [[Carrot2]]
* Extensible through plugins
* Flexible relevance - boost through function queries
* Caching - queries, filters, and documents
* Embeddable in a Java Application
* Geo-spatial search, including multiple points per documents and polygons
* Automated management of large clusters through [[Apache ZooKeeper|ZooKeeper]]
* Function queries
* Field Collapsing and grouping&lt;ref&gt;[http://searchhub.org//2010/09/16/2446/ Solr Result Grouping]&lt;/ref&gt;
* Auto-suggest features
* Streaming

== Apache Solr Operations ==
Apache Solr is a powerful tool with tremendous search capability. In order to search a document, it performs following operations in sequence:

# '''Indexing:''' First of all, it converts the documents into a machine-readable format which is called Indexing. 
# '''Querying:''' Understanding the terms of a query asked by the user. These terms can be images, keywords, and much more. 
# '''Mapping:''' Solr maps the user query to the documents stored in the database to find the appropriate result.
# '''Ranking the outcome:''' As soon as the engine searches the indexed documents, it ranks the outputs as per their relevance.

== Community ==
Solr has both individuals and companies who contribute new features and bug fixes.&lt;ref&gt;{{cite web|url=https://stackoverflow.com/questions/tagged/solr?sort=votes|title=Highest Voted 'solr' Questions|work=stackoverflow.com|accessdate=16 January 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://lucenerevolution.org/|title=Lucene/Solr Revolution 2016|work=lucenerevolution.org|accessdate=16 January 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://www.meetup.com/SFBay-Lucene-Solr-Meetup/|title=SFBay Apache Lucene/Solr  Meetup|work=meetup.com|accessdate=16 January 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://www.meetup.com/Oslo-Solr-Community/|title=Oslo Solr Community|work=meetup.com|accessdate=16 January 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://www.linkedin.com/groups/Solr-1557747|title=LinkedIn Solr Group|work=linkedin.com|accessdate=16 January 2017}}&lt;/ref&gt;

== Integrating Solr ==

Solr is bundled as the built-in search in many applications such as [[content management system]]s and [[enterprise content management]] systems.  [[Hadoop]] distributions from [[Cloudera]],&lt;ref&gt;{{cite web|url=http://blog.cloudera.com/blog/2013/06/hadoop-for-everyone-inside-cloudera-search/|title=Hadoop for Everyone: Inside Cloudera Search - Cloudera Engineering Blog|date=24 June 2013|work=cloudera.com|accessdate=16 January 2017}}&lt;/ref&gt; [[Hortonworks]]&lt;ref&gt;{{cite web|url=http://hortonworks.com/blog/bringing-enterprise-search-enterprise-hadoop/|title=Bringing Enterprise Search to Enterprise Hadoop - Hortonworks|date=2 April 2014|work=hortonworks.com|accessdate=16 January 2017}}&lt;/ref&gt; and [[MapR]] all bundle Solr as the search engine for their products marketed for [[big data]]. [[DataStax]] DSE integrates Solr as a search engine with [[Apache Cassandra|Cassandra]].&lt;ref&gt;{{cite web|url=http://www.datastax.com/dev/blog/datastax-enterprise-cassandra-with-solr-integration-details/|title=DataStax Enterprise: Cassandra with Solr Integration Details|date=12 April 2012|work=datastax.com|accessdate=6 February 2017}}&lt;/ref&gt;   Solr is supported as an end point in various data processing frameworks and [[Enterprise integration]] frameworks.{{citation needed|date=July 2015}}

Solr exposes industry standard [[HTTP]] [[REST|REST-like]] [[API]]s with both [[XML]] and [[JSON]] support, and will integrate with any system or programming language supporting these standards. For ease of use there are also client libraries available for [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[PHP]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]] and most other popular programming languages.&lt;ref&gt;{{cite web|url=https://wiki.apache.org/solr/IntegratingSolr|title=IntegratingSolr - Solr Wiki|work=apache.org|accessdate=16 January 2017}}&lt;/ref&gt;

== See also ==
{{Portal|Free software}}
* [[Open Semantic Framework]]
* [[Search oriented architecture]]
* [[List of information retrieval libraries]]

==References==
{{Reflist|3}}

==Bibliography==
{{Refbegin}}
* {{cite book
| first1      = David
| last1       = Smiley
| first2      = Eric
| last2       = Pugh
| first3      = Kranti
| last3       = Parisa
| first4      = Matt
| last4       = Mitchell
| date        = February 2014
| title       = Apache Solr 4 Enterprise Search Server 
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 451 
| isbn        = 9781782161363
| url         = http://www.packtpub.com/apache-solr-4-enterprise/book
}}
* {{cite book
| first1      = Alfredo
| last1       = Serafini
| date        = December 2013
| title       = Apache Solr Beginner’s Guide
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 324 
| isbn        = 9781782162520
| url         = http://www.packtpub.com/apache-solr-beginners-guide/book
}}
* {{cite book
| first1      = Alexandre
| last1       = Rafalovitch
| date        = June 2013
| title       = Instant Apache Solr for Indexing Data How-to 
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 90 
| isbn        = 9781782164845
| url         = http://www.packtpub.com/apache-solr-for-indexing-data/book
}}
* {{cite book
| first1      = Rafał
| last1       = Kuć
| date        = January 2013
| title       = Apache Solr 4 Cookbook
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 328 
| isbn        = 9781782161325
| url         = http://www.packtpub.com/apache-solr-4-cookbook/book
}}
* {{cite book
| first1      = David
| last1       = Smiley
| first2      = Eric
| last2       = Pugh
| date        = November 20, 2011
| title       = Apache Solr 3 Enterprise Search Server 
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 418 
| isbn        = 1-84951-606-5
| url         = http://www.packtpub.com/apache-solr-3-enterprise-search-server/book
}}
* {{cite book
| first1      = Rafał
| last1       = Kuć
| date        = July 22, 2011
| title       = Apache Solr 3.1 Cookbook
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 300 
| isbn        = 1-84951-218-3
| url         = http://www.packtpub.com/solr-3-1-enterprise-search-server-cookbook/book
}}
* {{cite book
| first1      = David
| last1       = Smiley
| first2      = Eric
| last2       = Pugh
| date        = August 19, 2009
| title       = Solr 1.4 Enterprise Search Server
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 336
| isbn        = 1-84719-588-1
| url         = http://www.packtpub.com/solr-1-4-enterprise-search-server
}}
{{Refend}}

==External links==
* [http://lucene.apache.org/solr Solr homepage]
* [http://lucene.apache.org/solr/tutorial.html Solr tutorial]
* [http://wiki.apache.org/solr Solr wiki]
* [https://web.archive.org/web/20120729142330/http://svn.apache.org/repos/asf/lucene/dev/trunk/solr/example/exampledocs/ Apache Solr Test Files]
* [http://www.xml.com/pub/a/2006/08/09/solr-indexing-xml-with-lucene-andrest.html Solr: Indexing XML with Lucene and REST]
* [http://public.dhe.ibm.com/software/dw/java/j-solr1-pdf.pdf Search smarter with Apache Solr, Part 1]
* [http://public.dhe.ibm.com/software/dw/java/j-solr2-pdf.pdf Search smarter with Apache Solr, Part 2]
* [https://web.archive.org/web/20081210122038/http://www.ibm.com/developerworks/java/library/j-solr-update/ What's new with Apache Solr]
* [http://quant-coder.blogspot.com/2013/11/1-2-3-to-integrate-apache-nutch-1.html Apache Nutch and Solr on Tomcat]
* [http://qnalist.com/g/solr Apache Solr Mailing List Archives]
{{Apache}}

[[Category:Apache Software Foundation|Solr]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free search engine software]]
[[Category:Search engine software]]
[[Category:Database-related software for Linux]]
[[Category:NoSQL]]</text>
      <sha1>7omvzatfjq0dkspaet4e4i2j279pg9a</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Tuscany</title>
    <ns>0</ns>
    <id>9073093</id>
    <revision>
      <id>839214637</id>
      <parentid>778021698</parentid>
      <timestamp>2018-05-02T01:03:48Z</timestamp>
      <contributor>
        <username>Ankit Pati</username>
        <id>7768466</id>
      </contributor>
      <comment>Update status of project. The project was retired more than two years ago.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3099">{{Confusing|date=May 2008}}
{{Infobox Software 
| name                   = Apache Tuscany
| logo                   =
| screenshot             = 
| caption                =  
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version = SCA Java 1.6.2 (April 2011), SCA Java 2.0-Beta2 (February 2011), SDO Java 1.1.1 (July 2008), DAS Java 1.0-incubating-beta2 (Oct 2007), SCA Native Incubator-M3 (May 2007) 
| operating system       = [[Cross-platform]] 
| programming language   = [[C++]] and [[Java (programming language)|Java]]
| genre                  = [[Service-oriented architecture|SOA]]
| license                = [[Apache License]] 2.0
| website                = http://tuscany.apache.org/
}}
'''Apache Tuscany''' was an [[open-source software]] project for developing and running software applications using a [[service-oriented architecture]] (SOA). 

==Description==
This lightweight runtime is designed to be embedded in, or provisioned to, a number of different host environments. Apache Tuscany implements [[Service component architecture]] (SCA) which defines a flexible, service-based model for construction, assembly and deployment of network of services (existing and new ones). 

With SCA as its foundation, Tuscany was promoted to push handling of protocol out of the application business logic into pluggable bindings. As a result, protocols can be changed at only one time with minimal configuration changes. Tuscany also removes the need for applications to deal with infrastructure concerns such as security and transaction and handles this declaratively.

Tuscany provides support for SCA 1.0 specification in Java. It also provides a wide range of bindings (web services, web20 bindings, etc.), implementation types (Spring, BPEL, Java, etc.) as well as integration with technologies such as web20 and [[OSGi]]. Tuscany was working on implementing SCA 1.1 that is being standardized at OASIS.

Apache Tuscany also implements [[Service Data Objects]] (SDO) which provides a uniform interface for handling different forms of data, including XML documents, that can exist in a network of services and provides the mechanism for tracking changes. Tuscany supports the SCO and the SDO (2.01 for C++ / 2.1 for Java) specification.

The [[Apache Software Foundation]] announced on August 2016 that the project was no longer maintained.&lt;ref&gt;{{cite mailing list |url=https://marc.info/?l=apache-announce&amp;m=147020938010828&amp;w=2 |title=Apache Tuscany retired |date=2016-08-03 |accessdate=4 August 2016 |mailing-list=apache-announce |author=Henri Yandell }}&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* [http://tuscany.apache.org/ Apache Tuscany Project Home Page]
* [http://www.oasis-opencsa.org/sca SCA Home Page at OASIS web site]
* [http://www.infoq.com/articles/tuscanysca "Introduction to SCA bindings" from Ch 7 of Tuscany SCA in Action]
{{apache}}

[[Category:Apache Software Foundation|Tuscany]]
[[Category:Enterprise application integration]]
[[Category:Service-oriented architecture-related products]]</text>
      <sha1>ho4hwh9njg48l1krk29nv1rm7uk9jf2</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Velocity</title>
    <ns>0</ns>
    <id>2285690</id>
    <revision>
      <id>800476573</id>
      <parentid>783204733</parentid>
      <timestamp>2017-09-13T19:56:14Z</timestamp>
      <contributor>
        <username>Ssustrich</username>
        <id>10709403</id>
      </contributor>
      <minor/>
      <comment>Updated most recent stable release to reflect 2.0 and not 1.7</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4832">{{for|the Microsoft project codenamed Velocity|Velocity (memory cache)}}
{{More footnotes|date=March 2010}}
{{Infobox Software
| name                   = Apache Velocity
| logo                   = [[File:Jakarta Velocity Logo.png|Jakarta Velocity Logo]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.0
| latest release date    = {{release date|mf=yes|2017|08|06}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Template engine (web)|template engine]]
| license                = [[Apache License]] 2.0
| website                = http://velocity.apache.org/
}}
'''Apache Velocity''' is a [[Java (programming language)|Java]]-based [[template engine (web)|template engine]] that provides a [[Web template|template language]] to reference [[object (computer science)|object]]s defined in Java code. It aims to ensure clean separation between the presentation tier and business tiers in a [[Web application]] (the [[model–view–controller]] design pattern).

Velocity is an [[open source]] software project hosted by the [[Apache Software Foundation]]. It is released under the [[Apache License]].

== Uses ==
Some common types of applications that use Velocity are:
* [[Web application]]s: [[Web developer]]s render [[HTML]] page structures.  The content is populated with dynamic information. The page is processed with ''VelocityViewServlet'' or any of a number of frameworks that support Velocity.
* [[Source code]] generation: Velocity can be used to generate [[Java (programming language)|Java]], [[SQL]], or [[PostScript]] source code, based on [[Web template|templates]]. A number of [[open source]] and commercial development [[software package (installation)|software packages]] use Velocity in this manner.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/velocity/PoweredByVelocity |title=PoweredByVelocity | work = Velocity Wiki |publisher=Wiki.apache.org |date=2009-12-30 |accessdate=2010-03-29| archiveurl= https://web.archive.org/web/20100323223306/http://wiki.apache.org/velocity/PoweredByVelocity| archivedate= 23 March 2010 | deadurl= no}}&lt;/ref&gt;
* Automatic [[email]]s: Many applications generate automatic emails for account signup, [[password]] reminders, or automatically sent reports. Using Velocity, the email [[web template|template]] can be stored in a [[text file]], rather than directly embedded in Java code.
* [[XML]] transformation: Velocity provides an [[Apache Ant|Ant]] task, called Anakia, which reads an XML file and makes it available to a Velocity template. A common application is to convert [[software documentation]] stored in a generic "xdoc" format into a styled HTML document.

== Code example ==
The following [[web template|template]]:
&lt;source lang="html+velocity"&gt;
## Velocity Hello World
&lt;html&gt;
    &lt;body&gt;
       #set( $foo = "Velocity" )
       ## followed by
       Hello $foo World!
    &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

processed by Velocity produces the following HTML:
&lt;source lang="html"&gt;
&lt;html&gt;
    &lt;body&gt;
     Hello Velocity World!
    &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

The syntax and overall concept of the Apache Velocity templates is similar to the syntax of the older [[WebMacro]] template engine, which is now also an open source project. {{Citation needed|date=January 2011}}

== See also ==
{{Portal|Free software}}
* [[Apache Torque]]
* [[FreeMarker]]
* [[JavaServer Pages]]
* [[Thymeleaf]]

== References ==
{{reflist}}

== Bibliography ==
{{refbegin}}
*{{citation
| first1     = Rob
| last1      = Harrop
| first2     = Ian
| last2      = Darwin
| date       = August 30, 2004
| title      = Pro Jakarta Velocity: From Professional to Expert
| edition    = 1st
| publisher  = [[Apress]]
| page      = 370
| isbn       = 978-1-59059-410-0
| url        = http://www.apress.com/book/view/9781590594100
}}
*{{citation
| first1     = Jim
| last1      = Cole
| first2     = Joseph
| last2      = D. Gradecki
| date       = July 16, 2003
| title      = Mastering Apache Velocity
| edition    = 1st
| publisher  = [[John Wiley &amp; Sons|Wiley]]
| page      = 372
| isbn       = 978-0-471-45794-7
| url        = http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471457949.html
}}
{{refend}}

== External links ==
* [http://velocity.apache.org/ Velocity at Apache]
* [http://wiki.apache.org/velocity/ Velocity wiki]
* [http://www.javaworld.com/javaworld/jw-11-2007/jw-11-java-template-engines.html Java templates comparison]

{{Apache}}

{{Authority control}}
[[Category:Apache Software Foundation|Velocity]]
[[Category:Java (programming language) libraries]]
[[Category:Template engines]]

{{programming-software-stub}}</text>
      <sha1>3op8sal5j7hgf7bmq19qw35u8osi5iz</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ZooKeeper</title>
    <ns>0</ns>
    <id>26039352</id>
    <revision>
      <id>839922809</id>
      <parentid>839759305</parentid>
      <timestamp>2018-05-06T16:11:34Z</timestamp>
      <contributor>
        <username>Diannaa</username>
        <id>10728040</id>
      </contributor>
      <comment>remove copyright content copied from http://curator.apache.org/, https://kazoo.readthedocs.io/en/latest/</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6231">{{Infobox software
| name                   =
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 3.4.11
| latest release date    = {{release date|2017|11|09}}&lt;ref name="releases"&gt;{{cite web|url=https://zookeeper.apache.org/releases.html|title=Apache ZooKeeper - Releases|accessdate=2 February 2018}}&lt;/ref&gt;
| latest preview version = 3.5.3-beta
| latest preview date    ={{release date|2017|04|17}}&lt;ref name="releases" /&gt;
| operating system       = [[Cross-platform]]
| platform               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Distributed computing]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://zookeeper.apache.org}}
}}

'''Apache ZooKeeper''' is a software project of the [[Apache Software Foundation]]. It is essentially a centralized service for distributed systems to a hierarchical [[Key-value database|key-value store]], which is used to provide a distributed configuration service, synchronization service, and naming registry for large [[distributed systems]].&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/ZOOKEEPER/|title=Index - Apache ZooKeeper - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-08-26}}&lt;/ref&gt; ZooKeeper was a sub-project of [[Hadoop]] but is now a [[Apache Software Foundation#Projects|top-level Apache project]] in its own right.

ZooKeeper's architecture supports [[High-availability cluster|high availability]] through redundant services. The clients can thus ask another ZooKeeper leader if the first fails to answer. ZooKeeper nodes store their data in a hierarchical name space, much like a file system or a [[Tree (data structure)|tree]] data structure. Clients can read from and write to the nodes and in this way have a shared configuration service. ZooKeeper can be viewed as an [[atomic broadcast]] system, through which updates are [[total order|totally ordered]]. The ZooKeeper Atomic Broadcast (ZAB) protocol is the core of the system.&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/ZOOKEEPER/ProjectDescription|title=Zookeeper Overview}}&lt;/ref&gt;

ZooKeeper is used by companies including [[Rackspace]], [[Yahoo!]],&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/ZooKeeper/PoweredBy |title=ZooKeeper/Powered By}}&lt;/ref&gt; [[Odnoklassniki]], [[Reddit]],&lt;ref&gt;{{cite web|url=https://www.reddit.com/r/announcements/comments/4y0m56/why_reddit_was_down_on_aug_11/|title=Why Reddit was down on Aug 11}}&lt;/ref&gt; [[NetApp]] [[SolidFire]]&lt;ref&gt;{{Cite news|url=https://newsroom.netapp.com/blogs/5-big-daas-challenges-and-how-to-overcome-them/|title=5 Big DaaS Challenges and How to Overcome Them {{!}} NetApp Newsroom|date=2016-06-20|work=NetApp Newsroom|access-date=2017-05-24|language=en-US}}&lt;/ref&gt; and [[eBay]]  as well as [[open source]] [[enterprise search]] systems like [[Solr]].&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/solr/SolrCloud|title=SolrCloud}}&lt;/ref&gt;

ZooKeeper was originally developed at Yahoo for streamlining the processes running on Big Data cluster by storing the status in local log files on the ZooKeeper servers. These servers communicate with the client machines to provide them the information. ZooKeeper was developed in order to fix the bugs occurred while deploying distributed big data applications. 
Some of the prime features of Apache ZooKeeper are:

* Reliable System: This system is very reliable as it keeps working even if a node fails.
* Simple Architecture: The architecture of ZooKeeper is quite simple as there is a shared hierarchical namespace which helps coordinating the processes.
* Fast Processing: ZooKeeper is especially fast in "read-dominant" workloads (i.e. workloads in which reads are much more common than writes).
* Scalable: The performance of ZooKeeper can be improved by adding nodes.

==Apache ZooKeeper Architecture==

The basic terminologies that you need to know before knowing the architecture are:
* Node: The systems installed on the cluster
* ZNode: The nodes where the status is updated by other nodes in cluster
* Client Applications: The tools that interact with the distributed applications
* Server Applications: Allows the client applications to interact using a common interface

The services in the cluster are replicated and stored on a set of servers (called an "ensemble"), each of which maintains an in-memory database containing the entire data tree of state as well as a transaction log and snapshots stored persistently. Multiple client applications can connect to a server, and each client maintains a TCP connection through which it sends requests and heartbeats and receives responses and watch events for monitoring.&lt;ref&gt;{{cite web|url=https://zookeeper.apache.org/doc/current/zookeeperOver.html|title= Zookeeper}}&lt;/ref&gt;

==Typical use cases==
* [[Name service|Naming service]]
* [[Configuration management]]
* [[Synchronization]]
* [[Leader election]]
* [[Message queue]]
* [[Notification system]]

== Client libraries ==
In addition to the client libraries included with the ZooKeeper distribution, a number of third party libraries such as Apache Curator and Kazoo are available that make using ZooKeeper easier, add additional functionality, additional programming languages, etc.

==See also==
{{Portal|Java}}
* [[Hadoop]]

==References==
{{Reflist}}

==External links==
*{{Official website|https://zookeeper.apache.org}}
*[http://highscalability.com/blog/2008/7/15/zookeeper-a-reliable-scalable-distributed-coordination-syste.html Article in highscalability.com]
*[http://www.sdtimes.com/ZOOKEEPER_SERVICES_COORDINATOR_MOVES_TO_APACHE/About_SERVICECOORDINATION_and_ZOOKEEPER_and_APACHE_and_YAHOO/33011 Software Development Times article of ZooKeeper moving to Apache]
*[http://wiki.eclipse.org/index.php?title=Zookeeper_Based_ECF_Discovery  Eclipse ECF Discovery based on Apache ZooKeeper]

{{Apache}}

{{DEFAULTSORT:Apache Zookeeper}}
[[Category:Apache Software Foundation|ZooKeeper]]
[[Category:Configuration management]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Hadoop]]</text>
      <sha1>fym53obffyfu8yxg38gqcdgokm5k5nc</sha1>
    </revision>
  </page>
  <page>
    <title>Sigar (software)</title>
    <ns>0</ns>
    <id>8253497</id>
    <revision>
      <id>691356340</id>
      <parentid>549110827</parentid>
      <timestamp>2015-11-19T08:24:45Z</timestamp>
      <contributor>
        <ip>173.183.38.201</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="895">'''Sigar''' is a [[free software]] library (under the [[Apache License]]) that provides a [[cross-platform]], cross-language [[Computer programming|programming]] interface to low-level information on computer hardware and [[operating system]] activity.  The library provides bindings for many popular [[computer languages]] and has been ported to over 25 different operating system/hardware combinations.  Sigar stands for System Information Gatherer And Reporter and was originally developed by [[Doug MacEachern]], the author of the popular [[mod_perl]] module for the [[Apache web server]].

==External links==
*[http://sigar.hyperic.com hyperic.com], SIGAR home page.
*[https://github.com/hyperic/sigar], Source Code at GitHub.
*{{Freshmeat|hyperic-sigar|SIGAR}}

{{Compu-prog-stub}}

[[Category:Apache Software Foundation]]
[[Category:Computer programming]]
[[Category:Computing platforms]]</text>
      <sha1>sr4lx1f1z65jfzd09npvjzg2wgp6w0k</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Roller</title>
    <ns>0</ns>
    <id>30854540</id>
    <revision>
      <id>834149292</id>
      <parentid>789525592</parentid>
      <timestamp>2018-04-04T05:30:34Z</timestamp>
      <contributor>
        <username>HacksawPC</username>
        <id>12568908</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2719">{{Primary sources|date=August 2008}}
{{Infobox software
| name                   = Apache Roller
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 5.2.0&lt;ref name="apache_roller_release"&gt;{{cite web | url=http://www.apache.org/dyn/closer.cgi/roller/roller-5.2/v5.2.0/ | title=Apache Download Mirrors | publisher=apache.org | accessdate=4 April 2018}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.apache.org/dist/roller/roller-5.2/v5.2.0/ | title=Index of /roller/roller-5.2/v5.2.0 | publisher=www.apache.org | accessdate=4 April 2018}}&lt;/ref&gt;
| latest release date    = {{Start date and age|2017|11|06}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[weblog]] server
| license                = [[Apache License]] 2.0&lt;ref name="ohloh.net"&gt;http://www.ohloh.net/p/roller OhLoh Apache Roller Summary information page at Ohloh&lt;/ref&gt;
| website                = http://roller.apache.org/
}}
{{Portal|Free software}}

'''Apache Roller''' is a [[Java (programming language)|Java]]-based [[Open Source]] "full-featured, [[Multi-blog]], [[Multi-user]], and [[Multi-blog|group-blog]] server suitable for [[weblog|blog sites]] large and small".&lt;ref&gt;http://roller.apache.org/ Apache Roller official website&lt;/ref&gt; Roller was originally written by Dave Johnson in 2002 for a magazine article on open source development tools,&lt;ref&gt;http://onjava.com/pub/a/onjava/2002/04/17/wblogosj2ee.html&lt;/ref&gt; but became popular at FreeRoller.net (now JRoller.com) and was later chosen to drive the employee blogs at [[Sun Microsystems, Inc.]] and [[IBM developerWorks]] blogs.&lt;ref name="ohloh.net"/&gt; 

On April 23, 2007, Roller project graduated from incubator, so it became an official project of the [[Apache Software Foundation]] and it was released 3.1 version, first official release.&lt;ref&gt;{{cite web|url=http://www.apachenews.org/archives/000992.html |title=Apache News Online, 23 april 2007 - Announcing project graduation and new Apache Roller 3.1 release |publisher=Apache News |date=2007-04-23 |accessdate=2010-11-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20110928080043/http://www.apachenews.org/archives/000992.html |archivedate=28 September 2011 }}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://roller.apache.org Roller website]
{{Apache}}

[[Category:Apache Software Foundation|Roller]]
[[Category:Free content management systems]]
[[Category:Blog software]]
[[Category:Free software programmed in Java (programming language)]]

{{cms-software-stub}}
{{web-software-stub}}</text>
      <sha1>hegdd4541inicnn0fj4qg0lltuw5g6n</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OpenEJB</title>
    <ns>0</ns>
    <id>30858021</id>
    <revision>
      <id>792055298</id>
      <parentid>759377606</parentid>
      <timestamp>2017-07-24T05:25:29Z</timestamp>
      <contributor>
        <ip>180.181.102.164</ip>
      </contributor>
      <comment>update infobox with details of latest version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6536">{{ Infobox Software
| name                   = Apache OpenEJB 
| logo                   = &lt;!-- Deleted image removed: [[Image:Apache OpenEJB Logo.gif|150px|Apache OpenEJB Logo]]  --&gt;
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 7.0.3
| latest release date    = {{release date and age|2017|3|07}}
&lt;!-- don't know if there is any preview version at present
| latest preview version = 7.0.0-M1
| latest preview date    = {{release date and age|2015|12|12}}
--&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[EJB|EJB Container System]] 
| license                = [[Apache License]] 2.0
| website                = http://openejb.apache.org
}}

'''OpenEJB''' is an [[open source]], embeddable and lightweight [[EJB]] Container System and EJB Server, released under the [[Apache Software License|Apache 2.0 License]]. OpenEJB has been integrated with Java EE application servers such as [[Apache Geronimo|Geronimo]],&lt;ref&gt;[http://www-128.ibm.com/developerworks/opensource/library/os-ag-renegade14/ The Geronimo renegade: What's new in OpenEJB 3.0]&lt;/ref&gt; and [[WebObjects]].&lt;ref&gt;[http://www.ibm.com/developerworks/opensource/library/os-ag-renegade7/index.html?S_TACT=105AGX44&amp;S_CMP=ART OpenEJB and Apache Geronimo's EJB implementation An Interview with David Blevins] {{webarchive |url=https://web.archive.org/web/20121024195301/http://www.ibm.com/developerworks/opensource/library/os-ag-renegade7/index.html?S_TACT=105AGX44&amp;S_CMP=ART |date=October 24, 2012 }}&lt;/ref&gt;

==History==

OpenEJB was founded by Richard Monson-Haefel and David Blevins in December 1999.  At the time there were new vendors moving in the [[Business|enterprise]] [[Java (programming language)|Java]] space seemingly every week.  Rather than join the space as a competitor, the project was focused entirely on providing these new platforms with a way to quickly get EJB compliance via plugging OpenEJB into their application server.

The first to integrate OpenEJB in this fashion was Apple's [[WebObjects]] in late 2000, released in 2001.  When the project moved to [[Source Forge]] in 2002 an [[Apache Tomcat]] integration was created.  Again rather than follow what most in the industry were doing and putting Tomcat into OpenEJB, the project decided to follow its vision and provide an integration that allowed Tomcat users to plug in OpenEJB to gain EJB support in the Tomcat platform.  It was in this same vein of putting an EJB container into a Web server that the project developed the [http://openejb.apache.org/collapsed-ear.html Collapsed EAR] concept of putting EJBs inside the .war file.

As part of the work that OpenEJB did to prepare for the integration with Apple's WebObjects, a very large integration test suite was developed.  The test suite was developed as a generic application since it would need to be run against both [[WebObjects]] and other platforms that integrated OpenEJB.  For simplicity in the build the test suite, based on [[JUnit]], was run with OpenEJB right inside the tests rather than as a separate process, which was easy to do as the container was designed to be plugged into other platforms and make as little assumptions about its environment as possible.  It was from this work that the concept of combining an EJB application with plain unit tests and an embeddable EJB container was born.  Originally referred to as a "local" EJB container and what lead the project to describe itself as being able to run in two modes: Local and Remote.

In August 2003 the project helped launch the [[Apache Geronimo]] application server.  Originally a new version of OpenEJB was developed ground up based on Geronimo's GBean architecture and released as OpenEJB 2.0 which lived throughout the Geronimo 1.x cycle.  In 2006 when EJB 3.0 was released which had a focus on simplicity, the project went back to its roots and [http://blog.dblevins.com/2008/04/openejb-revival.html revived the OpenEJB 1.0 codebase], ported select bits of the 2.0 codebase, and eventually brought it up to the EJB 3.0 spec level in what is now called OpenEJB 3.0.
{| class="wikitable"  style="margin:auto; margin:0 0 0 2em; font-size:85%;"
|+ Apache OpenEJB Versions
|-
! Version
! Release Date
! Description
|-
| 0.01 (initial release)
| December 1999
| Born in [[Exolab]]
|-
| 
| January 2002
| Moved to [[SourceForge.net]]
|-
| 
| March 2004
| Moved to [[Codehaus]]
|-
|
| September 29, 2006
| Moved into the Apache Incubator
|-
|
| June 1, 2007
| Graduated as Apache OpenEJB
|-
| 4.7.3
| December 4, 2015
| Latest stable release.
|}

==Major features==
* Supports EJB 3.0, 2.1, 2.0, 1.1 in all modes; embedded, standalone or otherwise.
* Partial EJB 3.1 support
* [[JAX-WS]] support
* [[Java Message Service|JMS]] support
* [[Java EE Connector Architecture|J2EE connector]] support
* Can be dropped into Tomcat 5 or 6 adding various JavaEE 5 and EJB 3.0 features to a standard Tomcat install.
* [[Container-Managed Persistence|CMP]] support is implemented over [[Java Persistence API|JPA]] allowing to freely mix CMP and JPA usage.
* Complete support for [[GlassFish]] descriptors allowing those users to embedded test their applications.
* Incredibly flexible [[JNDI]] name support allows you to specify formats at macro and micro levels and imitate the format of other vendors.
* Allows for easy testing and debugging in [[Integrated development environment|IDE]]s such as [[Eclipse (software)|Eclipse]], [[IntelliJ IDEA]] or [[NetBeans]] with no [[Plug-in (computing)|plugin]]s required.
* Usable in ordinary [[JUnit]] or other style [[test case]]s without complicated setup or external processes.
* Validates applications entirely and reports all failures at once, with three selectable levels of detail, avoiding several hours worth of "fix, recompile, redeploy, fail, repeat" cycles.
* [[OSGi]] support&lt;ref&gt;[http://www.infoq.com/news/2008/05/openejb-3.0-release InfoQ: OpenEJB 3.0 Supports DI of Enums and Collections, OSGi and EJB 3.0 features&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

==See also==
*[[EJB|Enterprise Java Bean (EJB)]]

==References==
&lt;references /&gt;

==External links==
* {{official website|http://openejb.apache.org}}

{{Java Persistence API}}
{{apache}}

{{DEFAULTSORT:Apache Openejb}}
[[Category:Apache Software Foundation|OpenEJB]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>dbcdg94l34frq4q20lv2264sa951is1</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ActiveMQ</title>
    <ns>0</ns>
    <id>30874771</id>
    <revision>
      <id>845524860</id>
      <parentid>845524792</parentid>
      <timestamp>2018-06-12T09:51:37Z</timestamp>
      <contributor>
        <username>Broadwaygenius</username>
        <id>30130018</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/183.82.0.13|183.82.0.13]] ([[User talk:183.82.0.13|talk]]): Unexplained removal. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6029">{{Infobox software
| name = Apache ActiveMQ
| logo = [[File:Apache-activemq-logo.png|150px|Apache ActiveMQ Logo]]
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| latest release version = 5.15.3
| latest release date = {{release date|2018|02|12}}&lt;ref name="activemq-news"&gt;{{cite web|url=http://activemq.apache.org/news.html|title=News|accessdate=15 February 2018}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| genre = [[Java Message Service]] [[Message-oriented middleware]] [[Enterprise Messaging System]] [[Service-oriented architecture|SOA]]
| license = [[Apache License]] 2.0
| website = {{URL|http://activemq.apache.org}}
}}

'''Apache ActiveMQ''' is an [[open source]] [[message broker]] written in Java together with a full [[Java Message Service]] (JMS) client. It provides "Enterprise Features" which in this case means fostering the communication from more than one client or server. Supported clients include Java via JMS 1.1 as well as several other "cross language" clients.&lt;ref&gt;[http://activemq.apache.org/cross-language-clients.html Apache ActiveMQ - Cross Language Clients]&lt;/ref&gt; The communication is managed with features such as [[computer clustering]] and ability to use any [[database]] as a JMS [[persistence (computer science)|persistence]] provider besides [[virtual memory]], [[cache (computing)|cache]], and [[journal (computing)|journal]] persistency.&lt;ref&gt;[http://activemq.apache.org/features.html Apache ActiveMQ - Features&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

The ActiveMQ project was originally created by its founders from LogicBlaze in 2004, as an open source message broker, hosted by CodeHaus. The code and ActiveMQ trademark were donated to the Apache Software Foundation in 2007, where the founders continued to develop the codebase with the extended Apache community.

ActiveMQ employs several modes for high availability, including both file-system and database row-level locking mechanisms, sharing of the persistence store via a shared filesystem, or true replication using [[Apache ZooKeeper]].  A robust horizontal scaling mechanism called a Network of Brokers,&lt;ref&gt;[http://activemq.apache.org/networks-of-brokers.html ActiveMQ Network of Brokers]&lt;/ref&gt; is also supported out of the box.  In the enterprise, ActiveMQ is celebrated for its flexibility in configuration, and its support for a relatively large number of transport protocols, including OpenWire, [[Streaming Text Oriented Messaging Protocol|STOMP]], [[MQTT]], [[Advanced Message Queuing Protocol|AMQP]], REST, and WebSockets.&lt;ref&gt;[http://activemq.apache.org/protocols.html ActiveMQ Protocols]&lt;/ref&gt;

ActiveMQ is used in [[enterprise service bus]] implementations such as [[Apache ServiceMix]] and [[Mule (software)|Mule]]. Other projects using ActiveMQ include [[Apache Camel]] and [[Apache CXF]] in [[Service-Oriented Architecture|SOA]] infrastructure projects.&lt;ref&gt;[http://activemq.apache.org/projects-using-activemq.html Apache ActiveMQ - Projects using ActiveMQ]&lt;/ref&gt;

Coinciding with the release of Apache ActiveMQ 5.3, the world's first results for the SPECjms2007 industry standard benchmark were announced. Four results were submitted to the [[SPEC]] and accepted for publication. The results cover different topologies to analyze the scalability of Apache ActiveMQ in two dimensions.&lt;ref&gt;[http://www.dvs.tu-darmstadt.de/research/performance/jmsmom/amqjms2007.html Worlds first SPECjms2007 Results using ActiveMQ 5.3]&lt;/ref&gt;&lt;ref&gt;[http://www.spec.org/jms2007/results/res2009q4/ SPECjms2007 Results]&lt;/ref&gt;

ActiveMQ is currently in major version 5, minor version 15&lt;ref name="activemq-news"&gt;&lt;/ref&gt;. There's also a separate product called [[Apache ActiveMQ Artemis]] which is a new JMS Broker based on the [[HornetQ]] code base which was previously owned by [[RedHat|Red Hat]], and bringing the broker's JMS implementation up to the [https://java.net/projects/jms-spec/pages/JMS20FinalRelease 2.0 specification].&lt;ref&gt;[http://hornetq.blogspot.com/2015/06/hornetq-apache-donation-and-apache.html HornetQ Donation to ActiveMQ]&lt;/ref&gt; Red Hat, therefore, has a substantial control over the project due to its previous ownership of the code base. 

==See also==
{{Portal|Free software|Java}}
*[[Amazon SQS]]
*[[Amazon Simple Notification Service]]
*[[StormMQ]]
*[[Apache Qpid]]
*[[Message-oriented middleware]]
*[[Enterprise messaging system]]
*''[[Enterprise Integration Patterns]]''
*[[Service-oriented architecture]]
*[[Event-driven SOA]]
*[[RabbitMQ]]
*[https://activemq.apache.org/artemis/ Apache ActiveMQ Artemis]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{citation
| first1     = Bruce
| last1      = Snyder
| first2     = Dejan
| last2      = Bosanac
| first3     = Rob
| last3      = Davies
| date       = March 28, 2010
| title      = ActiveMQ in Action
| edition    = 1st
| publisher  = [[Manning Publications]]
| pages      = 375
| isbn       = 978-1-933988-94-8
| url        = 
}}
{{Refend}}

==External links==
*{{Official website|http://activemq.apache.org}}
*[http://searchsoa.techtarget.com/feature/Book-Excerpt-ActiveMQ-in-Action "Apache ActiveMQ" excerpt from ActiveMQ in Action]
*[https://web.archive.org/web/20111003055351/http://www.javabeat.net/articles/267-deploying-activemq-for-large-numbers-of-concurrent-appl-1.html "Deploying ActiveMQ for large numbers of concurrent applications" excerpt from ActiveMQ in Action]
*[http://www.spec.org/jms2007/results/res2009q4/ SPECjms2007 Results]
*[http://qnalist.com/g/activemq ActiveMq Mailing List Archives]
*[http://www.devglan.com/spring-mvc/spring-jms-activemq-integration-example ActiveMQ Integration in Java]

{{Apache}}

{{DEFAULTSORT:Apache Activemq}}
[[Category:Message-oriented middleware]]
[[Category:Apache Software Foundation|ActiveMQ]]
[[Category:Service-oriented architecture-related products]]
[[Category:Enterprise application integration]]
[[Category:Java enterprise platform]]</text>
      <sha1>epockq1wot2dfom13hm7854fgkhpm2t</sha1>
    </revision>
  </page>
  <page>
    <title>Commons Daemon</title>
    <ns>0</ns>
    <id>3233639</id>
    <revision>
      <id>839992547</id>
      <parentid>828728585</parentid>
      <timestamp>2018-05-07T01:25:28Z</timestamp>
      <contributor>
        <username>Googol30</username>
        <id>16053044</id>
      </contributor>
      <minor/>
      <comment>removing unknown parameter</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1795">{{noref|date=November 2010}}
{{Infobox Software
| name                   = Commons Daemon
| logo                   = 
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.1.0
| latest release date    = {{release date|2017|11|23}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = 
| license                = [[Apache License]] 2.0
| website                = http://commons.apache.org/daemon
}}
'''Commons Daemon''', formerly known as '''JSVC''', is a [[Java (Sun)|Java]] [[Library (computer science)|software library]] belonging to the [[Apache Software Foundation|Apache]] [[Apache Commons|Commons Project]].

Daemon provides a portable means of starting and stopping a [[Java Virtual Machine]] (JVM) that is running server-side applications.  Such applications often have additional requirements compared to client-side applications.  For example, the servlet container [[Apache Tomcat|Tomcat 4]] would need to serialize sessions and shutdown web applications before the JVM process terminates.

Daemon comprises 2 parts: a native library written in  [[C (programming language)|C]] that interfaces with the operating system, and the library that provides the Daemon API, written in Java.

There are two ways to use Commons Daemon: by implementing the daemon interface or by calling a class that provides the required methods for daemon. For example, Tomcat-4.1.x uses the daemon interface and Tomcat-5.0.x provides a class whose methods are called by JSVC directly.


{{Apache}}
[[Category:Java (programming language) libraries]]
[[Category:Apache Software Foundation|Commons Daemon]]</text>
      <sha1>j0feamo6wwwuar2whkf8mao3lvxa8hs</sha1>
    </revision>
  </page>
  <page>
    <title>Apache JMeter</title>
    <ns>0</ns>
    <id>1389448</id>
    <revision>
      <id>830336698</id>
      <parentid>830334406</parentid>
      <timestamp>2018-03-14T05:27:06Z</timestamp>
      <contributor>
        <username>Walter Görlitz</username>
        <id>121745</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/157.39.31.173|157.39.31.173]] ([[User talk:157.39.31.173|talk]]) to last revision by Milamberspace. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5491">{{primary sources|date=September 2013}}
{{Infobox software
| name                   = Apache JMeter
| logo                   = Apache JMeter.png
| screenshot             = Jakarta jmeter screenshot.png
| caption                = Apache JMeter screenshot
| developer              = [[Apache Software Foundation]] 
| latest release version = 4.0
| latest release date    = {{release date|2018|02|10}}
| latest preview version = 
| latest preview date    = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Load testing]]
| license                = [[Apache License]] 2.0
| website                = {{url|//jmeter.apache.org/}}
| repo                   = {{url|jmeter.apache.org/svnindex.html}}
}}
'''Apache JMeter''' is an [[Apache Software Foundation|Apache]] [[project]] that can be used as a [[load testing]] tool for analyzing and measuring the performance of a variety of services, with a focus on [[web application]]s.

JMeter can be used as a unit-test tool for JDBC database connections,&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/build-db-test-plan.html |title= Apache JMeter - User's Manual: Building a Database Test Plan |publisher= jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; FTP,&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/build-ftp-test-plan.html |title= Apache JMeter - User's Manual: Building an FTP Test Plan |publisher= Jmeter.apache.org |date= |accessdate= 2013-09-20}}&lt;/ref&gt; LDAP,&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/build-ldap-test-plan.html |title= Apache JMeter - User's Manual: Building an LDAP Test Plan |publisher= Jmeter.apache.org |date= |accessdate= 2015-04-05}}&lt;/ref&gt; Webservices,&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/build-ws-test-plan.html |title= Apache JMeter - User's Manual: Building a WebService Test Plan |publisher= Jmeter.apache.org |date= |accessdate= 2013-09-20}}&lt;/ref&gt; JMS,&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/build-jms-topic-test-plan.html |title= Apache JMeter - User's Manual: Building a JMS (Java Messaging Service) Test Plan |publisher= Jmeter.apache.org |date= |accessdate= 2013-09-20}}&lt;/ref&gt; HTTP,&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/build-web-test-plan.html |title= Apache JMeter - User's Manual: Building a Web Test Plan |publisher= Jmeter.apache.org |date= |accessdate= 2013-09-20}}&lt;/ref&gt; generic TCP connections and OS native processes.{{cn|date=April 2016}} One can also configure JMeter as a monitor,&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/build-monitor-test-plan.html |title= Apache JMeter - User's Manual: Building a Monitor Test Plan |publisher= Jmeter.apache.org |date= |accessdate= 2013-09-20}}&lt;/ref&gt; although this is typically considered{{by whom?|date=April 2016}} ''ad hoc'' rather than advanced monitoring. It can be used for some functional testing as well.&lt;ref&gt;{{cite web|url= http://jmeter.apache.org/usermanual/intro.html#history |title= Apache JMeter - User's Manual: Introduction |publisher= Jmeter.apache.org |date= |accessdate= 2013-09-20}}&lt;/ref&gt;

JMeter supports variable parameterization, assertions (response validation), per-thread cookies, configuration variables and a variety of reports. 

JMeter architecture is based on [[plugins]]. Most of its "out of the box" features are implemented with plugins. Off-site developers can easily extend JMeter with custom plugins.

== JMeter Plugins ==

JMeter Plugins is an independent project for Apache JMeter. Each plugin serves different purpose and expedites the process of creating and executing JMeter Test Plan. Users can install the plugins via Plugin Manager. Currently there are 64 plugins available.

== Releases ==
{| class="wikitable sortable"
|+ Apache JMeter versions
|-
! Version
! Release date
! Description
|-
| 1.0
| 1998-12-15
| first official release
|-
|-
| 1.0.2
| 1999-02-05
| earliest in archive
|-
| ... 
| ...
| 
|-
| 2.3RC3 
| 2007-07-11
|
|-
| 2.3RC4
| 2007-09-02
|
|-
| 2.3 
| 2007-09-24
| 
|-
| 2.3.1
| 2007-11-28
|
|-
| 2.3.2
| 2008-06-10
|
|-
| 2.3.3
| 2009-05-24
|
|-
| 2.3.4
| 2009-06-21
| Java 1.4+
|-
| 2.4
| 2010-07-14
| Java 5+
|-
| 2.5
| 2011-08-17
| Java 5
|-
| 2.5.1
| 2011-10-03
| Java 5+
|-
| 2.6
| 2012-02-01
| Java 5+
|-
| 2.7
| 2012-05-27
| Java 5+
|-
| 2.8
| 2012-10-06
| Java 5+
|-
| 2.9
| 2013-01-28
| Java 6+
|-
| 2.10
| 2013-10-21
| Java 6+
|-
| 2.11
| 2014-01-05
| Java 6+
|-
| 2.12
| 2014-11-10
| Java 6+
|-
| 2.13
| 2015-03-14
| Java 6+
|-
| 3.0
| 2016-05-17
| Java 7+
|-
| 3.1
| 2016-11-19
| Java 7+
|-
| 3.2
| 2017-04-13
| Java 8+
|-
| 3.3
| 2017-09-21
| Java 8
|-
| 4.0
| 2018-02-10
| Java 8 / 9
|}
Source:&lt;ref&gt;{{cite web|url=http://projects.apache.org/projects/jmeter.html |title=JMeter's releases |publisher=ASF |date= |accessdate=2015-04-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20150419065701/http://projects.apache.org/projects/jmeter.html |archivedate=2015-04-19 }}&lt;/ref&gt;

==See also==
*[[iMacros]]
*[[Performance Engineering]]
*[[Selenium (software)]]
*[[Software performance testing]]
*[[Software testing]]
*[[Web server benchmarking]]

== References ==
{{Reflist}}

== External links ==
{{Portal|Software Testing}}
* {{official website|//jmeter.apache.org/}}
*[http://jmeter-plugins.org/ Example of JMeter Custom Plugins]

{{apache}}

[[Category:Apache Software Foundation|JMeter]]
[[Category:Java development tools]]
[[Category:Java enterprise platform]]
[[Category:Load testing tools]]

{{Network-software-stub}}</text>
      <sha1>s1q63xstau3iejplqwsuuasneyifjoa</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Apache Software Foundation members</title>
    <ns>14</ns>
    <id>31555734</id>
    <revision>
      <id>450533126</id>
      <parentid>425149077</parentid>
      <timestamp>2011-09-14T21:00:59Z</timestamp>
      <contributor>
        <username>Grossenhayn</username>
        <id>12441884</id>
      </contributor>
      <minor/>
      <comment>Adding {{Commons cat}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="91">{{Commons cat|Apache Software Foundation members}}

[[Category:Apache Software Foundation]]</text>
      <sha1>juuigbwzun04f51srxu7ppc48plyetr</sha1>
    </revision>
  </page>
  <page>
    <title>Apache MINA</title>
    <ns>0</ns>
    <id>32517945</id>
    <revision>
      <id>792183835</id>
      <parentid>785517980</parentid>
      <timestamp>2017-07-25T00:08:44Z</timestamp>
      <contributor>
        <ip>70.184.214.35</ip>
      </contributor>
      <comment>alphabetized</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2975">{{Infobox software
| name                   = Apache MINA
| logo                   = [[File:Apache-MINA-logo.png]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 2.0.16
| latest release date    = {{release date|2016|10|31}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Enterprise Integration Patterns]] [[Message Oriented Middleware]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//mina.apache.org}}
}}

'''Apache MINA''' ('''M'''ultipurpose '''I'''nfrastructure for '''N'''etwork '''A'''pplications)&lt;ref&gt;https://mina.apache.org/mina-project/faq.html#what-does-mina-mean&lt;/ref&gt; is an [[open source]] [[Java (programming language)|Java]] network [[application framework]]. MINA can be used to create [[Scalability|scalable]], high performance [[Computer network|network applications]]. MINA provides unified [[Application programming interface|API]]s for various transports like [[Transmission Control Protocol|TCP]], [[User Datagram Protocol|UDP]], [[serial communication]]. It also makes it easy to make an implementation of custom transport type. MINA provides both high-level and low-level network APIs.

A user application interacts with MINA APIs, shielding the user application from low level [[I/O]] details. MINA internally uses I/O APIs to perform the actual I/O functions. This makes it easy for the users to concentrate on the application logic and leave the I/O handling to Apache MINA.&lt;ref&gt;{{cite web|title=Apache MINA - FAQ|url=http://mina.apache.org/mina-project/faq.html|accessdate=2016-03-08| archiveurl= https://web.archive.org/web/20110725012454/http://mina.apache.org/faq.html| archivedate= 25 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

==Advantages==
*Unified APIs for various transports (TCP/UDP etc.)&lt;ref&gt;{{cite web|title=Apache MINA features|url=http://mina.apache.org/mina-project/features.html|accessdate=2016-03-08}}&lt;/ref&gt;
*Provides high/low level APIs
*Customizable Thread Model
*Easy Unit Testing using [[Mock object|Mock Objects]]
*Integration with DI frameworks like [[Spring Framework|Spring]], [[Google Guice]], picocontainer
*JMX Manageability

==Tooling==
Graphical tools such as [[Eclipse IDE]], [[IntelliJ IDEA]] can be used.

==Alternatives==
*[[Project Grizzly (software)|Grizzly]]
*[[Netty (software)|Netty 4]]
*QuickServer
*xSocket

==See also==
{{Portal|Free software}}
*[[Apache Camel]]
*[[Enterprise messaging system]]
*[[Message-oriented middleware]]
*[[Service-oriented architecture]]
*[[Event-driven SOA]]

==References==
{{Reflist|colwidth=50em}}

==External links==
*{{Official website}}

{{Apache}}

[[Category:Apache Software Foundation|MINA]]
[[Category:Java platform]]
[[Category:Message-oriented middleware]]

{{Network-software-stub}}</text>
      <sha1>rfe2adqsqy659s1731y0yphglf4dbl1</sha1>
    </revision>
  </page>
  <page>
    <title>Apache James</title>
    <ns>0</ns>
    <id>15896167</id>
    <revision>
      <id>829389900</id>
      <parentid>800227679</parentid>
      <timestamp>2018-03-08T09:58:25Z</timestamp>
      <contributor>
        <username>Renamed user 2560613081</username>
        <id>16847332</id>
      </contributor>
      <comment>/* top */ Cleanup. Removed erroneous parameters from the infobox. For a list of supported parameters please consult [[Template:Infobox software/doc]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4807">{{Infobox Software
| name                   = Apache James
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = 
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = 
| latest release version = 3.0.0
| latest release date    = {{release date and age|2017|07|20}}
| latest preview version =
| latest preview date    =
| programming language   = [[Java (programming language)|Java]]
| operating system       = 
| platform               = [[Java SE]]
| language               = 
| status                 = 
| genre                  = [[Mail transfer agent]], [[news server]]
| license                = [[Apache License]]
| website                = {{url|//james.apache.org}}
}}

'''Apache James''', a.k.a. '''Java Apache Mail Enterprise Server''' or some variation thereof, is an [[open source]] [[Simple Mail Transfer Protocol|SMTP]] and [[Post Office Protocol|POP3]] [[mail transfer agent]] and [[Network News Transfer Protocol|NNTP]] [[news server]] written entirely in [[Java (programming language)|Java]].&lt;ref name="overview"&gt;[http://james.apache.org/server/index.html James Server - Overview]&lt;/ref&gt; James is maintained by contributors to the [[Apache Software Foundation]], with initial contributions by [[Serge Knystautas]].&lt;ref&gt;[http://james.apache.org/weare.html James Project - Who We Are]&lt;/ref&gt;&lt;ref name="boardminutes"/&gt; [[Internet Message Access Protocol|IMAP]] support has been added as of preview version 3.0-M2,&lt;ref&gt;[http://james.apache.org/imap/index.html 3.0-M2]&lt;/ref&gt; which now requires Java 1.5 or later.

The James project manages the Apache Mailet [[Application programming interface|API]] which defines "matchers" and "mailets". These allow users to write their own mail-handling code, such as to update a database, build a message archive, or filter [[e-mail spam|spam]].&lt;ref name="overview"/&gt; A matcher is used to classify messages based on some criteria, and then determines whether the message should be passed to an appropriate mailet for processing. Mailets are so-called due to their conceptual similarity to a [[servlet]],&lt;ref name="ibm"&gt;{{cite web |url=http://www.ibm.com/developerworks/java/library/j-james1.html |title=Working with James |accessdate=2008-02-22 |author=Claude Duguay |date=2003-06-10 |publisher=[[IBM]]| archiveurl= https://web.archive.org/web/20080129160018/http://www.ibm.com/developerworks/java/library/j-james1.html#resources| archivedate= 29 January 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; and arose because [[Sun Microsystems]] declined a proposal to include mail-handling in the servlet implementation.&lt;ref name="boardminutes"/&gt; James ships with a variety of pre-written matchers and mailets to serve common tasks.&lt;ref name="ibm"/&gt; Many sets of mailets and matchers can be combined to produce sophisticated and complex functional behaviour.

The Apache James project also produces pure Java libraries for implementing Sender Policy Framework (SPF), the [[Sieve (mail filtering language)|Sieve]] mail filtering language, and parsing MIME content streams, independent of Sun's JavaMail API.

==Development==
James was originally formed under the [[Jakarta Project]] as Jakarta-James.

&lt;ref name="boardminutes"&gt;
{{cite web |url=http://www.apache.org/foundation/records/minutes/2003/board_minutes_2003_01_22.txt |title=Board of Directors Meeting Minutes |accessdate=2008-02-23 |date=2003-01-22 |publisher=[[Apache Software Foundation]]}}&lt;/ref&gt; In January 2003, James was upgraded to a top-level Apache project in a unanimous decision by the ASF Board of Directors under the chairmanship of Serge Knystautas.

James is distributed to within the Phoenix container,&lt;ref name="ibm"/&gt; which implements the [[Apache Avalon]] [[application framework]].

Recent developments include a version which runs in the [[Spring Framework]] application framework.

Version 2.3.0 was released in October 2006.

Version 2.3.1 was released in April 2007.

Version 2.3.2 was released in August 2009.&lt;ref&gt;[http://james.apache.org/newsarchive.html James Project - News Archive]&lt;/ref&gt;

Version 2.3.2.1 (security fix) was released in September 08, 2015

Version 3.0.0 was released in July 20, 2017

==See also==
* [[Comparison of mail servers]]
* [[List of mail servers]]

==References==
{{reflist}}

==External links==
* {{official website|//james.apache.org}}
* [https://www.ibm.com/developerworks/java/library/j-james1.html Working with James] at IBM developerWorks

{{apache}}
{{Email servers}}

[[Category:Apache Software Foundation|James]]
[[Category:Free email server software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Message transfer agents]]
[[Category:Usenet servers]]
[[Category:Email server software for Linux]]</text>
      <sha1>l29sekv3br41txg6c4ghy2n9y3ysxlb</sha1>
    </revision>
  </page>
  <page>
    <title>Apache TomEE</title>
    <ns>0</ns>
    <id>34750556</id>
    <revision>
      <id>831813975</id>
      <parentid>824044811</parentid>
      <timestamp>2018-03-22T09:35:59Z</timestamp>
      <contributor>
        <username>UncommonDesign</username>
        <id>33363717</id>
      </contributor>
      <minor/>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7855">{{ Infobox Software
| name                   = Apache TomEE
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.7.5
| latest release date    = {{release date and age|2017|10|24}}
| latest preview version = 7.0.4
| latest preview date    = {{release date and age|2017|09|26}}
| operating system       = [[Cross-platform]] ([[JVM]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[application server|Web Application Server]]
| license                = [[Apache License]] 2.0
| website                = http://tomee.apache.org
}}
'''Apache TomEE''' (pronounced "Tommy") is the [[Java Enterprise Edition]] of [[Apache Tomcat]] (Tomcat + Java EE = TomEE) that combines several Java enterprise projects including [[Apache OpenEJB]], Apache OpenWebBeans, [[Apache OpenJPA]], [[Apache MyFaces]] and others.&lt;ref&gt;{{cite web|url=http://openejb.apache.org/apache-tomee.html|title=Apache TomEE|publisher=[[Apache OpenEJB]]}}&lt;/ref&gt; In October 2011, the project obtained certification by [[Oracle Corporation]] as a compatible implementation of the Java EE 6 Web Profile.&lt;ref&gt;{{cite web|url=http://www.marketwatch.com/story/the-apache-software-foundation-announces-apache-tomee-certified-as-java-ee-6-web-profile-compatible-2011-10-04|title=The Apache Software Foundation Announces Apache TomEE Certified as Java EE 6 Web Profile Compatible|publisher=[[MarketWatch]]|date=4 Oct 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.infoworld.com/d/application-development/apache-tomee-web-stack-gains-approval-175341|title=Apache TomEE Web stack gains approval|publisher=[[InfoWorld]]|date=7 Oct 2011}}&lt;/ref&gt;

== Components ==

TomEE includes the following open-source components:

{| class="wikitable"
|-
! Component
! Description
|-
| [[Apache Tomcat]] 
| HTTP server and Servlet container supporting [[Java Servlet]] and [[JavaServer Pages]] (JSP).
|-
| [[Apache OpenEJB]] 
| Open-source [[Enterprise JavaBean]]s (EJB) container system.
|-
| [[Apache OpenWebBeans]] 
| Open-source [[Java Contexts and Dependency Injection]] (CDI) implementation.
|-
| [[Apache OpenJPA]] 
| Open-source [[Java Persistence API]] (JPA) 2.1 implementation.
|-
| [[Apache Geronimo Transaction]] 
| Open-source [[Java Transaction API]] (JTA) 1.2 implementation.
|-
| [[Apache MyFaces]] 
| Open-source [[Java Server Faces]] (JSF) implementation.
|-
| [[Apache ActiveMQ]] 
| Open-source [[Java Message Service|Java Message Service (JMS)]] implementation.
|-
| [[Apache CXF]] 
| [[Web Services]] frameworks with a variety of protocols - such as SOAP, XML/HTTP, [[Representational State Transfer|RESTful]] [[HTTP]].
|-
| [[Apache Derby]] 
| Full-fledged [[relational database management system]] (RDBMS) with native [[Java Database Connectivity]] (JDBC) support.
|}

'''TomEE JAX-RS''', a second distribution, adds support for [[Java API for RESTful Web Services]] (JAX-RS).

The full '''TomEE Plus''' distribution adds additional support for:&lt;ref&gt;
{{cite web|url=http://tomee.apache.org/comparison.html|title=Apache TomEE comparison
}}
&lt;/ref&gt;

* [[Java API for XML Web Services]] (JAX-WS)
* Java EE Connector Architecture
* Java Messaging Service (JMS)

== History ==

The OpenEJB project was started by Richard Monson-Haefel and David Blevins in 1999 as an open source implementation of the Enterprise JavaBeans specification.  Blevins continued to develop OpenEJB and integrate components of this project with Apache Geronimo.  In 2003, the OpenEJB component became a project operating under the auspices of the Apache Software Foundation at which time it was rewritten with a focus on leveraging Tomcat as an embedded web container.  A beta version of TomEE was releases in October 2011, and the first production-ready version was shipped in April 2012.&lt;ref&gt;{{cite web|title=Apache TomEE|publisher=[[Apache Software Foundation]]|url=http://tomee.apache.org/apache-tomee.html}}&lt;/ref&gt;

=== Versions ===

{| class="wikitable"
|-
! Version !! Release Date
|-
| 7.0.4 || September 26, 2017&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12339959&amp;styleName=Html&amp;projectId=12312320|title=Apache TomEE 7.0.4 released|date=26 September 2017}}&lt;/ref&gt;
|-
| 7.0.3 || March 13, 2017&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12338670&amp;styleName=Html&amp;projectId=12312320|title=Apache TomEE 7.0.3 released|date=13 March 2017}}&lt;/ref&gt;
|-
| 7.0.2 || November 6, 2016&lt;ref&gt;{{cite web|url=http://tomee.apache.org/tomee-7.0.2-release-notes.html|title=Apache TomEE 7.0.2 released|date=6 November 2016}}&lt;/ref&gt;
|-
| 7.0.1 || June 23, 2016&lt;ref&gt;{{cite web|url=https://openejb.apache.org/tomee-7.0.1-release-notes.html|title=Apache TomEE 7.0.1 released|date=23 June 2016}}&lt;/ref&gt;
|-
| 7.0.0 || May 25, 2016&lt;ref&gt;{{cite web|url=https://openejb.apache.org/tomee-7.0.0-release-notes.html|title=Apache TomEE 7.0.0 released|date=25 May 2016}}&lt;/ref&gt;
|-
| 7.0.0-M3 || March 6, 2016&lt;ref&gt;{{cite web|url=https://tomee.apache.org/tomee-7.0.0-M3.html|title=Apache TomEE 7.0.0-M3 released|date=6 Mar 2016}}&lt;/ref&gt;
|-
| 7.0.0-M2 || March 1, 2016&lt;ref&gt;{{cite web|url=http://tomee-openejb.979440.n4.nabble.com/ANNOUNCE-Apache-TomEE-7-0-0-M2-released-td4677727.html|title=Apache TomEE 7.0.0-M2 released|date=1 Mar 2016}}&lt;/ref&gt;
|-
| 7.0.0-M1 || December 12, 2015&lt;ref&gt;{{cite web|url=http://tomee-openejb.979440.n4.nabble.com/TomEE-7-0-0-M1-is-here-td4677207.html|title=TomEE 7.0.0-M1 is here!|date=12 Dec 2015}}&lt;/ref&gt;
|-
|1.7.5
|October 24, 2017&lt;ref&gt;{{Cite web|url=https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12312320&amp;version=12335485|title=Release Notes - ASF JIRA|website=issues.apache.org|language=en|access-date=2018-02-05}}&lt;/ref&gt;
|-
| 1.7.4 || March 7, 2016&lt;ref&gt;{{cite web|url=https://tomee.apache.org/tomee-1.7.4.html|title=Apache TomEE 1.7.4 released|date=7 Mar 2016}}&lt;/ref&gt;
|-
| 1.7.3 || December 4, 2015
|-
| 1.7.2 || May 18, 2015
|-
| 1.7.1 || September 12, 2014
|-
| 1.7.0 || August 10, 2014
|-
| 1.6.0.2 || May 6, 2014
|-
| 1.6.0.1 || April 16, 2014
|-
| 1.6.0 || November 17, 2013
|-
| 1.5.2 || March 17, 2013
|-
| 1.5.1 || December 14, 2012
|-
| 1.5.0 || September 28, 2012
|-
| 1.0.0 || April 27, 2012
|-
| 1.0.0 Beta 2 || January 17, 2012
|-
| 1.0.0 Beta 1 || October 2, 2011
|}

== Commercial Support ==

Two years after the announcement of Apache TomEE at JavaOne 2011 several Apache TomEE creators united to form Tomitribe,&lt;ref&gt;{{cite web|url=http://jaxenter.com/tomee-creator-lifts-lid-on-new-company-tomitribe-48210.html|title=TomEE creator lifts lid on new company Tomitribe}}&lt;/ref&gt; a commercial support company dedicated to the Apache TomEE community and focused on promoting open source values.&lt;ref&gt;{{cite web|url=http://www.tomitribe.com/blog/2013/11/feed-the-fish/|title=Feed the Fish}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.tomitribe.com/blog/2013/09/who-are-the-real-heroes-of-open-source/|title=Who Are The Real Heroes Of Open Source?}}&lt;/ref&gt;

Another commercial support company provides enterprise support for Apache TomEE is ManageCat,.&lt;ref&gt;{{cite web|url=http://managecat.com|title=ManageCat, Cloud Management and Monitoring}}&lt;/ref&gt;  ManageCat involves with a lot of Apache Java EE projects to contribute open source Java EE ecosystem.

== See also ==
* [[Comparison of application servers]]

==References==
{{reflist}}

==External links==
*{{Official website|http://tomee.apache.org}}
*[http://www.slideshare.net/stratwine/apache-tomee-tomcat-with-a-kick Apache TomEE - Tomcat with a Kick at JAXLondon 2011]
*[http://www.slideshare.net/dblevins1/2011-java-oneapachetomeejavaee6webprofile Apache TomEE Java EE 6 Web Profile at JavaOne 2011]

{{apache}}

[[Category:Apache Software Foundation|TomEE]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>7ddlmymfb18g93micfzxvto6ldan5bg</sha1>
    </revision>
  </page>
  <page>
    <title>Apache CXF</title>
    <ns>0</ns>
    <id>11787206</id>
    <revision>
      <id>847580331</id>
      <parentid>847580306</parentid>
      <timestamp>2018-06-26T10:59:31Z</timestamp>
      <contributor>
        <ip>14.141.114.218</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4626">{{Advert|date=October 2016}}
{{Infobox Software
| name                   = Apache CXF
| logo                   =
| screenshot             = 
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 3.2.4
| latest release date    = {{release date|2018|3|25}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web Services]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://cxf.apache.org}}
}}

Apache CXF is an [[Open source software|open-source]], fully featured [[Web services]] framework. It originated as the combination of two [[Open source software|open-source]] projects: [[Celtix]] developed by [[IONA Technologies]] and [[Codehaus XFire|XFire]] developed by a team hosted at [[Codehaus]]. These two projects were combined by people working together at the [[Apache Software Foundation]] and the new name '''CXF''' was derived by combining "''Celtix''" and "''XFire''".

The CXF key design considerations include:
*Clean separation of [[Front-end and back-end|front-ends]], like [[Java API for XML Web Services|JAX-WS]], from the core [[Source code|code]].
*Simplicity with, for instance, the creation of [[Client (computing)|clients]] and endpoints without annotations.
*High performance with minimum [[computational overhead]].
*Embeddable Web service component: example embeddings include [[Spring Framework]] and [[Apache Geronimo|Geronimo]].

CXF is often used with [[Apache ServiceMix]], [[Apache Camel]] and [[Apache ActiveMQ]] in [[service-oriented architecture]] (SOA) infrastructure projects.

==Features==
CXF includes a broad feature set, but it is primarily focused on the following areas:
*Web Services Standards Support:
**[[SOAP]]
**[[WS-Addressing]]
**[[WS-Policy]]
**[[WS-ReliableMessaging]]
**[[WS-SecureConversation]]
**[[WS-Security]]
**[[WS-SecurityPolicy]]
*[[JAX-WS]] API for Web service development
**[[Java (software platform)|Java]]-first support
**[[Web Services Description Language|WSDL]]-first tooling
*[[Java API for RESTful Web Services|JAX-RS]] (JSR 339 2.0) API for [[Representational State Transfer|RESTful]] Web service development
*[[JavaScript]] programming model for service and client development
*[[Apache Maven|Maven]] tooling
*[[CORBA]] support
*[[HTTP]], [[Java Message Service|JMS]] and [[WebSocket]] transport layers
*Embeddable Deployment:
**ServiceMix or other [[Java Business Integration|JBI]] containers
**Geronimo or other [[Java EE]] containers
**[[Apache Tomcat|Tomcat]] or other servlet containers
**[[OSGi]]
*Reference [[OSGi]] Remote Services implementation

==Commercial support==
Enterprise support for CXF is available from independent vendors, including [http://www.redhat.com/support/we-are/ Red Hat], [https://web.archive.org/web/20101227135323/http://www.jboss.com/products/platforms/application/components/ JBoss], [http://talend.com/ Talend], and [http://www.sosnoski.com Sosnoski Software Associates]. See the [http://cxf.apache.org/support.html CXF Support Page] for details on all support options.

==See also==
{{Portal|Java}}
*[[JAX-WS|JAX-WS RI]], the reference implementation of the JAX-WS specification, used directly by [[GlassFish Metro]]
*The [[Apache Axis|Axis Web Services framework]]
*[[Apache Wink]], a project in incubation with JAX-RS support
*The [https://jcp.org/en/jsr/detail?id=339 JAX-RS] specification
*[[List of web service frameworks]]

==Further reading==
*[http://www.pluralsight.com/training/Courses/TableOfContents/building-jax-ws-web-services-apache-cxf Building JAX-WS Web Services with Apache CXF]

==External links==
*[https://cxf.apache.org Apache CXF website]
*[https://cxf.apache.org/download.html Apache CXF download]
*[https://cxf.apache.org/docs/index.html Apache CXF documentation]
*[https://www.jboss.org/products/fuse.html JBoss Fuse website]
*[http://www.methodsandtools.com/tools/tools.php?apachecxf Apache CXF Evaluation]
*[http://www.oreillynet.com/onjava/blog/2007/07/apache_cxf_interview_with_dan.html Apache CXF: Interview with Dan Diephouse and Paul Brown]
*[http://www.talend.com/products/esb-standard-edition.php Talend ESB home page]
*[http://cxf.apache.org/people.html List of Apache CXF Committers]

{{Apache}}
{{Java API for RESTful Web Services}}

[[Category:Apache Software Foundation|CXF]]
[[Category:Web services]]
[[Category:Web service specifications]]
[[Category:Web applications]]
[[Category:Java enterprise platform]]
[[Category:Java (programming language) libraries]]</text>
      <sha1>4w756cskovw3sqa3qyyfn4p79mob567</sha1>
    </revision>
  </page>
  <page>
    <title>Hortonworks</title>
    <ns>0</ns>
    <id>35142247</id>
    <revision>
      <id>839428258</id>
      <parentid>839045181</parentid>
      <timestamp>2018-05-03T10:49:07Z</timestamp>
      <contributor>
        <username>A12jun</username>
        <id>13191526</id>
      </contributor>
      <minor/>
      <comment>Improved wording and flow in opening statement.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19969">{{Infobox company
| name             = Hortonworks
| logo             = File:Hortonworks logo.gif
| type             = [[Public company|Public]]
| traded_as        = {{NASDAQ|HDP}}
| foundation       = {{start date and age|2011}}
| location_city    = [[Santa Clara, California]]
| location_country = United States
| num_employees    = ~1,110 {{small|(2017)}}&lt;ref&gt;{{cite web|url=http://hortonworks.com/about-us/quick-facts/|title=Hortonworks : Quick Facts - Hortonworks|publisher=|accessdate=15 July 2017}}&lt;/ref&gt;
| key_people       = &lt;!-- only if notable and cited --&gt;
| industry         = [[Software|Computer software]]
| products         = Hortonworks Data Platform, Hortonworks DataFlow
| services         = 
| homepage         = {{url|http://hortonworks.com/|Hortonworks.com}}
}}

'''Hortonworks''' is a big data software company based in [[Santa Clara, California]]. The company develops, supports, and provides expertise on an expansive set of entirely open source software designed to manage data and processing for everything from IOT (think: connected car), to single view of X (think: customer, risk, patient), to advanced analytics and machine learning (think: next best action and realtime cybersecurity).  Hortonworks believes it is a data management company bridging the cloud and the datacenter. Hortonworks now offers three interoperable product lines: HDP (based on [[Apache Hadoop]], [[Apache Hive]], [[Apache Spark]]), HDF (based on [[Apache NiFi]], [[Apache Storm]], [[Apache Kafka]]), and Data Plane Services (based on [[Apache Atlas]] and [[Cloudbreak]] and a pluggable architecture into which partners such as [[IBM]] can add their services. &lt;ref&gt;{{cite web|url=https://www.datanami.com/2018/04/17/hortonworks-upgrades-data-plane-service/|title=Hortonworks upgrades DataPlane Services|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|accessdate=April 30, 2018|}}&lt;/ref&gt; 

==History==
{{news release|section|date=February 2017}}
Hortonworks was formed in June 2011 as an independent company, funded by $23 million [[venture capital]] from  [[Yahoo!]] and [[Benchmark Capital]]. Its first office was in [[Sunnyvale, California]].&lt;ref name="info"&gt;{{Cite news|title=Hadoop Big Data Startup Spins Out Of Yahoo |work=Information Week |date=June 29, 2011 |author=Charles Babcock |url=http://www.informationweek.com/news/development/database/231000658 |deadurl=no |archiveurl=https://web.archive.org/web/20110704055947/http://www.informationweek.com/news/development/database/231000658 |archivedate=July 4, 2011 |accessdate=February 21, 2017 |df= }}&lt;/ref&gt; The company employs contributors to the [[open source software]] project [[Apache Hadoop]].&lt;ref&gt;{{cite news |url= https://www.reuters.com/article/us-splunk-bigdata-idUSBRE83K00B20120421 |authors= Sarah McBride and Alistair Bar |date= April 20, 2012 |title= Big-data investors look for the next Splunk |publisher= Reuters |accessdate= February 21, 2017 }}&lt;/ref&gt; 
The Hortonworks Data Platform (HDP) product includes Apache Hadoop and is used for storing, processing, and analyzing large volumes of data. The platform is designed to deal with data from many sources and formats.
The platform includes Hadoop technology such as the Hadoop Distributed File System, [[MapReduce]], Pig, Hive, [[HBase]], [[Apache ZooKeeper|ZooKeeper]], and additional components.&lt;ref&gt;{{Cite news |title= HortonWorks Hones a Hadoop Distribution |work= PC World |date= November 1, 2011 |author= Joab Jackson |url= http://www.pcworld.com/article/242916/hortonworks_hones_a_hadoop_distribution.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

Eric Baldeschweiler (from Yahoo) was initial chief executive, and Rob Bearden chief operating officer, formerly from [[SpringSource]]. Benchmark partner [[Peter Fenton (venture capitalist)|Peter Fenton]] was a board member.The company name refers to the character [[Horton the Elephant]], since the elephant is the symbol for Hadoop.&lt;ref name="info" /&gt;&lt;ref&gt;{{Cite news |title= Yahoo! seeds Hadoop startup on open source dream: Hortonworks hears a Big Data revolution |author= Cade Metz |date= June 28, 2011 |work= The Register |url= https://www.theregister.co.uk/2011/06/28/yahoo_spins_off_hadoop_startup/ |accessdate= February 21, 2017 }}&lt;/ref&gt;
Additional investors included a $25 million round led by [[Index Ventures]] in November 2011.&lt;ref&gt;{{Cite news |title= Everything you ever wanted to know about Yahoo’s Hadoop spinoff Hortonworks |author= Derrick Harris |work= Giga Om |date= August 20, 2013 |url= http://gigaom.com/2013/08/20/everything-you-ever-wanted-to-know-about-yahoo-spinoff-hortonworks/ |accessdate= October 14, 2013 }}&lt;/ref&gt;

In October 2011 Hortonworks announced [[Microsoft]] would collaborate on a Hadoop distribution for [[Microsoft Azure]] and [[Windows Server]].&lt;ref&gt;{{Cite news |title= Microsoft climbs onto Hadoop bandwagon  |date= October 12, 2011 |author= Jaikumar Vijayan |work= Computer World |url= http://www.computerworld.com/s/article/9220779/Microsoft_climbs_onto_Hadoop_bandwagon |accessdate= October 14, 2013 }}&lt;/ref&gt;
On February 25, 2013, Hortonworks announced availability of a beta version of the Hortonworks Data Platform for Windows.&lt;ref&gt;{{Cite news | title=Hortonworks and Microsoft Bring the Hortonworks Data Platform to Windows|date=February 25, 2013|work=SQL Server Blog|author=Herain Oberoi|url= https://blogs.technet.microsoft.com/dataplatforminsider/2013/02/25/hortonworks-and-microsoft-bring-the-hortonworks-data-platform-to-windows/ |accessdate= October 5, 2016}}&lt;/ref&gt;
In November 2011 it announced HParser software from [[Informatica]] would be available for free download by its customers.&lt;ref&gt;{{Cite news |title= Informatica, Hortonworks Team Up to Release Free Data Parser for Hadoop |work= e Week |author= Chris Preimesberger  |date= November 2, 2011 |url= http://www.eweek.com/c/a/Enterprise-Applications/Informatica-Hortonworks-Team-Up-to-Release-Free-Data-Parser-for-Hadoop-788698 |accessdate= October 14, 2013 }}&lt;/ref&gt;

In February 2012 [[Teradata]] announced an alliance.&lt;ref&gt;{{Cite news |title= Teradata and Hortonworks Join Forces for a Big Data Boost |date= February 21, 2012 |author= Quentin Hardy |work= Bits blog |publisher= The New York Times |url= http://bits.blogs.nytimes.com/2012/02/21/teradata-and-hortonworks-join-forces-for-a-big-data-boost |accessdate= October 14, 2013 }}&lt;/ref&gt;
In October 2012 Teradata's [[Aster Data Systems]] division announced an appliance supporting Hortonworks' distribution,&lt;ref&gt;{{Cite news |title= It's happening: Hadoop and SQL worlds are converging |author= Tony Baer |date= October 26, 2012 |work= ZDNet |url=http://www.zdnet.com/its-happening-hadoop-and-sql-worlds-are-converging-7000006455/ |accessdate= October 14, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Teradata Big Analytics Appliance Enables New Business Insights on All Enterprise Data |date= October 17, 2012 |publisher= Teradata Aster |work= Press release  |url= http://www.asterdata.com/news/teradata-aster-big-analytics-appliance-breaks-down-barriers.php |accessdate= October 14, 2013 }}&lt;/ref&gt;
and Impetus Technologies announced a partnership.&lt;ref&gt;{{Cite news |title= Impetus and Hortonworks Strategic Partnership |work= Press release |date=  October 2, 2012  |author= Pletus |url= https://finance.yahoo.com/news/impetus-hortonworks-strategic-partnership-103000189.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

In June 2013 Hortonworks announced another $50 million in financing, from previous investors and adding [[Tenaya Capital]] and Dragoneer Investment Group.&lt;ref&gt;{{Cite news |title= Hortonworks Raises $50M For Expansion And Development In Growing Hadoop Oriented Data Analytics Market |author= Alex Williams |date= June 25, 2013 |work= Tech Crunch |url= https://techcrunch.com/2013/06/25/hortonworks-raises-50m-for-expansion-and-development-in-growing-hadoop-oriented-data-analytics-market/ |accessdate= October 14, 2013 }}&lt;/ref&gt;
In September 2013 [[SAP AG]] announced it would resell the Hortonworks distribution (as well as one from Intel).&lt;ref&gt;{{Cite news |title= SAP looks to boost 'big data' position with Hadoop deals, new apps |quote= SAP agrees to resell Hadoop distributions from Intel and Hortonworks; unveils specialized use case apps |date= September 11, 2013 |author= Chris Kanaracus |work= Computer World |url= http://www.computerworld.com/s/article/9242337/SAP_looks_to_boost_big_data_position_with_Hadoop_deals_new_apps |accessdate= October 14, 2013 }}&lt;/ref&gt;
By the end of 2013, Bearden, who had become chief executive by 2012, denied rumors the company would soon go public or be acquired.&lt;ref&gt;{{Cite news |title= Rob 'Flipper' Bearden plans to FLOAT his Hadoop heffalump: Hortonworks hears an IPO-OOOO |author= Gavin Clarke |date= November 21, 2013 |work= The Register |url= https://www.theregister.co.uk/2011/06/28/yahoo_spins_off_hadoop_startup/ |accessdate= February 21, 2017 }}&lt;/ref&gt;

In March 2014, another $100 million investment was announced, led by [[BlackRock]] and passport capital.&lt;ref&gt;{{Cite news |title= Hortonworks loads up $100M to keep fighting the Hadoop fight |author= Jordan Novet |date= March 25, 2014 |work= Venture Beat |url= https://venturebeat.com/2014/03/25/hortonworks-loads-up-100m-to-keep-fighting-the-hadoop-fight/ |accessdate= February 21, 2017 }}&lt;/ref&gt;
In May 2014, Hortonworks acquired XA Secure, a small data security company founded in January 2013, for undisclosed terms.&lt;ref&gt;{{Cite news |title= Hortonworks buys XA Secure to make Hadoop even more enterprise-friendly |author= Jordan Novet |date= May 15, 2014 |work= Venture Beat |url= https://venturebeat.com/2014/05/15/hortonworks-buys-xa-secure-to-make-hadoop-even-more-enterprise-friendly/ |accessdate= February 21, 2017 }}&lt;/ref&gt;
The company used a provision of the [[Jumpstart Our Business Startups Act]] to avoid disclosing registration forms with the [[United States Securities and Exchange Commission]] in June 2014.&lt;ref&gt;{{Cite news |title= Short of $100M Revenue Goal, Hadoop Provider Hortonworks Files for IPO |author= Deborah Gage |date= November 10, 2014 |work= The Wall Street Journal |url= https://blogs.wsj.com/venturecapital/2014/11/10/hadoop-provider-hortonworks-files-for-ipo/ |access-date= February 21, 2017 }}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title= Response to Confidential Draft Registration Statement Submitted June 27, 2014 |date= July 24, 2014 |url= https://www.sec.gov/Archives/edgar/data/1610532/000000000014037586/filename1.pdf |publisher= US SEC |access-date= February 21, 2017 }}&lt;/ref&gt;
The filings showed about $37 million lost in 2013, and $87 million in the first half of 2014.&lt;ref&gt;{{Cite news |title= Hortonworks Lifts The Lid On Its IPO Plans--Documents Show Little Revenue And Big Losses |author= Ben Kepes |date= November 10, 2014 |work= Forbes |url= https://www.forbes.com/sites/benkepes/2014/11/10/hortonworks-lifts-the-lid-on-its-ipo-plans-s1-shows-little-revenue-and-big-losses |access-date= February 21, 2017 }}&lt;/ref&gt;
In December 2014, Hortonworks had their [[initial public offering]], listed on the [[NASDAQ]] as HDP. Almost $100 million was raised.&lt;ref&gt;{{cite web|title=Hortonworks IPO|url=http://www.nasdaq.com/markets/ipos/company/hortonworks-inc-949081-76930|publisher=NASDAQ |accessdate= February 21, 2017 }}&lt;/ref&gt;

In August, 2015, Hortonworks announced it would acquire Onyara, Inc., the creator of [[Apache NiFi]], a top-level open source project. Apache NiFi was made available through the NSA technology transfer program in 2014. Over eight years, Onyara’s engineers were the key contributors to the U.S. government software project that evolved into Apache NiFi.&lt;ref&gt;{{cite web|url=https://techcrunch.com/2015/08/25/hortonworks-acquires-onyara-early-startup-with-roots-in-nsa/|title=Hortonworks Acquires Onyara, Early Startup With Roots In NSA|first1=Ron|last1=Miller|first2=Alex|last2=Wilhelm|publisher=|accessdate=17 February 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://fortune.com/2015/08/26/hortonworks-buys-onyara/|title=Hortonworks to acquire Onyara to add a focus on small data |accessdate= February 17, 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://hortonworks.com/press-releases/hortonworks-to-acquire-onyara-to-turn-internet-of-anything-data-into-actionable-insights/|title=Hortonworks to Acquire Onyara to Turn Internet of Anything Data Into Actionable Insights - Hortonworks|publisher=|accessdate= February 17, 2017}}&lt;/ref&gt;

When a secondary share offering was announced in January 2016, the share price dropped to about half of what it was at the IPO.&lt;ref&gt;{{Cite news |title= Hortonworks (HDP) Stock Plunging on Secondary Share Offering |author= Kaya Yurieff |date= January 19, 2016 |work= The Street |url= https://www.thestreet.com/story/13428336/1/hortonworks-hdp-stock-plunging-on-secondary-share-offering.html |accessdate= February 21, 2017 }}&lt;/ref&gt;
After operating losses deepened through 2015 and 2016, Herb Cunitz (who had been president since 2012) left the company in August 2016.&lt;ref&gt;{{Cite news |title= Hortonworks losses deepen despite growth in subscription sales: Unprofitable firm still down on market expectations |author= Kat Hall |date= August 5, 2016 |work= The Register |url= https://www.theregister.co.uk/2016/08/05/hortonworks_q2_fy2016_results/ |accessdate= February 21, 2017 }}&lt;/ref&gt; In 2016, the company was ranked #42 on the [[Deloitte Fast 500#Fast 500 North America|Deloitte Fast 500 North America]] list.&lt;ref&gt;{{cite web|url=https://www2.deloitte.com/content/dam/Deloitte/us/Documents/technology-media-telecommunications/us-tmt-2016-fast-500-winners-by-rank.pdf|title=2016 Winners by rank|publisher=[[Deloitte]]|accessdate=31 October 2017}}&lt;/ref&gt; After a round of layoffs, Raj Verma was the president and chief operating officer from January 2017 to July 2017.&lt;ref&gt;{{Cite news |title= Hadoop hurler Hortonworks votes Tibco veteran for president: Corporate veteran to re-invigorate big-data sales |author= Alexander J Martin |date= January 13, 2017 |work= The Register |url= https://www.theregister.co.uk/2017/01/13/president_raj_verma_arrives_at_hortonworks_to_spur_desperate_sales/ |accessdate= February 21, 2017 }}&lt;/ref&gt; Scott Davidson was named the new chief operating officer from July 2017.

Originally a host with Yahoo! of the Hadoop Summit in California, Hortonworks now hosts the DataWorks Summit community trade show on several continents.&lt;ref&gt;{{Cite web |title= Sponsors |work= Hadoop Summit 2013 web site |deadurl=no |url= https://dataworkssummit.com |archiveurl= https://web.archive.org/web/20130430202257/http://hadoopsummit.org/san-jose/sponsors/ |archivedate= April 30, 2013 |accessdate= October 14, 2013 }}&lt;/ref&gt;
Hortonworks is a sponsor of the [[Apache Software Foundation]].&lt;ref&gt;[http://www.apache.org/foundation/thanks.html Sponsors], Apache Software Foundation &lt;/ref&gt;

==Partnerships==
Hortonworks partners with software-related companies, including [[IBM]], [[SAS Institute|SAS]], [[BMC Software]] for business service management and automation,&lt;ref&gt;{{cite web|url=http://www.infoworld.com/t/hadoop/hortonworks-bmc-team-support-hadoop-automation-242857|title=Hortonworks, BMC team up to support Hadoop automation|first=Serdar|last=Yegulalp|publisher=|accessdate=17 February 2017}}&lt;/ref&gt;  
[[Attunity]]&lt;ref&gt;{{cite web|url=http://www.complianceweek.com/attunity-hortonworks-partner-to-simplify-big-data-integration/article/230615/|title=Attunity, Hortonworks Partner to Simplify Big Data Integration - Compliance Week|publisher=|accessdate=17 February 2017}}&lt;/ref&gt; and [[Cleo (company)|Cleo]] for data integration,&lt;ref&gt;{{cite web|url=http://hortonworks.com/blog/secure-reliable-hadoop-data-transfer-option-cleo-mft/|title=Secure, reliable Hadoop data transfer with Cleo MFT - Hortonworks|date=1 April 2014|publisher=|accessdate=17 February 2017}}&lt;/ref&gt; and [[SAP AG|SAP]] and [[VMware]] for cloud, database and other virtualization infrastructure, and with [[Stratoscale]].&lt;ref&gt;{{cite web|url=http://gigaom.com/2012/06/12/hortonworks-teams-with-vmware-to-keep-hadoop-running/|title=Hortonworks releases Hadoop distro, teams with VMware|first=Derrick|last=Harris|date=12 June 2012|publisher=|accessdate=17 February 2017}}&lt;/ref&gt;  In 2015 Hortonworks started partnering with [[ManTech Commercial Services]] and B23 to develop OpenSOC.&lt;ref&gt;William Terdoslavich for InformationWeek. October 1, 2015 [http://www.informationweek.com/cloud/hortonworks-aims-to-make-hadoop-easier-to-use/d/d-id/1322438Hortonworks Aims To Make Hadoop Easier To Use]&lt;/ref&gt;


==Hortonworks Data Platform (HDP)==
The Hortonworks Data Platform, powered by [[Apache Hadoop]], is a massively scalable and 100% open source platform for storing, processing and analyzing large volumes of data. It is designed to deal with data from many sources and formats in a very quick, easy and cost-effective manner. The Hortonworks Data Platform consists of the essential set of [[Apache Hadoop]] projects including [[MapReduce]], [[Hadoop Distributed File System]] (HDFS), [[HCatalog]], Pig, Hive, HBase, Zookeeper and [[Apache_Ambari|Ambari]]. Hortonworks is the major contributor of code and patches to many of these projects. These projects have been integrated and tested as part of the Hortonworks Data Platform release process and installation and configuration tools have also been included.&lt;ref&gt;{{Cite |title=Hortonworks Data Platform crochure|url= http://info.hortonworks.com/rs/549-QAL-086/images/hdp-buyers-guide.pdf | accessdate= January 13, 2018 }}&lt;/ref&gt;

The HDP distribution consists of the following components:&lt;ref&gt;{{Cite |title= About Hortonworks Data Platform |url= https://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.3.0/bk_getting-started-guide/content/ch_about-hortonworks-data-platform.html | accessdate= January 13, 2018 }}&lt;/ref&gt;

[[File:Hortonworks Data Platform June 04.png|thumb|HDP Components]]
* Core Hadoop platform (Hadoop HDFS and Hadoop MapReduce)
* Non-relational database (Apache HBase)
* Metadata services (Apache HCatalog)
* Scripting platform (Apache Pig)
* Data access and query (Apache Hive)
* Workflow scheduler ([[Apache Oozie]])
* Cluster coordination (Apache Zookeeper)
* Management and monitoring (Apache Ambari)
* Data integration services (HCatalog APIs, WebHDFS, Talend Open Studio for Big Data, and Apache Sqoop)
* Distributed log management services (Apache Flume)
* Machine learning library ([[Mahout]])

The foundational components of HDP are YARN and Hadoop Distributed File System (HDFS). While HDFS provides the scalable, fault-tolerant, cost-efficient storage for Big Data lake, YARN provides the centralized architecture that enables organizations to process multiple workloads simultaneously. YARN also provides the resource management and pluggable architecture for enabling a wide variety of data access methods. HDP enables enterprises to deploy, integrate and work with unprecedented volumes of structured and unstructured data.

With YARN at its architectural center, HDP provides a range of processing engines that allow users to simultaneously interact with data in multiple ways. YARN enables a range of access methods to coexist within the same cluster against shared datasets, thereby avoiding unnecessary and costly data silos.

HDP enables multiple data processing engines that range from interactive SQL, real-time streaming, data science and batch processing to leverage data stored in a single platform thereby unlocking an entirely new approach to analytics.

===Deployment===
HDP can be deployed using any one of the following options:
* Easy tryouts for HDP
* Automated Install ([[Ambari]])
* Deploying Manually installing software packages (RPMs)

==References==
{{reflist|30em}}

==External links==
* {{Official website }}

[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Companies based in Sunnyvale, California]]
[[Category:Companies listed on NASDAQ]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Companies established in 2011]]
[[Category:Big data companies]]
[[Category:2014 initial public offerings]]</text>
      <sha1>nfbbnc3tko9hks3ragpqfvm2kn5j15r</sha1>
    </revision>
  </page>
  <page>
    <title>OpenCMIS</title>
    <ns>0</ns>
    <id>35624889</id>
    <revision>
      <id>728286213</id>
      <parentid>572858546</parentid>
      <timestamp>2016-07-04T11:13:50Z</timestamp>
      <contributor>
        <username>Diego Moya</username>
        <id>124142</id>
      </contributor>
      <comment>Explain what it's for</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1012">'''OpenCMIS''' is a subproject of the Apache Chemistry project of the Apache Software Foundation (ASF).
It is an [[Open Source]] collection of Java libraries, frameworks and tools around the [[Content Management Interoperability Services|CMIS]] specification for document [[interoperability]].

The goal of OpenCMIS is to make CMIS simple for client and server Java developers. It hides the binding details and provides APIs and SPIs on different abstraction levels. It also includes test tools for content repository developers and client application developers.

The OpenCMIS products are:

* A server-side Java library
* A client-side Java library
* A client-side Android library
* CMIS Workbench: a CMIS heavy client, that allows all CMIS operations, and allows to run the TCK unit tests
* OpenCMIS Server Webapps: a CMIS server and web user interface
* OpenCMIS JCR Repository: A wrapper to use JCR repositories via CMIS
* OpenCMIS Bridge


[[Category:Apache Software Foundation]]
[[Category:Free software]]</text>
      <sha1>tgnomb4dauqgwksaq04cuqjqz4o1oa1</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Struts 1</title>
    <ns>0</ns>
    <id>453331</id>
    <revision>
      <id>799583674</id>
      <parentid>796162069</parentid>
      <timestamp>2017-09-08T16:15:34Z</timestamp>
      <contributor>
        <username>Alistair1978</username>
        <id>86816</id>
      </contributor>
      <minor/>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6257">{{for|the successor of Apache Struts 1|Apache Struts 2}}
{{More footnotes|date=April 2009}}
{{Infobox software
| name                   = Apache Struts
| logo                   = [[File:Struts logo.gif|frameless|Apache Struts Logo]]
| author                 = [[Craig McClanahan]]
| developer              = [[Apache Software Foundation]]
| released               = {{Start date|2000|05}}
| discontinued           = {{Start date and age|2013|04|05}}&lt;ref&gt;[http://struts.apache.org/struts1eol-press.html Apache Struts 1 EOL Press Release]&lt;/ref&gt;
| latest release version = 1.3.10
| latest release date    = {{release date|2008|12|08}}
| status                 = [[End-of-life (product)|End-of-life]],&lt;ref&gt;[http://struts.apache.org/struts1eol-announcement.html Apache Struts 1 EOL Announcement]&lt;/ref&gt; superseded by [[Apache Struts 2]]
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| platform               = [[Cross-platform]] ([[Java Virtual Machine|JVM]])
| license                = [[Apache License]] 2.0
| website                = {{URL|http://struts.apache.org}}
}}
'''Apache Struts 1''' is an [[open-source]] [[web application framework]] for developing [[Java EE]] [[web application]]s. It uses and extends the [[Java Servlet]] [[application programming interface|API]] to encourage developers to adopt a [[model–view–controller]] (MVC) architecture. It was originally created by [[Craig McClanahan]] and donated to the [[Apache Foundation]] in May 2000. Formerly located under the Apache [[Jakarta Project]] and known as '''Jakarta Struts''', it became a top-level Apache project in 2005.

The [[WebWork]] framework spun off from Apache Struts aiming to offer enhancements and refinements while retaining the same general architecture of the original Struts framework. However, it was announced in December 2005 that Struts would re-merge with WebWork. WebWork 2.2 has been adopted as [[Apache Struts 2]], which reached its first full release in February 2007.

== Design goals and overview ==
In a standard [[Java Platform, Enterprise Edition|Java EE]] web application, the client will typically call to the server via a [[Form (web)|web form]]. The information is then either handed over to a [[Java Servlet]] which interacts with a database and produces an [[HTML]]-formatted response, or it is given to a [[JavaServer Pages]] (JSP) document that intermingles HTML and Java code to achieve the same result.
Both approaches are often considered inadequate for large projects because they mix application logic with presentation and make maintenance difficult.

The goal of Struts is to separate the ''model'' (application logic that interacts with a database) from the ''view'' (HTML pages presented to the client) and the ''controller'' (instance that passes information between view and model). Struts provides the controller (a servlet known as &lt;code&gt;ActionServlet&lt;/code&gt;) and facilitates the writing of templates for the view or presentation layer (typically in JSP, but [[XML]]/[[Extensible Stylesheet Language Transformations|XSLT]] and [[Jakarta Velocity|Velocity]] are also supported). The web application programmer is responsible for writing the model code, and for creating a central configuration file &lt;code&gt;struts-config.xml&lt;/code&gt; that binds together model, view, and controller.

Requests from the client are sent to the controller in the form of "Actions" defined in the configuration file; if the controller receives such a request it calls the corresponding Action class that interacts with the application-specific model code. The model code returns an "ActionForward", a string telling the controller what output page to send to the client. Information is passed between model and view in the form of special [[JavaBeans]]. A powerful custom tag library allows it from the presentation layer to read and write the content of these beans without the need for any embedded Java code.

Struts is categorized as a [[JSP model 2 architecture|Model 2]] request-based web application framework.&lt;ref&gt;{{cite web|first = Tony|last = Shan|year = 2006|accessdate = 2010-10-10|url = http://portal.acm.org/citation.cfm?id=1190953|title = Taxonomy of Java Web Application Frameworks|publisher = Proceedings of 2006 IEEE International Conference on e-Business Engineering (ICEBE 2006)}}&lt;/ref&gt;

Struts also supports [[Internationalization and localization|internationalization]] by web forms, and includes a template mechanism called "Tiles" that (for instance) allows the presentation layer to be composed from independent header, footer, menu navigation and content components.

== See also ==
*[[Comparison of web frameworks]]

== References ==
{{Reflist}}

== Bibliography ==
{{Refbegin}}
* [[James Holmes (programmer)|James Holmes]]: &lt;cite&gt;Struts: The Complete Reference&lt;/cite&gt;, McGraw-Hill Osborne Media, {{ISBN|0-07-223131-9}}
* Bill Dudney and Jonathan Lehr: &lt;cite&gt;Jakarta Pitfalls&lt;/cite&gt;, Wiley, {{ISBN|978-0-471-44915-7}}
* [[Bill Siggelkow]]: &lt;cite&gt;Jakarta Struts Cookbook&lt;/cite&gt;, O'Reilly, {{ISBN|0-596-00771-X}}
* [[James Goodwill]], [[Richard Hightower]]: &lt;cite&gt;Professional Jakarta Struts&lt;/cite&gt;, [[Wrox Press]], {{ISBN|0-7645-4437-3}}
* John Carnell and [[Rob Harrop]]: &lt;cite&gt;Pro Jakarta Struts, Second Edition&lt;/cite&gt;, Apress, {{ISBN|1-59059-228-X}}
* John Carnell, [[Jeff Linwood]] and [[Maciej Zawadzki]]: &lt;cite&gt;Professional Struts Applications: Building Web Sites with Struts, ObjectRelationalBridge, Lucene, and Velocity&lt;/cite&gt;, Apress, {{ISBN|1-59059-255-7}}
* [[Ted Husted]], etc.: &lt;cite&gt;Struts in Action&lt;/cite&gt;, Manning Publications Company, {{ISBN|1-930110-50-2}}
* [http://www.softwaresummit.com/2003/speakers/AshleyStrutsView.pdf Struts View Assembly and Validation], (PDF format).
* Stephan Wiesner: &lt;cite&gt;Learning Jakarta Struts 1.2&lt;/cite&gt;, Packt Publishing, 2005 {{ISBN|1-904811-54-X}}
{{Refend}}

== External links ==
* {{Official website}}
{{Application frameworks}}
{{apache}}

[[Category:Apache Software Foundation|Struts]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java enterprise platform]]
[[Category:Web frameworks]]
[[Category:Software using the Apache license]]</text>
      <sha1>1rddpwpgkwhjiwgud2355ijwbgdtggr</sha1>
    </revision>
  </page>
  <page>
    <title>Apache CouchDB</title>
    <ns>0</ns>
    <id>13427539</id>
    <revision>
      <id>841240408</id>
      <parentid>833021896</parentid>
      <timestamp>2018-05-14T18:45:35Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Roylenferink moved page [[CouchDB]] to [[Apache CouchDB]] over redirect: Naming conform the ASF project name</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19507">{{Infobox software
| name                   = Apache CouchDB
| logo                   = CouchDB.svg
| logo size              = 100px
| screenshot             = Apache CouchDB v2.1.1 Fauxton Console.png
| screenshot size        = 300px
| caption                = CouchDB's Futon Administration Interface, User database
| collapsible            = 
| author                 = Damien Katz, Jan Lehnardt, Noah Slater, Christopher Lenz, J. Chris Anderson, Paul Davis, Adam Kocoloski, Jason Davies, Benoît Chesneau, Filipe Manana, Robert Newson
| developer              = [[Apache Software Foundation]]
| released               = 2005
| latest release version = 2.1.1
| latest release date    = {{release date|2017|11|07}} &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = 
| repo = {{URL|https://github.com/apache/couchdb}}
| status                 = Active
| programming language   = [[Erlang (programming language)|Erlang]]
| operating system       = [[Cross-platform]]
| language               = 
| genre                  = [[Document-oriented database]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//couchdb.apache.org/}}
}}

'''Apache CouchDB''' is [[Open-source software|open source]] database software that focuses on ease of use and having a scalable architecture. It has a [[Document-oriented database|document-oriented]] [[NoSQL]] database architecture and is implemented in the concurrency-oriented language [[Erlang (programming language)|Erlang]]; it uses [[JSON]] to store data, [[JavaScript]] as its query language using [[MapReduce]], and [[HTTP]] for an [[API]].&lt;ref name="official-website"&gt;{{cite web|last=Apache Software Foundation|title=Apache CouchDB|url=http://couchdb.apache.org/|accessdate=15 April 2012}}&lt;/ref&gt;

CouchDB was first released in 2005 and later became an [[Apache Software Foundation]] project in 2008.

Unlike a [[relational database]], a CouchDB database does not store data and relationships in tables. Instead, each database is a collection of independent documents. Each document maintains its own data and self-contained schema. An application may access multiple databases, such as one stored on a user's mobile phone and another on a server. Document metadata contains revision information, making it possible to merge any differences that may have occurred while the databases were disconnected.

CouchDB implements a form of [[multiversion concurrency control]] (MVCC) so it does not lock the database file during writes. Conflicts are left to the application to resolve. Resolving a conflict generally involves first merging data into one of the documents, then deleting the stale one.&lt;ref&gt;{{cite web|last=Smith|first=Jason|title=What is the CouchDB replication protocol? Is it like Git?|url=https://stackoverflow.com/a/4766398/395287|work=StackOverflow|publisher=Stack Exchange|accessdate=14 April 2012}}&lt;/ref&gt;

Other features include document-level [[ACID]] semantics with [[eventual consistency]], (incremental) [[MapReduce]], and (incremental) replication. One of CouchDB's distinguishing features is [[multi-master replication]], which allows it to scale across machines to build high performance systems.  A built-in Web application called Fauxton (formerly Futon) helps with administration.

== History ==
''Couch'' is an acronym for ''cluster of unreliable commodity hardware''.&lt;ref&gt;{{Cite web |title= Exploring CouchDB |work= Developer Works |publisher= IBM |date= March 31, 2009 |url= http://www.ibm.com/developerworks/opensource/library/os-couchdb/index.html |accessdate= September 30, 2016 }}&lt;/ref&gt; 
The CouchDB project was created in April 2005 by Damien Katz, former [[Lotus Notes]] developer at [[IBM]]. He self-funded the project for almost two years and released it as an open source project under the [[GNU General Public License]].

In February 2008, it became an [[Apache Incubator]] project and was offered under the [[Apache License]] instead.&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/incubator-general/200802.mbox/%3c3d4032300802121136p361b52ceyfc0f3b0ad81a1793@mail.gmail.com%3e Apache mailing list announcement] on mail-archives.apache.org&lt;/ref&gt; A few months after, it graduated to a top-level project.&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/incubator-couchdb-dev/200811.mbox/%3c3F352A54-5FC8-4CB0-8A6B-7D3446F07462@jaguNET.com%3e Re: Proposed Resolution: Establish CouchDB TLP] on mail-archives.apache.org&lt;/ref&gt; This led to the first stable version being released in July 2010.&lt;ref&gt;[https://www.pcworld.com/article/201046/couchdb_nosql_database_ready_for_production_use.html "CouchDB NoSQL Database Ready for Production Use"], article from PC World of July 2010&lt;/ref&gt;

In early 2012, Katz left the project to focus on [[Couchbase Server]].&lt;ref&gt;{{cite web|last=Katz|first=Damien|title=The future of CouchDB|url=http://damienkatz.net/2012/01/the_future_of_couchdb.html|accessdate=15 April 2012}}&lt;/ref&gt;

Since Katz's departure, the Apache CouchDB project has continued, releasing 1.2 in April 2012 and 1.3 in April 2013. In July 2013, the CouchDB community merged the codebase for [[BigCouch]], [[Cloudant]]'s clustered version of CouchDB, into the Apache project.&lt;ref&gt;{{cite web|last=Slater|first=Noah|title=Welcome BigCouch|url=https://blogs.apache.org/couchdb/entry/welcome_bigcouch|accessdate=25 July 2013}}&lt;/ref&gt; The BigCouch clustering framework is included in the current release of Apache CouchDB.&lt;ref name="couch2bigcouch"&gt;{{cite web|url=https://blog.couchdb.org/2016/09/20/2-0/|title= '2.0'|accessdate= 13 January 2017}}&lt;/ref&gt;

Native clustering is supported at version 2.0.0. And the new Mango Query Server provides a simple JSON-based way to perform CouchDB queries without JavaScript or MapReduce.

== Main features ==
; ACID Semantics
: CouchDB provides [[atomicity, consistency, isolation, durability|ACID]] semantics.&lt;ref name="ACID"&gt;[http://couchdb.apache.org/docs/overview.html CouchDB, Technical Overview] {{webarchive |url=https://web.archive.org/web/20111020074113/http://couchdb.apache.org/docs/overview.html |date=October 20, 2011 }}&lt;/ref&gt; It does this by implementing a form of [[Multi-Version Concurrency Control]], meaning that CouchDB can handle a high volume of concurrent readers and writers without conflict.
; Built for Offline
: CouchDB can replicate to devices (like smartphones) that can go offline and handle data sync for you when the device is back online.
; Distributed Architecture with Replication
: CouchDB was designed with bi-direction replication (or synchronization) and off-line operation in mind. That means multiple replicas can have their own copies of the same data, modify it, and then sync those changes at a later time.
; Document Storage
: CouchDB stores data as "documents", as one or more field/value pairs expressed as [[JSON]]. Field values can be simple things like strings, numbers, or dates; but [[Array data structure|ordered lists]] and [[associative array]]s can also be used. Every document in a CouchDB database has a unique id and there is no required document schema.
; Eventual Consistency
: CouchDB guarantees [[eventual consistency]] to be able to provide both availability and partition tolerance.
; Map/Reduce Views and Indexes
: The stored data is structured using views. In CouchDB, each view is constructed by a [[JavaScript]] function that acts as the Map half of a [[map (higher-order function)|map]]/reduce operation. The function takes a document and transforms it into a single value that it returns. CouchDB can index views and keep those indexes updated as documents are added, removed, or updated.
; HTTP API
: All items have a unique URI that gets exposed via HTTP. It uses the [[Hypertext Transfer Protocol#Request methods|HTTP methods]] POST, GET, PUT and DELETE for the four basic [[Create, read, update and delete|CRUD]] (Create, Read, Update, Delete) operations on all resources.

CouchDB also offers a built-in administration interface accessible via Web called Futon.&lt;ref&gt;[http://guide.couchdb.org/draft/tour.html#welcome "Welcome to Futon"] from "CouchDB The Definitive Guide"&lt;/ref&gt;

== Use cases and production deployments ==
Replication and synchronization capabilities of CouchDB make it ideal for using it in mobile devices, where network connection is not guaranteed, and the application must keep on working offline.

CouchDB is well suited for applications with accumulating, occasionally changing data, on which pre-defined queries are to be run and where versioning is important (CRM, CMS systems, by example). Master-master replication is an especially interesting feature, allowing easy multi-site deployments.&lt;ref name="Analyse Kristof Kovacs"&gt;[http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase comparison] from Kristóf Kovács&lt;/ref&gt;

=== Users ===

Users of CouchDB include:
* [[Amadeus IT Group]], for some of their back-end systems.{{citation needed|date=June 2015}}
* [[Credit Suisse]], for internal use at commodities department for their marketplace framework.&lt;ref name="couchdbinthewild"&gt;[http://wiki.apache.org/couchdb/CouchDB_in_the_wild "CouchDB in the wild"] article of the product's Web, a list of software projects and websites using CouchDB&lt;/ref&gt;{{better source|reason=Given source is a wiki. See WP:SPS.|date=June 2015}}
* [[Meebo]], for their social platform (Web and applications).{{citation needed|date=January 2016}} Meebo was acquired by Google and most products were shut down on July 12, 2012.&lt;ref&gt;{{cite web|url=https://techcrunch.com/2012/06/09/meebo-product-shutdown/ |title=Meebo Gets The Classic Google Acq-hire Treatment: Most Products To Shut Down Soon |first=Kim-Mai |last=Cutler |date=9 June 2012|accessdate=7 January 2016|work=TechCrunch|publisher=AOL Inc.}}&lt;/ref&gt;
* [[npm (software)|npm]], for their package registry.&lt;ref&gt;{{cite web|url=https://github.com/npm/npm-registry-couchapp|title=npm-registry-couchapp|website=GitHub|publisher=npm|date=17 June 2015|accessdate=7 January 2016}}&lt;/ref&gt;
* [[Sophos]], for some of their back-end systems.{{citation needed|date=June 2015}}
* The [[BBC]], for its dynamic content platforms.&lt;ref&gt;[http://www.erlang-factory.com/conference/London2009/speakers/endafarrell CouchDB at the BBC as a fault tolerant, scalable, multi-data center key-value store]&lt;/ref&gt;
* [[Canonical (company)|Canonical]] began using it in 2009 for its synchronization service "Ubuntu One",&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/couchdb-dev/200910.mbox/%3C4AD53996.3090104@canonical.com%3E Email from Elliot Murphy (Canonical)] to the CouchDB-Devel list&lt;/ref&gt; but stopped using it in November 2011.&lt;ref&gt;[http://linux.slashdot.org/story/11/11/22/171228/canonical-drops-couchdb-from-ubuntu-one Canonical Drops CouchDB From Ubuntu One (Slashdot)]&lt;/ref&gt;
* [http://Airfi.aero AirFi] for their portable Inflight Entertainment platform.&lt;ref&gt;[http://airfi.aero/india-grails-engineer-team-lead/ AirFi has CouchDB as its job requirements]&lt;/ref&gt;
* [[Canal+|CANAL+]] for international on-demand platform at CANAL+ Overseas.
* Commusoft for managing offline mobile data
* [https://muzzley.com/ Muzzley] for their primary datastore

== Data manipulation: documents and views ==
CouchDB manages a collection of [[JSON]] documents. The documents are organised via views. Views are defined with [[aggregate function]]s and filters are computed in parallel, much like [[MapReduce]].

Views are generally stored in the database and their indexes updated continuously. CouchDB supports a view system using external socket servers and a JSON-based protocol.&lt;ref name="Apache, View Server" &gt;[http://wiki.apache.org/couchdb/ViewServer View Server Documentation] on wiki.apache.org&lt;/ref&gt; As a consequence, view servers have been developed in a variety of languages (JavaScript is the default, but there are also PHP, Ruby, Python and Erlang).

=== Accessing data via HTTP ===
Applications interact with CouchDB via HTTP. The following demonstrates a few examples using [[cURL]], a command-line utility. These examples assume that CouchDB is running on [[localhost]] (127.0.0.1) on port 5984.

{| class="wikitable"
|-
! Action !! Request !! Response
|-
| Accessing server information
|| &lt;syntaxhighlight lang="bash"&gt;curl http://127.0.0.1:5984/&lt;/syntaxhighlight&gt;
|| &lt;syntaxhighlight lang="javascript"&gt;{
  "couchdb": "Welcome",
  "version":"1.1.0"
}&lt;/syntaxhighlight&gt;
|-
| Creating a database named '''wiki'''
|| &lt;syntaxhighlight lang="bash"&gt;curl -X PUT http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt;
|| &lt;syntaxhighlight lang="javascript"&gt;{"ok": true}&lt;/syntaxhighlight&gt;
|-
| Attempting to create a second database named '''wiki'''
|| &lt;syntaxhighlight lang="bash"&gt;curl -X PUT http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt;
|| &lt;syntaxhighlight lang="javascript" enclose="div"&gt;{
  "error":"file_exists",
  "reason":"The database could not be created, the file already exists."
}&lt;/syntaxhighlight&gt;
|-
|| Retrieve information about the '''wiki''' database
|| &lt;syntaxhighlight lang="bash"&gt;curl http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt;
|| &lt;syntaxhighlight lang="javascript"&gt;{
  "db_name": "wiki",
  "doc_count": 0,
  "doc_del_count": 0,
  "update_seq": 0,
  "purge_seq": 0,
  "compact_running": false,
  "disk_size": 79,
  "instance_start_time": "1272453873691070",
  "disk_format_version": 5
}&lt;/syntaxhighlight&gt;
|-
| Delete the database '''wiki'''
| &lt;syntaxhighlight lang="bash"&gt;curl -X DELETE http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt;
| &lt;syntaxhighlight lang="javascript"&gt;{"ok": true}&lt;/syntaxhighlight&gt;
|-
| Create a document, asking CouchDB to supply a document id
| &lt;syntaxhighlight lang="bash"&gt;curl -X POST -H "Content-Type: application/json" --data \
'{ "text" : "Wikipedia on CouchDB", "rating": 5 }' \
http://127.0.0.1:5984/wiki
&lt;/syntaxhighlight&gt;
| &lt;syntaxhighlight lang="javascript"&gt;{
  "ok": true,
  "id": "123BAC",
  "rev": "946B7D1C"
}&lt;/syntaxhighlight&gt;
|}

==Open source components==
CouchDB includes a number of other open source projects as part of its default package.
{| class="wikitable sortable" width = "100%"
! Component
! Description
! License
|-
| [[Erlang (programming language)|Erlang]]
| Erlang is a general-purpose [[concurrent computing|concurrent]] [[programming language]] and [[Run time system|runtime]] system. The sequential subset of Erlang is a [[functional language]] with [[strict evaluation]], [[single assignment]], and [[dynamic typing]]
| [[Apache License|Apache 2.0]] (Release 18.0 and later)&lt;br /&gt;[[Erlang Public License]] (Earlier releases)
|-
| [[International Components for Unicode|ICU]]
| International Components for Unicode (ICU) is an [[open source]] project of mature [[C (programming language)|C]]/[[C++]] and [[Java (programming language)|Java]] libraries for [[Unicode]] support, software [[internationalization]] and software globalization
| [[Unicode License]]
|-
| [[jQuery]]
| jQuery is a lightweight [[cross-browser]] [[JavaScript library]] that emphasizes interaction between [[JavaScript]] and [[HTML]]
| [[MIT License]]
|-
| [[OpenSSL]]
| OpenSSL is an [[open source]] implementation of the [[Transport Layer Security|SSL and TLS]] protocols. The core [[library (computer science)|library]] (written in the [[C (programming language)|C programming language]]) implements the basic [[cryptography|cryptographic]] functions and provides various utility functions
| [[Apache License|Apache 1.0]] and the [[BSD License#4-clause license (original "BSD License")|four-clause BSD License]]
|-
| [[SpiderMonkey (JavaScript engine)|SpiderMonkey]]
| SpiderMonkey is a performant [[JavaScript engine]] maintained by the [[Mozilla Foundation]]. It contains an [[Interpreted language|interpreter]], a [[JIT compiler]] and a [[Garbage collection (computer science)|garbage collector]]
| [[Mozilla Public License|MPL 2.0]]
|}

== See also ==
{{Portal|Free software}}
&lt;!--{{too many see alsos|date=April 2015}} --&gt;
&lt;!--* [[Apache Accumulo|Accumulo]] --&gt;
* [[Apache Cassandra]]
* [[BigCouch]]
* [[Cloudant]]
* [[Couchbase Server]]
* [[Document-oriented database]]
* [[LYCE (software bundle)]]
&lt;!--* [[Lotus Notes]]
* [[MongoDB]]--&gt;
* [[Mnesia]]
&lt;!--* [[Redis]]--&gt;
* [[Oracle NoSQL Database]]
* [[OrientDB]]
&lt;!--* [[Riak]]--&gt;
* [[XML database]]

==References==
{{Reflist|3}}

==Bibliography==
{{Refbegin}}
* {{citation
| first1 = J. Chris | last1 = Anderson | first2 = Noah | last2 = Slater | first3 = Jan | last3 = Lehnardt | date = November 15, 2009 | title = CouchDB: The Definitive Guide | edition = 1st | publisher = [[O'Reilly Media]] | pages = 300 | isbn = 0-596-15816-5 | url = http://guide.couchdb.org/editions/1/en/index.html }}
* {{citation | first1 = Joe | last1 = Lennon | date = December 15, 2009 | title = Beginning CouchDB | edition = 1st | publisher = [[Apress]] | pages = 300 | isbn = 1-4302-7237-6 | url = http://www.apress.com/book/view/9781430272373 }}
* {{citation | first1 = Bradley | last1 = Holt | date = March 7, 2011 | title = Writing and Querying MapReduce Views in CouchDB | edition = 1st | publisher = [[O'Reilly Media]] | pages = 76 | isbn = 1-4493-0312-9| url = http://oreilly.com/catalog/0636920018247 }}
* {{citation | first1 = Bradley | last1 = Holt | date = April 11, 2011 | title = Scaling CouchDB | edition = 1st | publisher = [[O'Reilly Media]] | pages = 72 | isbn = 1-4493-0343-9 | url = http://oreilly.com/catalog/9781449303433}}
* {{citation | first1 = MC | last1 = Brown | date = October 31, 2011 | title = Getting Started with CouchDB | edition = 1st | publisher = [[O'Reilly Media]] | pages = 50 | isbn = 1-4493-0755-8 | url = http://oreilly.com/catalog/9781449307554}}
* {{citation | first1 = Mick | last1 = Thompson | date = August 2, 2011 | title = Getting Started with GEO, CouchDB, and Node.js | edition = 1st | publisher = [[O'Reilly Media]] | pages = 64 | isbn = 1-4493-0752-3 | url = http://oreilly.com/catalog/9781449307523}}
{{Refend}}

==External links==
* {{Official website|//couchdb.apache.org}}
* [http://books.couchdb.org/relax/ CouchDB: The Definitive Guide]
* [http://wiki.apache.org/couchdb/Complete_HTTP_API_Reference Complete HTTP API Reference]
* [https://github.com/1999/couchdb-php Simple PHP5 library to communicate with CouchDB]
* [https://code.google.com/p/async-couchdb-client/ Asynchronous CouchDB client for Java]
* [https://github.com/KimStebel/sprouch Asynchronous CouchDB client for Scala]
* {{cite web|last=Lehnardt|first=Jan|title=Couch DB at 10,000 feet|url=http://video.google.com/videoplay?docid=-3714560380544574985&amp;hl=en#|work=Erlang eXchange 2008|accessdate=15 April 2012|year=2008}}
* {{cite web|last=Lenhardt|first=Jan|title=CouchDB for Erlang Developers|url=http://www.erlang-factory.com/conference/London2009/speakers/janlehnardt|work=Erlang Factory London 2009|accessdate=15 April 2012|year=2009}}
* {{cite web|last=Katz|first=Damien|title=CouchDB and Me|url=http://www.infoq.com/presentations/katz-couchdb-and-me|work=RubyFringe|publisher=InfoQ|accessdate=15 April 2012|date=January 2009}}

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Client-server database management systems]]
[[Category:Cross-platform software]]
[[Category:Database-related software for Linux]]
[[Category:Distributed computing architecture]]
[[Category:Document-oriented databases]]
[[Category:Erlang (programming language)]]
[[Category:Free database management systems]]
[[Category:NoSQL]]
[[Category:Structured storage]]
[[Category:Unix network-related software]]
[[Category:Free software programmed in Erlang]]</text>
      <sha1>8698v8rkgkt0lga3nu8ufxdor1da5ol</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Stanbol</title>
    <ns>0</ns>
    <id>38167240</id>
    <revision>
      <id>825795348</id>
      <parentid>807670578</parentid>
      <timestamp>2018-02-15T13:29:11Z</timestamp>
      <contributor>
        <username>Tom Morris</username>
        <id>417165</id>
      </contributor>
      <minor/>
      <comment>removed [[Category:Java (programming language)]] using [[WP:HC|HotCat]] - already in a relevant subcat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12969">{{multiple issues|
{{primary sources|date=August 2013}}
{{notability|Products|date=August 2013}}
}}

{{Infobox software
| name                   = Apache Stanbol
| logo                   = Apache Stanbol - logo.png
| developer              = [[Apache Software Foundation]]
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://stanbol.apache.org/}}
}}
'''Apache Stanbol''' is an open source modular software stack and reusable set of components for semantic content management.
Apache Stanbol components are meant to be accessed over [[Representational state transfer|RESTful]] interfaces to provide semantic services for content management. Thus, one application is to extend traditional content management systems with (internal or external) semantic services.&lt;ref&gt;Apache Stanbol&lt;/ref&gt;

Additionally, Apache Stanbol lets you create new types of content management systems with semantics at their core. The current code is written in [[Java (programming language)|Java]] and based on the [[OSGi]] component framework.
Applications include extending existing content management systems with (internal or external) semantic services, and creating new types of content management systems with semantics at their core.

==History==

In 2008 the [[Salzburg Research]] led, as entity coordinator, a consortium of seven research partners and six industrial partners to the proposal of the IKS project with the aim of receiving funding by the European institutions under the [[Seventh Framework Programme|7th Framework Programme]].&lt;ref&gt;{{cite web|url=http://wiki.iks-project.eu/index.php/FAQ|publisher=IKS|title=IKS FAQ|accessdate=2013-08-20}}&lt;/ref&gt;

The consortium comprised:&lt;ref&gt;{{cite web|url=http://wiki.iks-project.eu/index.php/FAQ|publisher=IKS|title=IKS FAQ|accessdate=2013-08-20}}&lt;/ref&gt;
* [[Salzburg Research]] (Coordinator), Austria
* [[DFKI]] - Forschungsinstitut für Künstliche Intelligenz, Germany
* [[Hochschule St. Gallen]], Switzerland
* [[Consiglio Nazionale delle Ricerche|CNR-ISTC]] - Consiglio Nazionale delle Ricerche, Italy
* Software Quality Lab, [[University of Paderborn]], Germany
* SRDC - Software Research and Development and Consultancy Ltd., Turkey
* [[Furtwangen University of Applied Sciences|Hochschule Furtwangen]], Germany
* [[Nuxeo]], France
* Alkacon Software GmbH, Germany
* TXT Polymedia, Italy
* Pisano Holding GmbH, Germany
* Nemein Oy, Finland
* [[Day Software|Day Software AG]], Switzerland

In January 2009, the [http://www.iks-project.eu/ Interactive Knowledge Stack] (IKS) started partly funded by the [[European Commission]] to provide an "open source technology platform for semantically enhanced content management systems".&lt;ref&gt;{{cite web|url=http://wiki.iks-project.eu/index.php/Main_Page|title=IKS Project|publisher=IKS|accessdate=2013-08-20}}&lt;/ref&gt; IKS received €6.58m co-funding by the [[European Union]]&lt;ref&gt;{{cite web|url=http://www.iks-project.eu/about-us|publisher=IKS Project|title=About Us|accessdate=2013-08-20}}&lt;/ref&gt; for an overall project duration of 4 years, hence setting the project's end date by the end of 2012.&lt;ref&gt;{{cite web|url=http://www.iks-project.eu/community/partners/iks-eu-research-project|publisher=IKS Project|title=IKS EU Research Project|accessdate=2013-08-20}}&lt;/ref&gt;

Apache Stanbol was founded in November 2010 by members the EU research project [http://www.iks-project.eu/ Interactive Knowledge Stack] (IKS). It was the result of an ongoing discussion about how to ensure that the results, especially the developed software, of the IKS project would be available to vendors of content management systems (CMS) after the project’s official funding period ended in 2012. The members of the IKS project decided to initiate the Apache Stanbol project as part of the incubation program of the [[Apache Software Foundation]] (ASF).

One of the first code imports of Apache Stanbol was the so-called "Furtwangen IKS Semantic Engine" (FISE) which eventually became the Apache Stanbol Enhancer with its Enhancement Engines. Other contributions of code were the KReS (Knowledge Representation and Reasoning) and the RICK (Reference Infrastructure for Content and Knowledge) components. Later on followed the Contenthub, while KReS was split into the Apache Stanbol Ontology Manager and Reasoner components, and the RICK is today known as the Apache Stanbol Entityhub. From that moment Apache Stanbol was developed as an open source software project independent of the IKS research project.

On 15 November 2010 Apache Stanbol enters incubation.&lt;ref&gt;{{cite mailinglist|url=http://mail-archives.apache.org/mod_mbox/incubator-general/201011.mbox/%3CAANLkTimZbpCR2RLbJVzdGkBCV-iE1YhsfC4iPO400WAT@mail.gmail.com%3E|title=Accept Stanbol for incubation|last=Delacretaz|first= Bertrand|mailinglist= incubator-general |date=15 November 2010|accessdate=2013-08-20}}&lt;/ref&gt;

On 9 May 2012 version 0.9.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=http://markmail.org/message/grzlcpzmvv5wmeht|title=Apache Stanbol 0.9.0-incubating staging|last=Christ|first= Fabian|mailinglist=org.apache.incubator.stanbol-dev|date=9 May 2012|accessdate=2013-08-20}}&lt;/ref&gt;

On 10 July 2012 version 0.10.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=http://markmail.org/message/myrdbfszfotne6s5|title=Apache Stanbol Entityhub 0.10.0-incubating released|last=Christ|first= Fabian|mailinglist=org.apache.incubator.stanbol-dev|date=10 July 2012|accessdate=2013-08-20}}&lt;/ref&gt;

By the middle of 2012 Apache Stanbol had demonstrated that it has an active community and is able to produce software and releases according to the ASF standards. The board of directors of the ASF accepted the [http://www.apache.org/foundation/records/minutes/2012/board_minutes_2012_09_19.txt formal resolution] to establish Apache Stanbol as a top-level project on 2012-09-19.&lt;ref&gt;{{cite web|url=http://stanbol.apache.org/graduation-resolution.html|title=Graduation Resolution|publisher=Apache Stanbol|accessdate=2013-08-20}}&lt;/ref&gt;

On 5 March 2013 [[Salzburg Research]] announced that 8 entities, among those [[Sebastian Schaffert]] (head of the knowledge and media technologies group&lt;ref&gt;[http://www.salzburgresearch.at/person/schaffert-sebastian/ Dr. Sebastian Schaffert at Salzburg Research] {{webarchive |url=https://web.archive.org/web/20131220114714/http://www.salzburgresearch.at/person/schaffert-sebastian/ |date=December 20, 2013 }}&lt;/ref&gt;), [[Rupert Westenthaler]] (Stanbol initial committer and PMC) and Sergio Fernández (Stanbol commiter) set up an effort to deliver Apache Stanbol and [[Apache Marmotta]] services under the [[RedLink GmbH|Redlink]] brand.&lt;ref&gt;{{cite web|url=http://redlink.co/eight-players-joined-hands-to-commence-redlink-services-in-2013/|title= Eight players joined hands to commence services in April|publisher=Redlink|first=Andrea|last=Volpini|accessdate=2013-08-20}}&lt;/ref&gt;

==Main features==
Apache Stanbol's main features are:

===Content Enhancement===
Services that add semantic information to “non-semantic” pieces of content.
The Apache Stanbol Enhancer provides both a RESTful and a Java API that allows a caller to extract features from passed content. In more detail the passed content is processed by Enhancement Engines as defined by the called Enhancement Chain.

*Using the Stanbol Enhancer
**[[RESTful]]
**Java [[API]]
**Main Interfaces and Utility Classes
**Enhancement Structure
*List of Available Enhancement Engines

===Reasoning===
Services that are able to retrieve additional semantic information about the content based on the semantic information retrieved via content enhancement.
The Stanbol Reasoners component provides a set of services that take advantage of automatic inference engines.

The module implements a common api for reasoning services, providing the possibility to plug different reasoners and configurations in parallel.

Actually the module includes [[Web Ontology Language| OWLApi]] and [[Jena (framework)| Jena]] based abstract services, with concrete implementations for [[Jena RDFS]], [[Web Ontology Language|OWL]], [[OWLMini]] and [[HermiT]] reasoning service.

===Knowledge Models===
Services that are used to define and manipulate the [[Data model| data models]] (e.g. ontologies) that are used to store the semantic information.
The Apache Stanbol Ontology Manager provides a controlled environment for managing [[ontologies]], ontology networks and user sessions for semantic data modeled after them. It provides full access to ontologies stored into the Stanbol persistence layer. Managing an ontology network means that you can activate or deactivate parts of a complex model from time to time, so that your data can be viewed and classified under different "logical lenses". This is especially useful in Reasoning operations.

===Persistence===
Services that store (or cache) semantic information, i.e. enhanced content, entities, facts, and make it searchable.
The Apache Stanbol Contenthub is an [[Apache Solr]] based document repository which enables storage of text-based documents and customizable semantic search facilities. The Contenthub exposes an efficient [[Java API]] together with the corresponding [[RESTful]] services.

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    =  Reto
| last1     = Bachmann-Gmur
| date      = July 26, 2013
| title     = Instant Apache Stanbol
| edition   = 1st
| publisher = [[Packt Publishing]]
| isbn      = 1783281235
| url       = http://www.packtpub.com/apache-stanbol/book
}}
*{{Cite book
| editors = Gurevych, Iryna; Kim, Jungi
| year = 2013
| title     = The People’s Web Meets NLP
| edition   = 1st
| publisher = [[Springer Science+Business Media|Springer]]
| isbn      = 978-3-642-35085-6
| url       = https://www.springer.com/education+%26+language/linguistics/book/978-3-642-35084-9
}}
*{{Cite book
| title     = Media Networks: Architectures, Applications, and Standards
| editors =  Hassnaa, Moustafa; Sherali, Zeadally
| publisher = [[CRC Press]]
| date      = May 14, 2012
| isbn      = 1439877289
| url       = http://www.crcpress.com/product/isbn/9781439877289
}}
*{{Cite book
| title     = Semantic Technologies in Content Management Systems
| editors =  Maass, Wolfgang; Kowatsch, Tobias
| publisher = [[Springer Science+Business Media|Springer]]
| year      = 2012
| isbn      = 978-3-642-24960-0
| url       = https://www.springer.com/business+%26+management/technology+management/book/978-3-642-21549-0
}}
*{{Cite book
| title     = Semantic Mashups
| editors =  Endres-Niggemeyer, Brigitte
| publisher = [[Springer Science+Business Media|Springer]]
| year      = 2013
| page     = 131
| isbn      = 978-3-642-36403-7
| url       = https://www.springer.com/computer/database+management+%26+information+retrieval/book/978-3-642-36402-0?cm_mmc=Google-_-Book+Search-_-Springer-_-0&amp;otherVersion=978-3-642-36403-7
}}
*{{Cite book
| title     = The Semantic Web -- ISWC 2011
| editors =  Aroyo, L.; Welty, C.; Alani, H.; Taylor, J.; Bernstein, A.; Kagal, L.; Noy, N.; Blomqvist, E.
| publisher = [[Springer Science+Business Media|Springer]]
| year      = 2011
| pages     = 191
| isbn      = 978-3-642-25093-4
| url       = https://www.springer.com/computer/communication+networks/book/978-3-642-25092-7?wt_mc=Google-_-Book%20Search-_-Springer-_-EN&amp;token=gbgen
}}
*{{Cite book
| title     = Nosotros, los constructores de la Web Semántica
| language = Spanish
| first1 = Luis
| last1 = Criado-Fernández
| editors = del Amor León-Fariña, María
| date      = 10 June 2013
| asin      = B00DC8IAJA
| asin-tld = es
| url       = https://www.amazon.es/Nosotros-los-constructores-Sem%C3%A1ntica-ebook/dp/B00DC8IAJA
}}
*{{Cite book
| author = Snml-Tng Salzburg Newmedialab
| date     = May 14, 2013
| title     = Zukunft Von Linked Media: Trends, Entwicklungen Und Visionen.
| edition   = 1st
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft m.b.H]]
| page     = 9
| isbn      = 3902448369
| url       = http://de.slideshare.net/snml/zukunft-von-linked-media-trends-entwicklungen-und-visionen
| language = German
}}
*{{Cite book
| editors = Behrendt, Wernher; Damjanovic, Violeta
|date=March 2013
| title     = Developing Semantic CMS Applications - The IKS Handbook
| edition   = 1st
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft m.b.H]]
| isbn      = 978-3-902448-35-4
| url       = http://www.iks-project.eu/sites/default/files/IKS%20Handbook%202013.pdf
}}
{{Refend}}

==References==
{{reflist}}

==External links==
* {{Official website|https://stanbol.apache.org/}}
* [http://wiki.apache.org/incubator/StanbolProposal Stanbol Incubation Proposal]
* [http://stanbol.apache.org/presentations/Stanbol_Overview_2012-04.pdf Presentation of Apache Stanbol]

{{Apache}}

[[Category:Apache Software Foundation|Stanbol]]
[[Category:Cross-platform software]]
[[Category:Free software programmed in Java (programming language)]]</text>
      <sha1>8rlwr4ir99n9jslnrgguhdhwfy2qtl0</sha1>
    </revision>
  </page>
  <page>
    <title>Jakarta Project</title>
    <ns>0</ns>
    <id>55182</id>
    <revision>
      <id>831207833</id>
      <parentid>807463178</parentid>
      <timestamp>2018-03-19T11:01:07Z</timestamp>
      <contributor>
        <username>Jan.derijke</username>
        <id>7707610</id>
      </contributor>
      <minor/>
      <comment>added reference to jakarta EE</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3119">{{One source|date=February 2012}}
The '''Jakarta Project''' created and maintained [[open source software]] for the [[Java platform]]. It operated as an umbrella project under the auspices of the [[Apache Software Foundation]], and all Jakarta products are released under the [[Apache License]]. As of December 21, 2011 the Jakarta project was retired because no subprojects were remaining. 

In 2018 Jakarta EE became the new name for the Java Enterprise Edition project at the Eclipse foundation &lt;ref&gt;{{cite web |url=https://www.theregister.co.uk/2018/03/04/java_ee_is_now_jakarta_ee/ |title= Java EE renamed '''Jakarta EE''' after Big Red brand spat |author=Richard Chirgwin |date=March 2018 |work=[[The Register]] |page=1 |accessdate=19 March 2018}}&lt;/ref&gt;.

==Subprojects==&lt;!-- This section is linked from [[Comparison of regular expression engines]] --&gt;
Major contributions by the Jakarta Project include tools, [[library (software)|libraries]] and [[Software framework|frameworks]] such as:

*[[Byte Code Engineering Library|BCEL]] - a Java byte code manipulation library
*[[Bean Scripting Framework|BSF]] - a scripting framework
*[[Jakarta Cactus|Cactus]] - a unit testing framework for server-side Java classes
*[[Apache JMeter]] - a load- and stress-testing tool.

The following projects were formerly part of Jakarta, but now form independent projects within the Apache Software Foundation:
*[[Apache Ant|Ant]] - a [[build tool]]
*[[Apache Commons|Commons]] - a collection of useful classes intended to complement Java's standard library.
*[[Apache HiveMind|HiveMind]] - a services and configuration [[microkernel]]
*[[Apache Maven|Maven]] - a project build and management tool 
*[[Apache POI|POI]] - a pure [[Java (programming language)|Java]] port of Microsoft's popular file formats.
*[[Apache Struts|Struts]] - a web application development framework
*[[Jakarta Slide|Slide]] - a content repository primarily using [[WebDAV]].
*[[Tapestry (programming)|Tapestry]] - A component object model based on JavaBeans properties and strong specifications
*[[Apache Tomcat|Tomcat]] - a [[JavaServer Pages|JSP]]/[[Servlet]] container
*[[Apache Turbine|Turbine]] - a rapid development web application framework
*[[Apache Velocity|Velocity]] - a [[Template (programming)|template]] engine

==Project name==
Jakarta is named after the conference room at [[Sun Microsystems]] where the majority of discussions leading to the project's creation took place.&lt;ref&gt;{{cite web |url=http://www.javaworld.com/javaworld/jw-06-1999/jw-06-sunapache_p.html |title=Sun and Apache team up to deliver servlet and JSP code |author=Jason Hunter |date=June 1999 |work=[[JavaWorld]] |publisher=[[International Data Group|IDG]] |page=1 |accessdate=22 February 2012}}&lt;/ref&gt; At the time, Sun's Java software division was headquartered in a Cupertino building where the conference room names were all coffee references.{{Verify credibility|date=September 2017}}

==References==
{{Reflist}}


==External links==
*[http://jakarta.apache.org/ The Jakarta home page]

{{apache}}

[[Category:Java platform]]
[[Category:Apache Software Foundation]]</text>
      <sha1>hj4pqctif1m5s06jwkzu165zq5s1ire</sha1>
    </revision>
  </page>
  <page>
    <title>Jcrom</title>
    <ns>0</ns>
    <id>39890780</id>
    <revision>
      <id>796944473</id>
      <parentid>781666290</parentid>
      <timestamp>2017-08-24T00:54:36Z</timestamp>
      <contributor>
        <username>Shawnpoelo</username>
        <id>31746327</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2557">{{Orphan|date=July 2013}}

{{ Infobox Software
| name                   = JCROM
| logo                   = Jcrom-logo.png
| screenshot             = 
| caption                = 
| developer              = Olafur Gauti Gudmundsson, Nicolas Dos Santos
| status                 = Active
| latest release version = 2.1.0
| latest release date    = {{Release date|2013|06|19}}
| operating system       = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Content repository]]
| license                = [[Apache License]] 2.0
| website                = {{URL|jcrom.googlecode.com}}
}}
'''JCROM''' is an acronym that stands for [[Java Content Repository]] (JCR) Object Mapper. It is a simple and lightweight annotation-based framework for mapping [[Plain Old Java Objects]] (POJOs) to/from nodes in a JCR. This is commonly called [[Object Content Mapping]].

JCR specifies an API for application developers (and application frameworks) to use for interaction with modern content repositories that provide content services such as searching, versioning, transactions, etc.

There are object mapping frameworks for JDBC, like Hibernate and the Enterprise JavaBeans spec. There are also solutions for mapping to/from XML. The vision of JCROM is to provide the same for JCR.

==Features==
* Annotation based (needs Java 1.5)
* Lightweight, minimal external dependencies
* Works with any JCR implementation (e.g. Apache Jackrabbit,&lt;ref&gt;[http://jackrabbit.apache.org/ Apache Jackrabbit home page]&lt;/ref&gt; ModeShape,&lt;ref&gt;[http://www.jboss.org/modeshape ModeShape open source project]&lt;/ref&gt; Adobe CQ,&lt;ref&gt;[http://dev.day.com/docs/en/cq/5-5.html Adobe CQ]&lt;/ref&gt; ...)
* DAO support
* Works with the Spring Framework&lt;ref&gt;[http://www.springsource.org/spring-framework Spring Framework home page]&lt;/ref&gt; and Spring Extension JCR&lt;ref&gt;[http://se-jcr.sourceforge.net/guide.html Spring Extension JCR open source project]&lt;/ref&gt;
* Works with Google Guice&lt;ref&gt;[[Google Guice framework home page]]&lt;/ref&gt;

==See also==
*[http://jackrabbit.apache.org/object-content-mapping.html Jackrabbit OCM] - a framework used to persist java objects (pojos) in a JCR repository

==References==
{{reflist}}

==External links==
*[http://www.jcp.org/en/jsr/detail?id=170 JSR-170: Content Repository for Java(TM) Technology API]
*[http://www.jcp.org/en/jsr/detail?id=283 JSR-283: Content Repository for Java(TM) Technology API, version 2.0]

[[Category:Apache Software Foundation|Jackrabbit]]
[[Category:Java enterprise platform]]
[[Category:Structured storage]]</text>
      <sha1>3qc3pke187y501s21p95vsfdca0s1wn</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Marmotta</title>
    <ns>0</ns>
    <id>41457077</id>
    <revision>
      <id>832696425</id>
      <parentid>825795266</parentid>
      <timestamp>2018-03-27T14:05:21Z</timestamp>
      <contributor>
        <username>Devbug</username>
        <id>19355811</id>
      </contributor>
      <minor/>
      <comment>/* Notable users */ add WordLift among notable users; remove dead link to http://search.salzburg.com</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13171">{{multiple|
{{COI|date=December 2015}}
{{primary sources|date=December 2015}}
{{notability|Products|date=August 2013}}
}}
{{Infobox software
| name                   = Apache Marmotta
| logo                   = Apache Marmotta logo.jpg
| developer              = [[Apache Software Foundation]]
| released               = {{Start date and age|2013|10|03}}
| latest release version = 3.3.0&lt;!-- When updating, please update the "History" section as well. --&gt;
| latest release date    = {{Start date and age|2014|12|05}}
| programming language   = [[Java (programming language)|Java]]
| language               = English
| genre                  = [[Triplestore]], [[Semantic reasoner|rule reasoner]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//marmotta.apache.org}}
}}

'''Apache Marmotta''' is a [[linked data]] platform that comprises several [[#Components|components]]. In its most basic configuration it is a [[Linked Data]] server.&lt;ref&gt;{{cite web|url=http://marmotta.apache.org|publisher=Apache Marmotta|title=Apache Marmotta|accessdate=2013-08-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=GmbH|first1=RedLink|title=Apache Marmotta™ (top-level project) is an Open Platform for Linked Data.|url=http://redlink.co/portfolio/apache-marmotta/|website=redlink|accessdate=1 August 2014}}&lt;/ref&gt; Marmotta is one of the reference projects early implementing the new [[Linked Data Platform]] &lt;ref&gt;{{cite book|author1=Steve Speicher |author2=John Arwe |author3=Ashok Malhotra | date = July 30, 2013 | title = Linked Data Platform 1.0 | edition = Last Call Working Draft | publisher = W3C | url = http://www.w3.org/TR/2013/WD-ldp-20130730/ | accessdate=2013-11-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.w3.org/wiki/LDP_Implementations#Apache_Marmotta_.28Client_and_Server.29|publisher=W3C LDP Working Group|title=LDP Implementations|accessdate=2013-11-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=//lists.w3.org/Archives/Public/public-ldp-comments/2014Sep/0004.html|title=LDP Implementation Report for Apache Marmotta|accessdate=2014-09-21}}&lt;/ref&gt; recommendation that is being developed by [[W3C]].

It has been contributed by [[Salzburg Research]] from the Linked Media Framework,&lt;ref&gt;[http://lmf.googlecode.com Linked Media Framework]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://wiki.apache.org/incubator/MarmottaProposal#Background|publisher=Apache|title= Apache Marmotta incubation proposal |accessdate=2013-08-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://marmotta.apache.org/acknowledgements.html|publisher=Apache Marmotta|title=Apache Marmotta - Acknowledgements|accessdate=2013-08-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://redlink.co/apache-marmotta/|publisher=Redlink GmbH|title=Apache Marmotta is now a Top Level Apache Project|accessdate=2013-11-25}}&lt;/ref&gt; and continues its versioning, hence starting at version 3.0.0.

Since April 2013, it is listed among the [[Semantic Web]] tools by the [[World Wide Web Consortium|W3C]].&lt;ref&gt;[https://www.w3.org/2001/sw/wiki/Marmotta W3C - Semantic Web - Apache Marmotta]&lt;/ref&gt;

In December 2013, it has been nominated as "one of the [[Apache Software Foundation|ASF]]'s most active projects".&lt;ref&gt;{{cite web|url=http://globenewswire.com/news-release/2013/12/10/595835/10060945/en/The-ASF-asks-Have-you-met-Apache-tm-Marmotta-tm.html|publisher=The Apache Software Foundation |title=The ASF asks: Have you met Apache(tm) Marmotta(tm)?|accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_asf_asks_have_you2|publisher=The Apache Software Foundation |title=The ASF asks: Have you met Apache(tm) Marmotta(tm)?|accessdate=2013-12-25}}&lt;/ref&gt;

== Components ==

The project is split in several parts: the platform itself, which includes full Read Write [[Linked Data]], [[SPARQL]], [[Linked Data Platform|LDP]], Reasoning, and basic security. In addition to the platform, the project develops some libraries can also be used separately:

* KiWi, a [[Triplestore]] built on top of a [[relational database]].
* LDPath, a path language to navigate across [[Linked Data]] resources.  
* LDClient, a Linked Data client that allows retrieval of remote resources via different protocols by making use of pluggable adapters (data providers) that wrap other data sources (such as [[YouTube]] and [[Facebook]]).&lt;ref&gt;{{cite web|url=http://www.w3.org/wiki/ConverterToRdf#Apache_Marmotta_LDClient|publisher=W3C|title=Converter to RDF|accessdate=2013-08-20}}&lt;/ref&gt;
* LDCache, a [[Web cache|cache]] system that automatically retrieves resources by internally using LDClient.

== History ==

=== Linked Media Framework (''pre-Apache'') ===

Apache Marmotta is the continuation of the open source Linked Media Framework published in early 2012.&lt;ref&gt;{{cite web|url=https://www.w3.org/2001/sw/wiki/Marmotta|publisher=W3C|title= W3C - Semantic Web - Apache Marmotta|accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite mailinglist|title=[ANNOUNCE] Apache Marmotta 3.0.0-incubating released!|first=Sebastian|last=Schaffert|url=http://mail-archives.us.apache.org/mod_mbox/www-announce/201304.mbox/%3CB157D25D-A49B-46CA-A4F0-71C6566CE924@apache.org%3E|mailinglist=announce@apache.org|date=26 April 2013|accessdate=2013-12-25}}&lt;/ref&gt;

=== Apache Marmotta ===

* On November 16, 2012 it is proposed to the [[Apache Software Foundation]] under the name of Apache Linda, later changed to Apache Marmotta in order to avoid confusion with the [[Linda (coordination language)|Linda language]].&lt;ref&gt;{{cite mailinglist|url=http://markmail.org/thread/qesb3rbb2tyt3eti|title= Apache Marmotta (was: Apache Linda)|mailinglist= org.apache.incubator.general|date=16 November 2012|accessdate=2013-12-25}}&lt;/ref&gt;
* On 3 December 2012 Marmotta enters incubation.&lt;ref&gt;{{cite web|url=http://incubator.apache.org/projects/marmotta.html|publisher=Apache|title= Marmotta Project Incubation Status page|accessdate=2013-08-20}}&lt;/ref&gt;
* On April 26, 2013 Marmotta 3.0.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=//lists.w3.org/Archives/Public/semantic-web/2013Apr/0209.html|title= Apache Marmotta 3.0.0-incubating released!|last=Schaffert|first=Sebastian|authorlink=Sebastian Schaffert|mailinglist= semantic-web AT w3.org|date= 26 April 2013|accessdate=2013-11-21}}&lt;/ref&gt;
* On October 3, 2013 Marmotta 3.1.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=//lists.w3.org/Archives/Public/semantic-web/2013Oct/0040.html|title= Apache Marmotta 3.1.0-incubating released!|last=Schaffert|first=Sebastian|authorlink=Sebastian Schaffert|mailinglist= semantic-web AT w3.org|date= 3 October 2013|accessdate=2013-11-21}}&lt;/ref&gt;
* On November 2013, it graduated as [[Apache Software Foundation#Projects|top-level project]].&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/incubator-marmotta-dev/201310.mbox/%3C1382947773.2712.12.camel%40kis07%3E|last=Frank|first=Jakob|authorlink=Jakob Frank|title=org.apache.incubator.general |date=28 October 2013 |accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.salzburgresearch.at/en/2013/apache-marmotta-graduated-top-level-project/|publisher=Salzburg Research|title= Apache Marmotta graduated to top-level project |accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://markmail.org/message/tc5vrsmady3mdo42 |publisher=ASF |title=Resolved to establish the Apache Marmotta Project |deadurl=yes |archiveurl=https://web.archive.org/web/20131202222426/http://markmail.org/message/tc5vrsmady3mdo42 |archivedate=2013-12-02 }}&lt;/ref&gt;
* On April 2014, the project released its first actual release under the umbrella of the Apache Software Foundation: 3.2.0 version.&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/marmotta-dev/201404.mbox/%3C1397731707.32602.47.camel%40T440p%3E|last=Frank|first=Jakob|authorlink=Jakob Frank|title=[RESULT][VOTE] Release Apache Marmotta 3.2.0 |date=9 April 2014 |accessdate=2014-09-21}}&lt;/ref&gt;
* On 5 December 2014, the project published the version 3.3.0.&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/marmotta-dev/201412.mbox/%3C54929F77.2070209%40apache.org%3E|last=Frank|first=Jakob|authorlink=Jakob Frank|title=[ANNOUNCE] Apache Marmotta 3.3.0 released|date=18 December  2014 |accessdate=2015-01-05}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|title = Apache Marmotta -   Download|url = https://marmotta.apache.org/download.html#a3.3.0|website = marmotta.apache.org|access-date = 2016-01-28}}&lt;/ref&gt;

== Notable users ==

* The backend of [[Salzburger Nachrichten]]'s search and archive is powered by Marmotta.&lt;ref&gt;{{cite book| author = Georg Güntner, [[Sebastian Schaffert]]| date = June 30, 2013 | title = A Marmot against the Flood of Data | publisher = Salzburg NewMediaLab | url = http://www.newmedialab.at/wp-content/uploads/SNML-TNG_SuccessStory_Marmotta_EN.pdf | accessdate=2013-11-24}}&lt;/ref&gt;
* [[Enel]] uses Marmotta for its Open Data portal.&lt;ref&gt;[http://data.enel.com Open Data portal]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://cirullo.it/2013/07/liberate-dataset-fare-business-servono-linked-data/?lang=en|title=Free dataset! Linked data are necessary to do business|first=Raffaele|last=Cirullo|accessdate=2013-08-20}}&lt;/ref&gt;
* The [[Cloud computing|cloud]] infrastructure of Redlink&lt;ref&gt;[http://redlink.co Redlink]&lt;/ref&gt; is powered by Marmotta.{{cn|date=December 2015}}
* It is being used by some [[Framework Programmes for Research and Technological Development#Framework Programme 7|European research projects]] such as Fusepool and MICO (''Media in Context'').&lt;ref&gt;[http://challenge.semanticweb.org/2013/submissions/swc2013_submission_6.pdf Fusepool Linked Datapool for Technology Intelligence]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.mico-project.eu/mico-kick-off-meeting-salzburg/|title=MICO Kick-off Meeting in Salzburg|accessdate=2013-12-25}}&lt;/ref&gt;
* [[Digital Public Library of America]] uses Marmotta to run [[Linked Data Platform|LDP]] server and [[Resource Description Framework|RDF]] repository.&lt;ref&gt;{{cite web|url=https://digitalpubliclibraryofamerica.atlassian.net/wiki/display/TECH/Marmotta|title=DPLA wiki|accessdate=2015-09-22}}&lt;/ref&gt;
* [[WordLift]] uses Marmotta as Linked Data backend.&lt;ref&gt;{{cite web|url=https://docs.wordlift.io/en/latest/faq.html|title=Frequently Asked Questions|accessdate=2018-03-27}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://ceur-ws.org/Vol-1361/paper4.pdf|title=WordLift: Meaningful Navigation Systems and Content Recommendation for News Sites running WordPress |accessdate=2018-03-27}}&lt;/ref&gt;

== Bibliography ==

{{Refbegin}}
*{{Cite book
| first1    =  Reto
| last1     = Bachmann-Gmur
| date      = July 26, 2013
| title     = Instant Apache Stanbol
| edition   = 1st
| publisher = [[Packt Publishing]]
| isbn      = 1783281235
| url       = http://www.packtpub.com/apache-stanbol/book
}}
*{{Cite book
| first1 = Michael
| last1 = Kaschesky
| first2 = Luigi
| last2 = Selmi
| date = June 2013
| title     = Fusepool R5 linked data framework: concepts, methodologies, and tools for linked data
| publisher = ACM New York
| pages     = 156–165
| isbn      = 978-1-4503-2057-3
| url       = http://dl.acm.org/citation.cfm?id=2479748
}}
*{{Cite book
| date     = 14 May 2013
| title     = Zukunft Von Linked Media: Trends, Entwicklungen Und Visionen.
|trans-title=The future of Linked Media: trends, discoveries and visions.
| language = German
| edition   = 1st
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn      = 3902448369
| url       = http://www.slideshare.net/snml/zukunft-von-linked-media-trends-entwicklungen-und-visionen
}}
*{{Cite book
| date     = 2 January 2013
| title     = Qualitätssicherung Bei Annotationen
|trans-title=Quality assurance in annotations
| language = German
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn      = 3902448326
}}
* {{Cite book
| editor1-last = Hassnaa
| editor1-first = Moustafa
| editor2-last = Zeadally
| editor2-first = Sherali
| title = Media Networks: Architectures, Applications, and Standards
| publisher = CRC Press
| isbn = 9781439877289
| date = 14 May 2012
| quote = an ongoing open source development used to demonstrate the concepts of semantic lifting and interlinking of media resources 
}}
* {{Cite book
| title = Linked Media Interfaces
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn = 3902448296
| date = 5 October 2011
}}
* {{Cite book
| title = Linked Media. Ein White-Paper zu den Potentialen von Linked People, Linked Content und Linked Data.
|trans-title=A white paper on the potential of Linked People, Linked Content and Linked Data.
| language = German
| page = 8
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn = 390244827X
| date = 26 April 2011
}}
{{Refend}}

== References ==
{{reflist|30em}}

== External links ==
* {{Official website|//marmotta.apache.org}}
* [https://wiki.apache.org/incubator/MarmottaProposal Marmotta Incubation Proposal]
* [https://www.slideshare.net/Wikier/apache-marmotta Apache Marmotta presentation]
* [https://www.w3.org/2001/sw/wiki/Marmotta Apache Marmotta on the W3C]

{{Apache}}

[[Category:Apache Software Foundation|Marmotta]]
[[Category:Cross-platform software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Semantic Web]]
[[Category:Triplestores]]</text>
      <sha1>i6ifvio09q55ckx28sr3p5sk92a5xxx</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Giraph</title>
    <ns>0</ns>
    <id>37752641</id>
    <revision>
      <id>755665147</id>
      <parentid>746354198</parentid>
      <timestamp>2016-12-19T13:09:16Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor/>
      <comment>link [[big data]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2368">{{ Infobox Software
| name                   = Apache Giraph
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Active
| latest release version = 1.2.0
| latest release date    = {{release date|2016|10|20|df=yes}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Graph (computer science)|graph processing]]
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = {{url|http://giraph.apache.org}}
}}
'''Apache Giraph''' is an [[Apache Software Foundation|Apache]] project to perform [[Graph (computer science)|graph processing]] on [[big data]]. Giraph utilizes [[Apache Hadoop]]'s MapReduce implementation to process graphs. [[Facebook]] used Giraph with some performance improvements to analyze one trillion edges using 200 machines in 4 minutes.&lt;ref&gt;{{cite web|last=Ching|first=Avery|title=Scaling Apache Giraph to a trillion edges|url=http://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920|publisher=Facebook|accessdate=8 February 2014|date=August 14, 2013}}&lt;/ref&gt; Giraph is based on a paper published by Google about its own graph processing system called Pregel.&lt;ref&gt;{{cite news|last=Jackson|first=Joab|title=Facebook's Graph Search puts Apache Giraph on the map|url=http://www.pcworld.com/article/2046680/facebooks-graph-search-puts-apache-giraph-on-the-map.html|accessdate=8 February 2014|newspaper=[[PC World]]|date=Aug 14, 2013}}&lt;/ref&gt; It can be compared to other Big Graph processing libraries such as Cassovary.&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Facebook’s trillion-edge, Hadoop-based and open source graph-processing engine|url=http://gigaom.com/2013/08/14/facebooks-trillion-edge-hadoop-based-graph-processing-engine/|publisher=[[Gigaom]]|accessdate=8 February 2014|date=Aug 14, 2013}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* {{Official website|http://giraph.apache.org/}}

{{Apache}}

[[Category:Apache Software Foundation|Giraph]]
[[Category:Hadoop]]
[[Category:Data mining and machine learning software]]</text>
      <sha1>hdkdscha1d8qxzce8m3eggzydbjzwio</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Hadoop cluster administration</title>
    <ns>14</ns>
    <id>42741341</id>
    <revision>
      <id>791537966</id>
      <parentid>610238955</parentid>
      <timestamp>2017-07-20T22:21:14Z</timestamp>
      <contributor>
        <username>NaBUru38</username>
        <id>1144776</id>
      </contributor>
      <comment>removed [[Category:Software]]; added [[Category:Data management software]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="171">This category involves the software which supports cluster administration for Apache Hadoop.

[[Category:Apache Software Foundation]]
[[Category:Data management software]]</text>
      <sha1>nbrypwuuran67va4vymwkgp0v5zetvu</sha1>
    </revision>
  </page>
  <page>
    <title>Apache PDFBox</title>
    <ns>0</ns>
    <id>43118322</id>
    <revision>
      <id>839612005</id>
      <parentid>818490736</parentid>
      <timestamp>2018-05-04T15:08:31Z</timestamp>
      <contributor>
        <ip>95.90.228.20</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3408">{{Multiple issues|
{{notability|Products|date=June 2014}}
{{third-party|date=June 2014}}
}}

{{Infobox software
| name                   = PDFBox
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = 2.0.9
| latest release date    = {{release date and age|2018|03|23}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| language               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[PDF|Portable Document Format (PDF)]]
| license                = [[Apache License]] 2.0
| website                = https://pdfbox.apache.org
}}

'''Apache PDFBox''' is an open source pure-[[Java (software platform)|Java]] library that can be used to create, render, print, split, merge, alter, verify and extract text and meta-data of [[PDF]] files.

[[Open Hub]] reports over 11,000 commits (since the start as an Apache project) by 18 contributors representing more than 140,000 lines of code. PDFBox has a well established, mature codebase maintained by an average size development team with decreasing [[wikt:year-over-year|year-over-year]] commits. Using the [[COCOMO]] model, it took an estimated 36 [[person-year]]s of effort.&lt;ref&gt;{{cite web|author=&amp;nbsp;|url=https://www.openhub.net/p/pdfbox/ |title=The Apache PDFBox Open Source Project on Open Hub |publisher=openhub.net |date=2017-03-18 |accessdate=2017-03-18}}&lt;/ref&gt;

==Structure==
Apache PDFBox has these components:
* PDFBox: the main part
* FontBox: handles font information
* XmpBox: handles [[Extensible Metadata Platform|XMP metadata]]
* Preflight (optional): checks PDF files for [[PDF/A]]-1b conformity.

==History==
PDFBox was started in 2002 in [[SourceForge]] by Ben Litchfield who wanted to be able to extract text of PDF files for [[Lucene]].&lt;ref&gt;[http://www.h-online.com/open/news/item/Apache-PDFBox-and-FontBox-1-0-0-released-932436.html Apache PDFBox and FontBox 1.0.0 released], The H Open, 16 February 2010&lt;/ref&gt; It became an [[Apache Incubator]] project in 2008, and an Apache top level project in 2009.&lt;ref&gt;[https://incubator.apache.org/projects/pdfbox.html PDFBox Project Incubation Status]&lt;/ref&gt;

Preflight was originally named PaDaF and developed by [[Atos|Atos worldline]], and donated to the project in 2011.&lt;ref&gt;[https://incubator.apache.org/ip-clearance/pdfbox-padaf.html PaDaF Preflight Codebase Intellectual Property (IP) Clearance Status]&lt;/ref&gt;

In February 2015, Apache PDFBox was named an Open Source Partner Organization of the [[PDF Association]].&lt;ref&gt;[https://www.pdfa.org/new/apache%c2%99-pdfbox%c2%99-named-an-open-source-partner-organization-of-the-pdf-association/ Apache™ PDFBox™ named an Open Source Partner Organization of the PDF Association], February 3, 2015&lt;/ref&gt;

== See also ==
{{Portal|Free Software}}
* [[List of PDF software]]

==References==
{{Reflist}}

==External links==
*[https://pdfbox.apache.org Apache PDFBox Project]
*[https://people.apache.org/~lehmi/apachecon/ApacheConPDFBox.pdf Presentation at ApacheCon 2010 by Andreas Lehmkühler, PMC chair]

{{Apache}}

[[Category:Apache Software Foundation|PDFBox]]
[[Category:Java platform]]
[[Category:Free PDF software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) libraries]]
[[Category:Software using the Apache license]]</text>
      <sha1>n04hifbv1rr8dv86ik60hs6z05oqwnz</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Hive</title>
    <ns>0</ns>
    <id>30248516</id>
    <revision>
      <id>847557518</id>
      <parentid>843462479</parentid>
      <timestamp>2018-06-26T06:00:19Z</timestamp>
      <contributor>
        <ip>45.249.166.250</ip>
      </contributor>
      <comment>/* "Word count" program */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20981">{{Infobox Software
| name = Apache Hive
| logo = [[File:Apache Hive logo.svg|150px|Apache Hive]]
| screenshot =
| caption = Apache Hive
| developer = [https://hive.apache.org/people.html Contributors]
| status = Active
| latest release version = 2.3.0&lt;ref&gt;{{cite web|url=http://hive.apache.org/downloads.html|title=Apache Hive Download News}}&lt;/ref&gt;
| latest release date = {{Start date and age|2017|07|19}}
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[Data warehouse]]
| license = [[Apache License]] 2.0
| website = {{URL|https://hive.apache.org}}
|released=[https://github.com/apache/hive/releases/tag/release-1.0.0 {{Start date and age|2010|10|01}}]|repo=https://github.com/apache/hive|language=SQL}}

'''Apache Hive''' is a [[data warehouse]] software project built on top of [[Apache Hadoop]] for providing data summarization, query and analysis.&lt;ref&gt;{{cite book |last=Venner |first=Jason |title=Pro Hadoop |publisher=[[Apress]] |year=2009 |isbn=978-1-4302-1942-2}}&lt;/ref&gt; Hive gives an [[SQL]]-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the [[MapReduce]] Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries ([[#HiveQL|HiveQL]]) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids portability of SQL-based applications to Hadoop.&lt;ref name=":3"&gt;{{Cite book|url=https://www.safaribooksonline.com/library/view/programming-hive/9781449326944/|title=Programming Hive [Book]}}&lt;/ref&gt; While initially developed by [[Facebook]], Apache Hive is used and developed by other companies such as [[Netflix]] and the [[Financial Industry Regulatory Authority]] (FINRA).&lt;ref&gt;[http://www.slideshare.net/evamtse/hive-user-group-presentation-from-netflix-3182010-3483386 Use Case Study of Hive/Hadoop ]&lt;/ref&gt;&lt;ref&gt;{{YouTube|id=Idu9OKnAOis|title=OSCON Data 2011, Adrian Cockcroft, "Data Flow at Netflix"}}&lt;/ref&gt; Amazon maintains a software fork of Apache Hive included in [[Apache Hadoop#Amazon Elastic MapReduce|Amazon Elastic MapReduce]] on [[Amazon Web Services]].&lt;ref&gt;[http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf Amazon Elastic MapReduce Developer Guide]&lt;/ref&gt;

==Features==
Apache Hive supports analysis of large datasets stored in Hadoop's [[HDFS]] and compatible file systems such as [[Amazon S3]] filesystem. It provides an [[SQL]]-like query language called HiveQL&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual HiveQL Language Manual]&lt;/ref&gt; with schema on read and transparently converts queries to [[MapReduce]], Apache Tez&lt;ref&gt;[http://tez.apache.org/ Apache Tez]&lt;/ref&gt; and [[Apache Spark|Spark]] jobs. All three execution engines can run in [[Hadoop]] YARN. To accelerate queries, it provides indexes, including [[bitmap index]]es.&lt;ref&gt;[http://www.facebook.com/notes/facebook-engineering/working-with-students-to-improve-indexing-in-apache-hive/10150168427733920 Working with Students to Improve Indexing in Apache Hive]&lt;/ref&gt;
Other features of Hive include:
* Indexing to provide acceleration, index type including compaction and [[bitmap index]] as of 0.10, more index types are planned.
* Different storage types such as plain text, [[RCFile]], [[HBase]], ORC, and others.
* Metadata storage in a [[relational database management system]], significantly reducing the time to perform semantic checks during query execution.
* Operating on compressed data stored into the Hadoop ecosystem using algorithms including [[DEFLATE]], [[Burrows–Wheeler transform|BWT]], [[snappy (compression)|snappy]], etc.
* Built-in [[user-defined function]]s (UDFs) to manipulate dates, strings, and other data-mining tools. Hive supports extending the UDF set to handle use-cases not supported by built-in functions.
* SQL-like queries (HiveQL), which are implicitly converted into MapReduce or Tez, or Spark jobs.
By default, Hive stores metadata in an embedded [[Apache Derby]] database, and other client/server databases like [[MySQL]] can optionally be used.&lt;ref&gt;{{cite book |last=Lam |first=Chuck |title=Hadoop in Action |publisher=[[Manning Publications]] |year=2010 |isbn=1-935182-19-6}}&lt;/ref&gt;

The first four file formats supported in Hive were plain text,&lt;ref&gt;[http://www.semantikoz.com/blog/optimising-hadoop-big-data-text-hive/ Optimising Hadoop and Big Data with Text and HiveOptimising Hadoop and Big Data with Text and Hive]&lt;/ref&gt; sequence file, optimized row columnar (ORC) format&lt;ref&gt;{{Cite web |title= ORC Language Manual |url= https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC |work= Hive project wiki |access-date= April 24, 2017 }}&lt;/ref&gt; and [[RCFile]].&lt;ref&gt;[http://www.semantikoz.com/blog/faster-big-data-hadoop-hive-rcfile/ Faster Big Data on Hadoop with Hive and RCFile]&lt;/ref&gt;&lt;ref name=":0"&gt;[http://www.sfbayacm.org/wp/wp-content/uploads/2010/01/sig_2010_v21.pdf Facebook's Petabyte Scale Data Warehouse using Hive and Hadoop]&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf| title=RCFile: A Fast and Space-efficient Data Placement Structure in MapReduce-based Warehouse Systems|author1=Yongqiang He |author2=Rubao Lee |author3=Yin Huai |author4=Zheng Shao |author5=Namit Jain |author6=Xiaodong Zhang |author7=Zhiwei Xu |format=PDF}}&lt;/ref&gt; [[Apache Parquet]] can be read via plugin in versions later than 0.10 and natively starting at 0.13.&lt;ref&gt;{{cite web|title=Parquet|url=https://cwiki.apache.org/confluence/display/Hive/Parquet|accessdate=2 February 2015|archiveurl=https://web.archive.org/web/20150202145641/https://cwiki.apache.org/confluence/display/Hive/Parquet|archivedate=2 February 2015|date=18 Dec 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Massie|first1=Matt|title=A Powerful Big Data Trio: Spark, Parquet and Avro|url=http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/|website=zenfractal.com|accessdate=2 February 2015|archiveurl=https://web.archive.org/web/20150202145026/http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/|archivedate=2 February 2015|date=21 August 2013}}&lt;/ref&gt; Additional Hive plugins support querying of the [[Bitcoin]] [[Blockchain (database)|Blockchain]].&lt;ref&gt;{{cite web|last1=Franke|first1=Jörn|title=Hive &amp; Bitcoin: Analytics on Blockchain data with SQL|url=https://snippetessay.wordpress.com/2016/04/28/hive-bitcoin-analytics-on-blockchain-data-with-sql/}}&lt;/ref&gt;

== Architecture ==
{{Prose|section|date=October 2016}}

Major components of the Hive architecture are:
[[File:Hive architecture.png|thumb|300x300px|Hive Architecture&lt;ref&gt;{{Cite journal|last=Thusoo|first=Ashish|last2=Sarma|first2=Joydeep Sen|last3=Jain|first3=Namit|last4=Shao|first4=Zheng|last5=Chakka|first5=Prasad|last6=Anthony|first6=Suresh|last7=Liu|first7=Hao|last8=Wyckoff|first8=Pete|last9=Murthy|first9=Raghotham|date=2009-08-01|title=Hive: A Warehousing Solution over a Map-reduce Framework|url=https://dx.doi.org/10.14778/1687553.1687609|journal=Proc. VLDB Endow.|volume=2|issue=2|pages=1626–1629|doi=10.14778/1687553.1687609|issn=2150-8097}}&lt;/ref&gt;]]
* Metastore: Stores metadata for each of the tables such as their schema and location.  It also includes the partition metadata which helps the driver to track the progress of various data sets distributed over the cluster.&lt;ref name=":1"&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/Design|title=Design - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt; The data is stored in a traditional [[RDBMS]] format. The metadata helps the driver to keep a track of the data and it is highly crucial. Hence, a backup server regularly replicates the data which can be retrieved in case of data loss.
* Driver: Acts like a controller which receives the HiveQL statements. It starts the execution of statement by creating sessions and monitors the life cycle and progress of the execution. It stores the necessary metadata generated during the execution of an HiveQL statement. The driver also acts as a collection point of data or query result obtained after the Reduce operation.&lt;ref name=":0" /&gt;
* Compiler: Performs compilation of the HiveQL query, which converts the query to an execution plan.  This plan contains the tasks and steps needed to be performed by the [[Apache Hadoop|Hadoop]] [[MapReduce]] to get the output as translated by the query. The compiler converts the query to an [[abstract syntax tree]] (AST). After checking for compatibility and compile time errors, it converts the AST to a [[directed acyclic graph]] (DAG).&lt;ref&gt;{{Cite web|url=http://c2.com/cgi/wiki?AbstractSyntaxTree|title=Abstract Syntax Tree|website=c2.com|access-date=2016-09-12}}&lt;/ref&gt; The DAG divides operators to MapReduce stages and tasks based on the input query and data.&lt;ref name=":1" /&gt;
* Optimizer: Performs various transformations on the execution plan to get an optimized DAG.  Transformations can be aggregated together, such as converting a pipeline of joins to a single join, for better performance.&lt;ref name=":2"&gt;{{Cite journal|last=Dokeroglu|first=Tansel|last2=Ozal|first2=Serkan|last3=Bayir|first3=Murat Ali|last4=Cinar|first4=Muhammet Serkan|last5=Cosar|first5=Ahmet|date=2014-07-29|title=Improving the performance of Hadoop Hive by sharing scan and computation tasks|url=https://dx.doi.org/10.1186/s13677-014-0012-6|journal=Journal of Cloud Computing|language=English|volume=3|issue=1|pages=1–11|doi=10.1186/s13677-014-0012-6}}&lt;/ref&gt; It can also split the tasks, such as applying a transformation on data before a reduce operation, to provide better performance and scalability. However, the logic of transformation used for optimization used can be modified or pipelined using another optimizer.&lt;ref name=":0" /&gt;
* Executor: After compilation and optimization, the executor executes the tasks. It interacts with the job tracker of Hadoop to schedule tasks to be run. It takes care of pipelining the tasks by making sure that a task with dependency gets executed only if all other prerequisites are run.&lt;ref name=":2" /&gt;
* CLI, UI, and Thrift Server: A  [[command-line interface]] (CLI) provides a [[user interface]] for an external user to interact with Hive by submitting queries, instructions and monitoring the process status. Thrift server allows external clients to interact with Hive over a network, similar to the [[Jdbc|JDBC]] or [[Odbc|ODBC]] protocols.&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/HiveServer|title=HiveServer - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt;

==HiveQL==
While based on SQL, HiveQL does not strictly follow the full [[SQL-92]] standard. HiveQL offers extensions not in SQL, including ''multitable inserts'' and ''create table as select'', but only offers basic support for [[index (database)|indexes]]. 
HiveQL lacked support for [[database transaction|transactions]] and [[materialized view]]s, and only limited subquery support.&lt;ref name=":4"&gt;{{cite book |last=White |first=Tom |title=Hadoop: The Definitive Guide |publisher=[[O'Reilly Media]] |year=2010 |isbn=978-1-4493-8973-4}}&lt;/ref&gt;&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual Hive Language Manual]&lt;/ref&gt; Support for insert, update, and delete with full [[ACID]] functionality was made available with release 0.14.&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions ACID and Transactions in Hive]&lt;/ref&gt;

Internally, a [[compiler]] translates HiveQL statements into a [[directed acyclic graph]] of [[MapReduce]], Tez, or [[Apache Spark|Spark]] jobs, which are submitted to Hadoop for execution.&lt;ref&gt;[http://www.vldb.org/pvldb/2/vldb09-938.pdf Hive A Warehousing Solution Over a MapReduce Framework]&lt;/ref&gt;

=== Example ===
==== Word Count Program example in Pig ====

&lt;source lang="sql" line&gt;
input_lines = LOAD '/tmp/word.txt' AS (line:chararray);
words = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;
filtered_words = FILTER words BY word MATCHES '\\w+';
word_groups = GROUP filtered_words BY word;
word_count = FOREACH word_groups GENERATE COUNT(filtered_words) AS count, group AS word;
ordered_word_count = ORDER word_count BY count DESC;
STORE ordered_word_count INTO '/tmp/results.txt';
&lt;/source&gt;

==== "Word count" program ====

The word count program counts the number of times each word occurs in the input. The word count can be written in HiveQL as:&lt;ref name=":3" /&gt;

&lt;source lang="sql" line&gt;
DROP TABLE IF EXISTS docs;
CREATE TABLE docs (line STRING);
LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
 (SELECT explode(split(line, '\s')) AS word FROM docs) temp
GROUP BY word
ORDER BY word;
&lt;/source&gt;
A brief explanation of each of the statements is as follows:
&lt;source lang="sql" line&gt;
DROP TABLE IF EXISTS docs;
CREATE TABLE docs (line STRING);
&lt;/source&gt;
Checks if table &lt;code&gt;docs&lt;/code&gt; exists and drops it if it does. Creates a new table called &lt;code&gt;docs&lt;/code&gt; with a single column of type &lt;code&gt;STRING&lt;/code&gt; called &lt;code&gt;line&lt;/code&gt;.
&lt;source lang="sql" line start="3"&gt;
LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;
&lt;/source&gt;
Loads the specified file or directory (In this case “input_file”) into the table. &lt;code&gt;OVERWRITE&lt;/code&gt; specifies that the target table to which the data is being loaded into is to be re-written; Otherwise the data would be appended.
&lt;source lang="sql" line start="4" highlight="6"&gt;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
(SELECT explode(split(line, '\s')) AS word FROM docs) temp
GROUP BY word
ORDER BY word;
&lt;/source&gt;
The query {{code|CREATE TABLE word_counts AS SELECT word, count(1) AS count}} creates a table called &lt;code&gt;word_counts&lt;/code&gt; with two columns: &lt;code&gt;word&lt;/code&gt; and  &lt;code&gt;count&lt;/code&gt;. This query draws its input from the inner query {{code|lang=sql|(SELECT explode(split(line, '\s')) AS word FROM docs) temp"}}. This query serves to split the input words into different rows of a temporary table aliased as &lt;code&gt;temp&lt;/code&gt;. The {{code|lang=sql|GROUP BY WORD}} groups the results based on their keys. This results in the &lt;code&gt;count&lt;/code&gt; column holding the number of occurrences for each word of the &lt;code&gt;word&lt;/code&gt; column. The {{code|lang=sql|ORDER BY WORDS}} sorts the words alphabetically.

== Comparison with traditional databases ==
The storage and querying operations of Hive closely resemble those of traditional databases. While Hive is a SQL dialect, there are a lot of differences in structure and working of Hive in comparison to relational databases. The differences are mainly because Hive is built on top of the [[Apache Hadoop|Hadoop]] ecosystem, and has to comply with the restrictions of Hadoop and [[MapReduce]].

A schema is applied to a table in traditional databases. In such traditional databases, the table typically enforces the schema when the data is loaded into the table. This enables the database to make sure that the data entered follows the representation of the table as specified by the table definition. This design is called ''schema on write''. In comparison, Hive does not verify the data against the table schema on write. Instead, it subsequently does run time checks when the data is read. This model is called ''schema on read''.&lt;ref name=":4" /&gt; The two approaches have their own advantages and drawbacks. Checking data against table schema during the load time adds extra overhead, which is why traditional databases take a longer time to load data. Quality checks are performed against the data at the load time to ensure that the data is not corrupt. Early detection of corrupt data ensures early exception handling. Since the tables are forced to match the schema after/during the data load, it has better query time performance. Hive, on the other hand, can load data dynamically without any schema check, ensuring a fast initial load, but with the drawback of comparatively slower performance at query time. Hive does have an advantage when the schema is not available at the load time, but is instead generated later dynamically.&lt;ref name=":4" /&gt;

Transactions are key operations in traditional databases. A typical [[Relational database management system|RDBMS]] supports all 4 properties of transactions ([[ACID]]): [[Atomicity (database systems)|Atomicity]], [[Consistency (database systems)|Consistency]], [[Isolation (database systems)|Isolation]], and [[Durability (database systems)|Durability]]. Transactions in Hive were introduced in Hive 0.13 but were only limited to the partition level.&lt;ref&gt;{{Cite web|url=http://datametica.com/introduction-to-hive-transactions/|title=Introduction to Hive transactions|website=datametica.com|access-date=2016-09-12}}&lt;/ref&gt;  Only in the recent version of Hive 0.14 were these functions fully added to support complete [[ACID]] properties. This is because Hadoop does not support row level updates over specific partitions. These partitioned data are immutable and a new table with updated values has to be created. Hive 0.14 and later provides different row level transactions such as ''INSERT, DELETE and UPDATE''.&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-NewConfigurationParametersforTransactions|title=Hive Transactions - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt; Enabling ''INSERT, UPDATE, DELETE'' transactions require setting appropriate values for configuration properties such as &lt;code&gt;hive.support.concurrency&lt;/code&gt;, &lt;code&gt;hive.enforce.bucketing&lt;/code&gt;, and &lt;code&gt;hive.exec.dynamic.partition.mode&lt;/code&gt;.&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.txn.manager|title=Configuration Properties - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt;

==Security==
Hive v0.7.0 added integration with Hadoop security. Hadoop began using [[Kerberos (protocol)|Kerberos]] authorization support to provide security. Kerberos allows for mutual authentication between client and server. In this system, the client’s request for a ticket is passed along with the request. The previous versions of Hadoop had several issues such as users being able to spoof their username by setting the &lt;code&gt;hadoop.job.ugi&lt;/code&gt; property and also MapReduce operations being run under the same user: hadoop or mapred. With Hive v0.7.0’s integration with Hadoop security, these issues have largely been fixed. TaskTracker jobs are run by the user who launched it and the username can no longer be spoofed by setting the &lt;code&gt;hadoop.job.ugi&lt;/code&gt; property. Permissions for newly created files in Hive are dictated by the [[Apache Hadoop|HDFS]]. The Hadoop distributed file system authorization model uses three entities: user, group and others with three permissions: read, write and execute. The default permissions for newly created files can be set by changing the umask value for the Hive configuration variable &lt;code&gt;hive.files.umask.value&lt;/code&gt;.&lt;ref name=":3" /&gt;

==See also==
* [[Pig (programming tool)|Apache Pig]]
* [[Sqoop]]
* [[Apache Impala]]
* [[Apache Drill]]
* [[Apache Flume]]
* [[HBase|Apache HBase]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}
* [http://www.semantikoz.com/blog/the-free-apache-hive-book/ The Free Hive Book] (CC by-nc licensed)
* [http://www.vldb.org/pvldb/2/vldb09-938.pdf Hive A Warehousing Solution Over a MapReduce Framework] - Original paper presented by Facebook at [[VLDB]] 2009
* [https://www.youtube.com/watch?v=Y3UXDtDR9bg Using Apache Hive With Amazon Elastic MapReduce (Part 1)] and {{Youtube|id=1hDhpVmeSGI|title=Part 2}}, presented by an AWS Engineer
* [https://github.com/2013Commons/hive-cassandra Using hive + cassandra + shark. A hive cassandra cql storage handler.]
* [http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-14-2.pdf Major Technical Advancements in Apache Hive], Yin Huai, Ashutosh Chauhan, Alan Gates, Gunther Hagleitner, Eric N. Hanson, Owen O’Malley, Jitendra Pandey, Yuan Yuan, Rubao Lee and Xiaodong Zhang, [[SIGMOD]] 2014
* [https://cwiki.apache.org/confluence/display/Hive/Home Apache Hive Wiki]

{{Apache}}
{{Facebook navbox}}

{{DEFAULTSORT:Apache Hive}}
[[Category:2015 software]]
[[Category:Apache Software Foundation|Hive]]
[[Category:Apache Software Foundation projects|Hive]]
[[Category:Cloud computing]]
[[Category:Facebook software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>a6ecwbi8e9mal6ok7svrfvbgvblfjic</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Maven</title>
    <ns>0</ns>
    <id>1333305</id>
    <revision>
      <id>837443283</id>
      <parentid>831894260</parentid>
      <timestamp>2018-04-20T21:11:12Z</timestamp>
      <contributor>
        <ip>201.190.138.225</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22919">{{Multiple issues|
{{technical|date=October 2015}}
{{primary sources|date=October 2015}}
{{More citations needed|date=March 2012}}
}}

{{Infobox software
| name                   = Apache Maven
| logo                   = Maven logo.svg
| developer              = [[Apache Software Foundation]] 
| released               = {{Start date and age|df=yes|2004|07|13}}
| latest release version = 3.5.3&lt;ref&gt;{{cite web|url=https://maven.apache.org/docs/3.5.3/release-notes.html|title=Maven – Release Notes – Maven 3.5.3|publisher=}}&lt;/ref&gt;
| latest release date    = {{Start date and age|df=yes|2018|03|08}}&lt;ref&gt;{{cite web|url=https://maven.apache.org/docs/history.html|title=Maven – Maven Releases History|publisher=}}&lt;/ref&gt;
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Build tool]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://maven.apache.org}}
}}
'''Maven''' is a [[build automation]] tool used primarily for [[Java (programming language)|Java]] projects.

Maven addresses two aspects of building software: first, it describes how software is built,{{clarify|reason=What about how software is built?|date=December 2017}} and second, it describes its dependencies. Unlike earlier tools like [[Apache Ant]], it uses conventions for the build procedure, and only exceptions need to be written down. An [[XML]] file describes the software project being built, its dependencies on other external modules and components, the build order, directories, and required [[Plug-in (computing)|plug-ins]]. It comes with pre-defined targets for performing certain well-defined tasks such as compilation of code and its packaging.

Maven dynamically downloads [[Java (programming language)|Java]] libraries and Maven plug-ins from one or more repositories such as the Maven 2 Central Repository, and stores them in a local cache.&lt;ref name="maven2repo"&gt;{{cite web|url=http://repo1.maven.org/maven2/|title=Index of /maven2/|publisher=}}&lt;/ref&gt;  This local cache of downloaded [[Artifact (software development)|artifacts]] can also be updated with artifacts created by local projects.  Public repositories can also be updated.

Maven can also be used to build and manage projects written in [[C Sharp (programming language)|C#]], [[Ruby (programming language)|Ruby]], [[Scala (programming language)|Scala]], and other languages. The Maven project is hosted by the [[Apache Software Foundation]], where it was formerly part of the [[Jakarta Project]].

Maven is built using a plugin-based architecture that allows it to make use of any application controllable through standard input.  Theoretically, this would allow anyone to write plugins to interface with build tools (compilers, unit test tools, etc.) for any other language.  In reality, support and use for languages other than Java has been minimal. Currently a plugin for the .NET framework exists and is maintained,&lt;ref&gt;{{cite web|url=http://doodleproject.sourceforge.net/mavenite/dotnet-maven-plugin/index.html|title=.NET Maven Plugin :: .NET Maven Plugin|publisher=}}&lt;/ref&gt; and a [[C (programming language)|C]]/[[C++]] native plugin is maintained for Maven 2.&lt;ref&gt;{{cite web|url=http://www.mojohaus.org/maven-native/native-maven-plugin/|title=MojoHaus Native Maven Plugin|first=Trygve|last=Laugstol|publisher=}}&lt;/ref&gt;

Alternative technologies like [[Gradle]] and [[sbt]] as build tools do not rely on [[XML]], but keep the key concepts Maven introduced. With [[Apache Ivy]], a dedicated dependency manager was developed as well that also supports Maven repositories.&lt;ref&gt;{{cite web|url=https://ant.apache.org/ivy/history/2.2.0/resolver/ibiblio.html|title=IBiblio Resolver {{!}} Apache Ivy™}}&lt;/ref&gt;

==Example==
Maven projects are configured using a [[Project Object Model]], which is stored in a &lt;code&gt;pom.xml&lt;/code&gt;-file. Here's a minimal example:

&lt;syntaxhighlight lang="xml"&gt;
&lt;project&gt;
  &lt;!-- model version is always 4.0.0 for Maven 2.x POMs --&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  
  &lt;!-- project coordinates, i.e. a group of values which
       uniquely identify this project --&gt;
  
  &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt;
  &lt;artifactId&gt;my-app&lt;/artifactId&gt;
  &lt;version&gt;1.0&lt;/version&gt;

  &lt;!-- library dependencies --&gt;
  
  &lt;dependencies&gt;
    &lt;dependency&gt;
    
      &lt;!-- coordinates of the required library --&gt;
      
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;3.8.1&lt;/version&gt;
      
      &lt;!-- this dependency is only used for running and compiling tests --&gt;
      
      &lt;scope&gt;test&lt;/scope&gt;
      
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;
&lt;/syntaxhighlight&gt;

This POM only defines a unique identifier for the project (''coordinates'') and its dependency on the [[JUnit]] framework. However, that is already enough for building the project and running the [[Unit testing|unit tests]] associated with the project. Maven accomplishes this by embracing the idea of [[Convention over Configuration]], that is, Maven provides default values for the project's configuration. 

The directory structure of a normal [[Programming idiom|idiomatic]] Maven project has the following directory entries:
[[File:Maven CoC.svg|thumb|right|The Maven software tool auto-generated this directory structure for a Java project.]]
{| class="wikitable"
|-
! Directory name
! Purpose
|-
| project home
| Contains the pom.xml and all subdirectories.
|-
| src/main/java
| Contains the deliverable Java sourcecode for the project.
|-
| src/main/resources
| Contains the deliverable resources for the project, such as property files.
|-
| src/test/java
| Contains the testing Java sourcecode (JUnit or TestNG test cases, for example) for the project.
|-
| src/test/resources
| Contains resources necessary for testing.
|}

Then the command
&lt;syntaxhighlight lang="text"&gt;
 mvn package
&lt;/syntaxhighlight&gt;
will compile all the Java files, run any tests, and package the deliverable code and resources into &lt;code&gt;target/my-app-1.0.jar&lt;/code&gt; (assuming the artifactId is my-app and the version is 1.0.)

Using Maven, the user provides only configuration for the project, while the configurable plug-ins do the actual work of compiling the project, cleaning target directories, running unit tests, generating API documentation and so on. In general, users should not have to write plugins themselves. Contrast this with [[Apache Ant|Ant]] and [[make (software)|make]], in which one writes imperative procedures for doing the aforementioned tasks.

==Concepts==

===Project Object Model===
A Project Object Model (POM) provides all the configuration for a single project. General configuration covers the project's name, its owner and its dependencies on other projects. One can also configure individual phases of the build process, which are implemented as [[plug-in (computing)|plugin]]s. For example, one can configure the compiler-plugin to use Java version 1.5 for compilation, or specify packaging the project  even if some unit tests fail.

Larger projects should be divided into several modules, or sub-projects, each with its own POM. One can then write a root POM through which one can compile all the modules with a single command. POMs can also inherit configuration from other POMs. All POMs inherit from the Super POM&lt;ref&gt;[https://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Super_POM Super POM]
&lt;/ref&gt; by default. The Super POM provides default configuration, such as default source directories, default plugins, and so on.

===Plugins===
Most of Maven's functionality is in [[Plug-in (computing)|plugins]]. A plugin provides a set of goals that can be executed using the following syntax:
&lt;syntaxhighlight lang="text"&gt;
 mvn [plugin-name]:[goal-name]
&lt;/syntaxhighlight&gt;
For example, a Java project can be compiled with the compiler-plugin's compile-goal&lt;ref&gt;{{cite web|url=https://maven.apache.org/plugins/maven-compiler-plugin/|title=Apache Maven Compiler Plugin – Introduction|first=Edwin|last=Punzalan|publisher=}}&lt;/ref&gt; by running &lt;code&gt;mvn compiler:compile&lt;/code&gt;.

There are Maven plugins for building, testing, source control management, running a web server, generating [[Eclipse (software)|Eclipse]] project files, and much more.&lt;ref&gt;{{cite web|url=https://maven.apache.org/plugins/index.html|title=Maven – Available Plugins|first=Brett Porter Jason van Zyl Dennis Lundberg Olivier Lamy Benson Margulies Karl-Heinz|last=Marbaise|publisher=}}&lt;/ref&gt; Plugins are introduced and configured in a &lt;plugins&gt;-section of a &lt;code&gt;pom.xml&lt;/code&gt; file. Some basic plugins are included in every project by default, and they have sensible default settings.

However, it would be cumbersome if the archetypical build sequence of building, testing and packaging a software project required running each respective goal manually:
&lt;syntaxhighlight lang="text"&gt;
 mvn compiler:compile
 mvn surefire:test
 mvn jar:jar
&lt;/syntaxhighlight&gt;
Maven's lifecycle concept handles this issue.

Plugins are the primary way to extend Maven.  Developing a Maven plugin can be done by extending the org.apache.maven.plugin.AbstractMojo class.  Example code and explanation for a Maven plugin to create a cloud-based virtual machine running an application server is given in the article ''Automate development and management of cloud virtual machines''.&lt;ref&gt;{{Cite journal| last=Amies| first=Alex|author2=Zou P X |author3=Wang Yi S | title=Automate development and management of cloud virtual machines|journal=IBM developerWorks|publisher=IBM|date=29 Oct 2011| url=http://www.ibm.com/developerworks/cloud/library/cl-automatecloud/index.html}}&lt;/ref&gt;

===Build lifecycles===

Build lifecycle is a list of named ''phases'' that can be used to give order to goal execution. One of Maven's standard lifecycles is the ''default lifecycle'', which includes the following phases, in this order:&lt;ref&gt;{{cite web|url=https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference|title=Maven – Introduction to the Build Lifecycle|first=Brett|last=Porter|publisher=}}&lt;/ref&gt;
&lt;syntaxhighlight lang="text" line&gt;
 validate
 generate-sources
 process-sources
 generate-resources
 process-resources
 compile
 process-test-sources
 process-test-resources
 test-compile
 test
 package
 install
 deploy
&lt;/syntaxhighlight&gt;
Goals provided by plugins can be associated with different phases of the lifecycle. For example, by default, the goal "compiler:compile" is associated with the "compile" phase, while the goal "surefire:test" is associated with the "test" phase. Consider the following command:
 
 mvn test

When the preceding command is executed, Maven runs all goals associated with each of the phases up to and including the "test" phase. In such a case, Maven runs the "resources:resources" goal associated with the "process-resources" phase, then "compiler:compile", and so on until it finally runs the "surefire:test" goal.

Maven also has standard phases for cleaning the project and for generating a project site. If cleaning were part of the default lifecycle, the project would be cleaned every time it was built. This is clearly undesirable, so cleaning has been given its own lifecycle.

Standard lifecycles enable users new to a project the ability to accurately build, test and install every Maven project by issuing the single command:
 
 mvn install

By default, Maven packages the POM file in generated JAR and WAR files. Tools like diet4j&lt;ref&gt;{{cite web|url=http://diet4j.org/|title=diet4j - put Java JARs on a diet, and load maven modules as needed|publisher=}}&lt;/ref&gt; can use this information to recursively resolve and run Maven modules at run-time without requiring an "uber"-jar that contains all project code.

===Dependencies===
A central feature in Maven is [[library dependency|dependency management]].  Maven's dependency-handling mechanism is organized around a coordinate system identifying individual artifacts such as software libraries or modules.  The POM example above references the JUnit coordinates as a direct dependency of the project.  A project that needs, say, the [[Hibernate (Java)|Hibernate]] library simply has to declare Hibernate's project coordinates in its POM. Maven will automatically download the dependency and the dependencies that Hibernate itself needs (called transitive dependencies) and store them in the user's local repository. Maven 2 [http://central.sonatype.org Central Repository]&lt;ref name="maven2repo"/&gt; is used by default to search for libraries, but one can configure the repositories to be used (e.g., company-private repositories) within the POM.

There are search engines such as The Central Repository Search Engine&lt;ref&gt;[https://search.maven.org/ The Central Repository Search Engine],&lt;/ref&gt; which can be used to find out coordinates for different open-source libraries and frameworks.

Projects developed on a single machine can depend on each other through the local repository. The local repository is a simple folder structure that acts both as a cache for downloaded dependencies and as a centralized storage place for locally built artifacts. The Maven command &lt;code&gt;mvn install&lt;/code&gt; builds a project and places its binaries in the local repository. Then other projects can utilize this project by specifying its coordinates in their POMs.

== Maven compared with Ant ==
The fundamental difference between Maven and Ant is that Maven's design regards all projects as having a certain structure and a set of supported task work-flows (e.g., getting resources from source control, compiling the project, unit testing, etc.). While most software projects in effect support these operations and actually do have a well-defined structure, Maven requires that this structure and the operation implementation details be defined in the POM file. Thus, Maven [[Convention over configuration|relies on a convention]] on how to define projects and on the list of work-flows that are generally supported in all projects.&lt;ref&gt;{{cite web|title=Maven: The Complete Reference |url=http://www.sonatype.com/books/mvnref-book/reference/installation-sect-compare-ant-maven.html |publisher=Sonatype |accessdate=11 April 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20130421060027/http://www.sonatype.com/books/mvnref-book/reference/installation-sect-compare-ant-maven.html |archivedate=21 April 2013 }}&lt;/ref&gt;

This design constraint resembles the way that an IDE handles a project, and it provides many benefits, such as a succinct project definition, and the possibility of automatic integration of a Maven project with other development tools such as IDEs, build servers, etc.

But one drawback to this approach is that Maven requires a user to first understand what a project is from the Maven point of view, and how Maven works with projects, because what happens when one executes a phase in Maven is not immediately obvious just from examining the Maven project file. In many cases, this required structure is also a significant hurdle in migrating a mature project to Maven, because it is usually hard to adapt from other approaches.

In Ant, projects do not really exist from the tool's technical perspective. Ant works with XML build scripts defined in one or more files. It processes targets from these files and each target executes tasks. Each task performs a technical operation such as running a compiler or copying files around. Targets are executed primarily in the order given by their defined dependency on other targets. Thus, Ant is a tool that chains together targets and executes them based on inter-dependencies and other Boolean conditions.

The benefits provided by Ant are also numerous. It has an XML language optimized for clearer definition of what each task does and upon what it depends. Also, all the information about what will be executed by an Ant target can be found in the Ant script.

A developer not familiar with Ant would normally be able to determine what a simple Ant script does just by examining the script. This is not usually true for Maven.

However, even an experienced developer who is new to a project using Ant cannot infer what the higher level structure of an Ant script is and what it does without examining the script in detail. Depending on the script's complexity, this can quickly become a daunting challenge. With Maven, a developer who previously worked with other Maven projects can quickly examine the structure of a never-before-seen Maven project and execute the standard Maven work-flows against it while already knowing what to expect as an outcome.

It is possible to use Ant scripts that are defined and behave in a uniform manner for all projects in a working group or an organization. However, when the number and complexity of projects rises, it is also very easy to stray from the initially desired uniformity. With Maven this is less of a problem because the tool always imposes a certain way of doing things.

Note that it is also possible to extend and configure Maven in a way that departs from the Maven way of doing things.  This is particularly true for Maven 2 and newer releases, such as [https://maven.apache.org/guides/introduction/introduction-to-plugins.html Mojos] or more formally, [[Plug-in (computing)|plugins]] and custom&lt;ref&gt;{{cite web|title=Maven Build Customization|url=https://www.packtpub.com/application-development/maven-build-customization|publisher=Packt|accessdate=6 November 2014}}&lt;/ref&gt; project directory structures.

==IDE integration==
Add-ons to several popular [[integrated development environment]]s targeting the Java programming language exist to provide integration of Maven with the IDE's build mechanism and source editing tools, allowing Maven to compile projects from within the IDE, and also to set the classpath for code completion, highlighting compiler errors, etc. Examples of popular IDEs supporting development with Maven include:
* [[Eclipse (software)|Eclipse]]
* [[NetBeans]]
* [[IntelliJ IDEA]]
* [[JBuilder]]
* [[JDeveloper]] (version 11.1.2)
* [[MyEclipse]]

These add-ons also provide the ability to edit the POM or use the POM to determine a project's complete set of dependencies directly within the IDE.

Some built-in features of IDEs are forfeited when the IDE no longer performs compilation. For example, Eclipse's JDT has the ability to recompile a single java source file after it has been edited. Many IDEs work with a flat set of projects instead of the hierarchy of folders preferred by Maven. This complicates the use of [[Software configuration management|SCM]] systems in IDEs when using Maven.&lt;ref&gt;{{cite web|url=https://maven.apache.org/eclipse-plugin.html |title=maven.apache.org/eclipse-plugin.html |publisher= |deadurl=yes |archiveurl=https://web.archive.org/web/20150507014621/http://maven.apache.org/eclipse-plugin.html |archivedate=May 7, 2015 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.jetbrains.com/idea/features/ant_maven.html#Maven_Integration|title=IntelliJ IDEA :: Features|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://wiki.netbeans.org/MavenBestPractices|title=MavenBestPractices - NetBeans Wiki|publisher=}}&lt;/ref&gt;

==History==
Maven, created by Takari's Jason van Zyl, began as a subproject of [[Apache Turbine]] in 2002.  In 2003, it was voted on and accepted as a top level [[Apache Software Foundation]] project.  In July 2004, Maven's release was the critical first milestone, v1.0.  Maven 2 was declared v2.0 in October 2005 after about six months in beta cycles. Maven 3.0 was released in October 2010 being mostly backwards compatible with Maven 2.

===Maven 3===
Maven 3.0 information began trickling out in 2008. After eight alpha releases, the first beta version of Maven 3.0 was released in April 2010.
Maven 3.0 has reworked the core Project Builder infrastructure resulting in the POM's file-based representation being decoupled from its in-memory object representation.  This has expanded the possibility for Maven 3.0 add-ons to leverage non-XML based project definition files.  Languages suggested include [[Ruby (programming language)|Ruby]] (already in private prototype by Jason van Zyl), [[YAML]], and [[Groovy (programming language)|Groovy]].

Special attention was given to ensuring backward compatibility of Maven 3 to Maven 2. For most projects, upgrading to Maven 3 will not require any adjustments of their project structure. The first beta of Maven 3 saw the introduction of a parallel build feature which leverages a configurable number of cores on a multi-core machine and is especially suited for large multi-module projects.

==See also==
{{Portal|Free software}}
* [[Apache Ant]], an alternative dependency management tool for Java
* [[Apache Continuum]], a [[continuous integration]] server which integrates with Maven
* [[Apache Jelly]], a tool for turning XML into executable code
* [[Apache Ivy]], an alternative dependency management tool for Java
* [[Gradle]], a build tool based on convention over configuration
* [[Hudson (software)|Hudson]]
* [[Jenkins (software)|Jenkins]]
* [[List of build automation software]]

==References==
{{Reflist|colwidth=35em}}

==Further reading==
{{Refbegin}}
* A free online book - {{cite web|last=O'Brien|first=Tim|title=Maven: The Complete Reference|url=http://www.sonatype.com/books/mvnref-book/reference/|work=Sonatype.com|publisher=Sonatype|accessdate=15 March 2013|display-authors=etal}}
* The anteater book: {{
cite book
| others                = Sonatype Company
| title                 = Maven: The Definitive Guide
| url                   = https://books.google.com/books?id=cBvZ4s72Z0gC
| accessdate            = 2013-04-17
| year                  = 2009
| publisher             = O'Reilly Media, Inc.
| isbn                  = 9780596551780
| pages                 = 470
}}
* A printed book - {{citation
| first = Jason 
| last = Van Zyl
| title = Maven: Definitive Guide
| author-link =
| publication-date =
| date = 2008-10-01
| edition = first
| volume =
| series =
| publication-place =
| place =
| publisher = [[O'Reilly Media]]
| pages = 468 
| page =
| id =
| isbn = 0-596-51733-5
| doi =
| oclc =
| url =
| accessdate =
}}
*{{Cite book
|title= JUnit in Action
|publisher= [[Manning Publications]]
|chapter= Running JUnit tests from Maven2
|year= 2011
|edition= 2nd
|isbn= 978-1-935182-02-3
|pages= 152–168
}}
*{{Cite book
|title= Maven Build Customization
|publisher= Packt
|year= 2013
|isbn= 9781783987221
|pages= 1–250
}}
*{{Cite book
|title= Mastering Apache Maven 3
|url= https://www.packtpub.com/application-development/mastering-apache-maven-3
|publisher= Packt
|year= 2014
|isbn= 9781783983865
|pages= 298
}}
{{Refend}}

==External links==
*{{Official website}}

{{Apache}}

{{DEFAULTSORT:Apache Maven}}
[[Category:Compiling tools]]
[[Category:Java development tools]]
[[Category:Apache Software Foundation|Maven]]
[[Category:Build automation|Maven]]
[[Category:Software using the Apache license]]</text>
      <sha1>mryjjbeldu2fm0l1adbsxs6xgzmu8el</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Software Foundation</title>
    <ns>0</ns>
    <id>1336</id>
    <revision>
      <id>841274156</id>
      <parentid>837421599</parentid>
      <timestamp>2018-05-14T21:53:39Z</timestamp>
      <contributor>
        <ip>15.211.201.90</ip>
      </contributor>
      <comment>/* Financials */ fixed typos</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10352">{{Infobox organization
| name   = Apache Software Foundation
| logo   = Apache_Software_Foundation_Logo_(2016).svg
| logo_size = 250px
| type   = [[501(c)#501(c)(3)|501(c)(3)]]
| founded_date      = {{start date and age|1999|06}}
| founders          = [[Brian Behlendorf]], [[Ken Coar]], Mark Cox, [[Lars Eilebrecht]], [[Ralf S. Engelschall]], [[Roy T. Fielding]], [[Dean Gaudet]], [[Ben Hyde]], [[Jim Jagielski]], [[Alexei Kosut]], [[Martin Kraemer]], [[Ben Laurie]], [[Doug MacEachern]], [[Aram Mirzadeh]], [[Sameer Parekh]], [[Cliff Skolnick]], [[Marc Slemko]], [[William (Bill) Stoddard]], [[Paul Sutton 2|Paul Sutton]], [[Randy Terbush]], [[Dirk-Willem van Gulik]]
| location          = [[Forest Hill, Maryland]], USA 
| origins           = 
| key_people        = 
| area_served       = 
| product           =
| focus             = [[Open-source software]]
| method            = [[Apache License]]
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| owner             = 
| Non-profit_slogan = 
| homepage          = {{URL|https://www.apache.org}}
| dissolved         = 
| footnotes         = 
}}
The '''Apache Software Foundation''' {{IPAc-en|ə|ˈ|p|æ|tʃ|i}} ('''ASF''') is an American [[non-profit corporation]] (classified as [[501(c)#501(c)(3)|501(c)(3)]] in the United States) to support Apache software projects, including the [[Apache HTTP Server]]. The ASF was formed from the Apache Group and incorporated in [[Delaware corporation|Delaware]], U.S., in June 1999.&lt;ref name=incorporation&gt;
{{cite web
| first       = Roy T.
| last        = Fielding
| title       = Certificate of Incorporation of the Apache Software Foundation
| url         = https://www.apache.org/foundation/records/certificate.html
| accessdate  = 2009-05-26
| archiveurl= https://web.archive.org/web/20090531160220/http://apache.org/foundation/records/certificate.html| archivedate= 31 May 2009 &lt;!--DASHBot--&gt;| deadurl= no}}
&lt;/ref&gt;&lt;ref name=effDate&gt;{{cite web
| first       = Jim
| last        = Jagielski
| title       = The Apache Software Foundation Board of Directors Meeting Minutes 01 June 1999
| url         = https://www.apache.org/foundation/records/minutes/1999/board_minutes_1999_06_01.txt
| accessdate  = 2009-05-26
}}&lt;/ref&gt;

The Apache Software Foundation is a decentralized open source community of developers. The software they produce is distributed under the terms of the [[Apache License]] and is [[free and open-source software]] (FOSS). The Apache projects are characterized by a collaborative, consensus-based development process and an open and pragmatic software license. Each project is managed by a self-selected team of technical experts who are active contributors to the project. The ASF is a [[meritocracy]], implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a second generation&lt;ref&gt;[[François Letellier]], see 'Third Generation Open Source'&lt;/ref&gt; open-source organization, in that commercial support is provided without the risk of [[platform lock-in]].

Among the ASF's objectives are: to provide legal protection&lt;ref&gt;''See'' the [[Volunteer Protection Act]] article.&lt;/ref&gt; to volunteers working on Apache projects; to prevent the ''Apache'' brand name from being used by other organizations without permission.

The ASF also holds several ApacheCon&lt;ref&gt;{{cite web|url=http://www.apachecon.com/ |title=apachecon.com |publisher=apachecon.com |date= |accessdate=2014-06-26}}&lt;/ref&gt; conferences each year, highlighting Apache projects and related technology.

==History==
The history of the Apache Software Foundation is linked to the Apache HTTP Server, development beginning in February 1993. A group of eight developers started working on enhancing the [[NCSA HTTPd]] [[Daemon (computing)|daemon]]. They came to be known as the Apache Group. On March 25, 1999, the Apache Software Foundation was formed.&lt;ref name=incorporation/&gt; The first official meeting of the Apache Software Foundation was held on April 13, 1999, and by general consent that the initial membership list of the Apache Software Foundation, would be: [[Brian Behlendorf]], [[Ken Coar]], Miguel Gonzales, Mark Cox, [[Lars Eilebrecht]], Ralf S. Engelschall, [[Roy Fielding|Roy T. Fielding]], Dean Gaudet, Ben Hyde, [[Jim Jagielski]], Alexei Kosut, Martin Kraemer, [[Ben Laurie]], Doug MacEachern, Aram Mirzadeh, [[Sameer Parekh]], Cliff Skolnick, Marc Slemko, William (Bill) Stoddard, Paul Sutton, [[Randy Terbush]] and [[Dirk-Willem van Gulik]].&lt;ref name=firstMeeting&gt;{{cite web
| first       = Ben
| last        = Hyde
| title       = The Apache Software Foundation Board of Directors Meeting Minutes 13 April 1999
| url         = https://www.apache.org/foundation/records/minutes/1999/board_minutes_1999_04_13.txt
| accessdate  = 2009-05-26
}}&lt;/ref&gt; After a series of additional meetings to elect board members and resolve other legal matters regarding incorporation, the effective incorporation date of the Apache Software Foundation was set to June 1, 1999.&lt;ref name=effDate/&gt;

The foundation state the [[Apache HTTP Server#Name|name 'Apache']] was chosen ''"from respect for the Native American [[Apache]] Nation, well known for their superior skills in warfare strategy and their inexhaustible endurance."'' It also makes a pun on "a patchy web server"—a server made from a series of patches—but this was not its origin. The group of developers who released this new software soon started to call themselves the "Apache Group".&lt;ref name=":0" /&gt;

==Projects==
{{See also|List of Apache Software Foundation projects}}
Apache divides its [[software development]] activities into separate semi-autonomous areas called "top-level projects" (formally known as a "Project Management Committee" in the bylaws&lt;ref name=bylaws&gt;
{{cite web
| title       = Bylaws of The Apache Software Foundation
| url         = https://www.apache.org/foundation/bylaws.html
| publisher = Apache Software Foundation
| accessdate  = 2011-08-10
| archiveurl= https://web.archive.org/web/20110725012133/http://apache.org/foundation/bylaws.html| archivedate= 25 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}
&lt;/ref&gt;), some of which have a number of sub-projects. Unlike some other organizations that host [[Free and open-source software|FOSS]] projects, before a project is hosted at Apache it has to be licensed to the ASF with a grant or contributor agreement.&lt;ref name=licenses&gt;
{{cite web
| title       = Licenses
| url         = https://www.apache.org/licenses/#clas
| publisher = Apache Software Foundation
| accessdate  = 2011-08-10
| archiveurl= https://web.archive.org/web/20110725011702/http://www.apache.org/licenses/| archivedate= 25 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}
&lt;/ref&gt; In this way, the ASF gains the necessary intellectual property rights for the development and distribution of all its projects.&lt;ref&gt;{{cite book|last= St. Amant|first=Kirk|author2=Brian Still|title=Handbook of research on open source software: technological, economic, and social perspectives|publisher=Idea Group Inc (IGI)|year=2007|pages=217–219|isbn=978-1-59140-999-1|url=https://books.google.co.uk/books?id=75KT6GdcWbYC&amp;pg=PA218&amp;dq=%22apache+foundation%22#v=onepage&amp;q=%22apache%20foundation%22&amp;f=false}}&lt;/ref&gt;

==Board of directors==
The ASF board of directors has responsibility for overseeing the ASF's activities and acting as a central point of contact and communication for its projects. The board assigns corporate issues, assigning resources to projects, and manages corporate services, including funds and legal issues. It does not make technical decisions about individual projects; these are made by the individual Project Management Committees. The board is elected annually by members of the foundation.&lt;ref&gt;{{cite book
  | last = Weber
  | first = Steve
  | title = The success of open source
  | publisher = Harvard University Press
  | year = 2004
  | location = 
  | page = 187
  | url = https://books.google.co.uk/books?id=ELieXMxR1h4C&amp;pg=PA187&amp;dq=%22apache+software+foundation%22+board+of+directors#v=onepage&amp;q=%22apache%20software%20foundation%22%20board%20of%20directors&amp;f=false
  | isbn = 978-0-674-01292-9}}&lt;/ref&gt;&lt;ref&gt;{{cite web
  | title = Board of Directors
  | publisher = Apache Software Foundation
  | year = 2013
  | url = https://www.apache.org/foundation/board/
  | accessdate = 2013-05-23 }}&lt;/ref&gt;&lt;ref name=":0"&gt;{{cite web
  | title = How the ASF works
  | publisher = Apache Software Foundation
  | year = 2010
  | url = https://www.apache.org/foundation/how-it-works.html#structure
  | accessdate = 2010-04-08| archiveurl= https://web.archive.org/web/20100722185522/http://www.apache.org/foundation/how-it-works.html| archivedate= 22 July 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

==Financials==
In the 2010–11 fiscal year, the Foundation took in $539,410.00 almost entirely from grants and contributions with $12,349.00 from two ApacheCons.  With no employees and 2663 volunteers, it spent $270,846 on infrastructure, $92,364 on public relations, and $17,891 on two ApacheCons.&lt;ref&gt;{{cite web|url=https://www.apache.org/foundation/records/990-2010.pdf|date=2011-09-14|title=Apache Software Foundation IRS Form 990 for 2010|accessdate=2013-02-20}}&lt;/ref&gt;

==See also==
* [[Apache Attic]]
* [[Apache Incubator]]

==Notes==
{{Reflist}}

==Further reading==
*''[[Wikinomics|Wikinomics: How Mass Collaboration Changes Everything]]'' (2006); [[Don Tapscott]], [[Anthony D. Williams (author)|Anthony D. Williams]].

==External links==
{{Commons category|Apache Software Foundation}}
*{{Official website|https://www.apache.org}}
*[https://projects.apache.org ASF Projects Directory]
*[https://people.apache.org ASF Committer Directory]
*[https://wiki.apache.org/general General ASF Wiki]
*[http://www.apachecon.com ApacheCon website]
*[https://wiki.apache.org/apachecon/FrontPage ApacheCon Wiki]
*[http://apiwave.com/java/api/org.apache Apache popular APIs in GitHub]

{{Apache}}
{{FOSS}}

{{Authority control}}

[[Category:1999 establishments in Maryland]]
[[Category:Apache Software Foundation| ]]
[[Category:Free and open-source software organizations]]
[[Category:Free software project foundations]]
[[Category:Non-profit organizations based in Maryland]]
[[Category:Software companies established in 1999]]</text>
      <sha1>sccyhj9dllvh9wd6d12wo01zni3l2cu</sha1>
    </revision>
  </page>
  <page>
    <title>Storm (event processor)</title>
    <ns>0</ns>
    <id>38576265</id>
    <revision>
      <id>847975052</id>
      <parentid>838347561</parentid>
      <timestamp>2018-06-29T01:14:54Z</timestamp>
      <contributor>
        <username>Luism6n</username>
        <id>34094680</id>
      </contributor>
      <minor/>
      <comment>Add latest version release date</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7597">{{Use dmy dates|date=December 2014}}
{{Infobox software
| name                   = Apache Storm
| logo                   = [[File:Storm logo.png|frameless|Apache Storm's Logo]]
| developer              = Backtype, Twitter
| status                 = Active
| latest release version = 1.0.5
| latest release date    = {{release date|df=yes|2017|09|15}}
| operating system       = [[Cross-platform]]
| size                   =
| programming language   = [[Clojure]] &amp;amp; [[Java (programming language)|Java]]
| genre                  = [[Distributed computing|Distributed]] [[stream processing]]
| license                = [[Apache License]] 2.0
| website                = {{url|https://storm.apache.org/}}
|logo caption = Distributed and fault-tolerant realtime computation}}

'''Apache Storm''' is a distributed [[stream processing]] computation framework written predominantly in the [[Clojure]] programming language. Originally created by Nathan Marz&lt;ref&gt;{{cite web|last=Marz|first=Nathan|title=About Nathan Marz|url=http://nathanmarz.com/about/|publisher=Nathan Marz|accessdate=28 March 2013}}&lt;/ref&gt; and team at BackType,&lt;ref&gt;{{cite web|title=BackType Website (defunct)|url=http://www.backtype.com/|publisher=BackType|accessdate=28 March 2013}}&lt;/ref&gt; the project was open sourced after being acquired by Twitter.&lt;ref&gt;{{cite web|title=A Storm is coming: more details and plans for release|url=https://blog.twitter.com/2011/storm-coming-more-details-and-plans-release|work=Engineering Blog|publisher=Twitter Inc|accessdate=29 July 2015}}&lt;/ref&gt; It uses custom created "spouts" and "bolts" to define information sources and manipulations to allow batch, distributed processing of streaming data. The initial release was on 17 September 2011.&lt;ref&gt;{{cite web|title=Storm Codebase|url=https://github.com/nathanmarz/storm/commit/9d91adbdbde22e91779b91eb40805f598da5b004|publisher=Github|accessdate=8 February 2013}}&lt;/ref&gt;

A Storm application is designed as a "topology" in the shape of a [[directed acyclic graph]] (DAG) with spouts and bolts acting as the graph vertices. Edges on the graph are named streams and direct data from one node to another. Together, the topology acts as a data transformation pipeline. At a superficial level the general topology structure is similar to a [[MapReduce]] job, with the main difference being that data is processed in real time as opposed to in individual batches. Additionally, Storm topologies run indefinitely until killed, while a MapReduce job DAG must eventually end.&lt;ref&gt;{{cite web|title=Tutorial - Components of a Storm cluster|url=http://storm.apache.org/documentation/Tutorial.html|work=Documentation|publisher=Apache Storm|accessdate=29 July 2015}}&lt;/ref&gt;

Storm became an Apache Top-Level Project in September 2014&lt;ref&gt;{{cite web|title=Apache Storm Graduates to a Top-Level Project|url=http://hortonworks.com/blog/apache-storm-graduates-top-level-project/}}&lt;/ref&gt; and was previously in [[Apache Incubator|incubation]] since September 2013.&lt;ref&gt;{{cite web|title=Storm Project Incubation Status|url=http://incubator.apache.org/projects/storm.html|publisher=Apache Software Foundation|accessdate=29 October 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Storm Proposal|url=http://wiki.apache.org/incubator/StormProposal|publisher=Apache Software Foundation|accessdate=29 October 2013}}&lt;/ref&gt;

== Development ==
Apache Storm is developed under the [[Apache License]], making it available to most companies to use.&lt;ref&gt;{{cite web|title=Powered By Storm|url=http://storm.apache.org/documentation/Powered-By.html|work=Documentation|publisher=Apache Storm|accessdate=29 July 2015}}&lt;/ref&gt; Git is used for version control and Atlassian JIRA for issue tracking, under the Apache Incubator program.

{| class="wikitable" border="1"
|+ Major Releases&lt;ref&gt;{{Cite web|url=http://storm.apache.org/|title=Apache Storm|website=storm.apache.org|access-date=2017-08-18}}&lt;/ref&gt;
! Version !! Release Date
|-
|1.2.2
|4 June 2018
|-
|1.2.1
|19 February 2018
|-
|1.1.1
|1 August 2017
|-
|1.0.4
|28 July 2017
|-
| 1.1.0 || 29 Mar 2017
|-
| 1.0.0 || 12 April 2016
|-
| 0.10.0 || 5 November 2015
|-
| 0.9.6 || 5 November 2015
|-
| 0.9.5 || 4 June 2015

|-
| 0.9.4 || 25 March 2015

|-
| 0.9.3 || 25 November 2014

|-
| 0.9.2 || 25 June 2014

|-
| 0.9.1 || 10 February 2014

|-
! Historical (non-Apache) Version !! Release Date
|-
| 0.9.0 || 8 December 2013

|-
| 0.8.0 || 2 August 2012

|-
| 0.7.0 || 28 February 2012

|-
| 0.6.0 || 15 December 2011
|-
| 0.5.0 || 19 September 2011
|}

== Apache Storm Architecture ==
The Apache Storm cluster comprises following critical components:
* '''Nodes-''' There are two types of nodes, i.e., Master Node and Worker Node. The '''Master Node''' executes a daemon '''Nimbus''' which assigns tasks to machines and monitors their performances. On the other hand, the Worker Node runs the daemon called '''Supervisor''' which assigns the tasks to other worker node and operates them as per the need. As Storm cannot monitor the state and health of cluster, it deploys ZooKeeper to solve this issue which connects Nimbus with the Supervisors.  
* '''Components-''' Storm has three critical components, viz., Topology, Stream, and Spout. Topology is a network made of Stream and Spout. Stream is an unbounded pipeline of tuples and Spout is the source of the data streams which converts the data into the tuple of streams and sends to the bolts to be processed.{{citation needed|date=August 2017}}

== Peer platforms ==

Storm is but one of dozens of stream processing engines, for a more complete list see [[Stream processing]]. Twitter announced [[Heron (event processor)|Heron]] on June 2, 2015&lt;ref&gt;{{cite web|title=Flying faster with Twitter Heron|url=https://blog.twitter.com/2015/flying-faster-with-twitter-heron|work=Engineering Blog|publisher=Twitter Inc|accessdate=3 June 2015}}&lt;/ref&gt; which is API compatible with Storm. There are other comparable streaming data engines such as [[Apache Spark#Spark Streaming|Spark Streaming]] and [[Apache Flink|Flink]].&lt;ref&gt;{{cite web |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7530084 |format=PDF |title= Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming |date=May 2016 |publisher=IEEE }}&lt;/ref&gt;

== See also ==
* [[C++ AMP]]
* [[Data parallelism]]
* [[Lambda architecture]]
* [[Message passing]]
* [[OpenMP]]
* [[OpenCL]] 
* [[OpenHMPP]]
* [[Parallel computing]]
* [[Parallel Extensions#Task Parallel Library|TPL]]
* [[Thread (computing)]]

== References ==
{{Reflist}}

== External links ==
* [http://storm.apache.org Project Homepage]
* [https://issues.apache.org/jira/browse/STORM Storm's issue tracker]
* [https://github.com/apache/storm Storm Code Repository on Github]
* [http://blog.echen.me/2013/01/08/improving-twitter-search-with-real-time-human-computation/ Storm is used to improve Twitter Search]
* [http://www.infoq.com/presentations/Storm-Introduction Nathan Marz's Presentation on ''Storm: Distributed and Fault-Tolerant Real-time Computation'']
* [http://qnalist.com/g/storm Storm Mailing List Archives]
* [https://github.com/AirSage/Petrel Petrel, a tool for creating Storm applications in Python]
* [https://fsstorm.github.io/FsStorm/ FsStorm, a lib for authoring Storm components and topologies in F#]

{{Apache}}
{{Parallel computing}}

[[Category:Distributed computing architecture]]
[[Category:Parallel computing]]
[[Category:Cloud applications]]
[[Category:Cloud infrastructure]]
[[Category:Apache Software Foundation]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Distributed stream processing]]</text>
      <sha1>cjpklsfryqhrkyujteq72vax62yka49</sha1>
    </revision>
  </page>
  <page>
    <title>ApacheCon</title>
    <ns>0</ns>
    <id>41900032</id>
    <revision>
      <id>833952973</id>
      <parentid>798250359</parentid>
      <timestamp>2018-04-03T05:01:25Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:FrescoBot/Links|link syntax]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2039">{{refimprove|date=March 2015}}

'''ApacheCon''' is a conference about the [[Apache_HTTP_Server|Apache]] [[open source software]] and related technologies, run by the [[Apache Software Foundation]]. It has been in existence since 1998
&lt;ref name=apachecon-about&gt;{{cite web | url=https://www.apachecon.com/about/|title=About ApacheCon|publisher=The Apache Software Foundation||accessdate=August 31, 2017}}&lt;/ref&gt;
&lt;ref name="c2.net-apachecon"&gt;{{cite web |url=https://web.archive.org/web/19990225080530/http://www.c2.net:80/new/press/apachecon.php |title=Announcing ApacheCon '98 - the conference dedicated to the Apache Web Server |publisher=[[c2.net]] |accessdate=August 31, 2017}}&lt;/ref&gt;
&lt;ref&gt;{{cite web |url=http://drbacchus.com/apachecon-eu/|title=ApacheCon EU |last= Bowen|first=Rich |date= November 4, 2012|website=rcbowen.com |publisher= |access-date=August 31, 2017 |quote="In 1998, there was an event called ApacheCon, in San Francisco, hosted by CNet, but that was before the Apache Software Foundation was formed." -- Rich Bowen, Vice President of Conferences,  Apache Software Foundation.}}&lt;/ref&gt;
, and has been held once or twice a year since then, usually in North America or Europe but sometimes in Asia.&lt;ref&gt;[http://www.apachecon.com/c/ ApacheCon archives]&lt;/ref&gt; A typical size for the conference is 500 attendees, with 75% of attendees being developers.{{citation needed|date=November 2015}}

In addition to Apache, the conference showcases technologies including [[Hadoop]], [[Apache Cassandra|Cassandra]], [[Apache Spark]], [[Apache Mesos]], [[BigTop]], [[CloudStack]], [[Lucene]], and [[Solr]].

In 2015, discussions about Apache in the context of [[big data]] were split off into a separate event, Apache: Big Data.&lt;ref&gt;[http://events.linuxfoundation.org/events/apachecon-north-america/program/about ApacheCon North America: About]&lt;/ref&gt;

==See also==
* [[List of free-software events]]
* [[Linux Foundation#Events]]

==References==
{{Reflist}}

[[Category:Apache Software Foundation]]
[[Category:Free-software conferences]]</text>
      <sha1>ebrxjy5ie6433agc21t8zkydrrew7o3</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Software using the Apache license</title>
    <ns>14</ns>
    <id>36489491</id>
    <revision>
      <id>833919811</id>
      <parentid>712206027</parentid>
      <timestamp>2018-04-03T01:18:48Z</timestamp>
      <contributor>
        <username>JarBot</username>
        <id>27077959</id>
      </contributor>
      <minor/>
      <comment>Bot:add Commons category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="228">{{Commons category|Free software under the Apache License}}
{{catmain| Apache License}}
[[Software]] that uses the [[Apache License]].

[[Category:Free software by license|Apache License]]
[[Category:Apache Software Foundation]]</text>
      <sha1>3n7wub3qjm54k1wmtagom3ow6u6lxb4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:OpenOffice</title>
    <ns>14</ns>
    <id>38052934</id>
    <revision>
      <id>670451640</id>
      <parentid>670451558</parentid>
      <timestamp>2015-07-08T01:38:58Z</timestamp>
      <contributor>
        <username>Saectar</username>
        <id>20111688</id>
      </contributor>
      <comment>removed [[Category:Software using the Apache license]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="769">{{Commons cat|OpenOffice.org}}
'''OpenOffice.org''' is a discontinued [[open-source software|open-source]] [[office suite|office productivity software suite]] containing [[word processor]], [[spreadsheet]], [[Presentation program|presentation]], [[Graphics software|graphics]], [[Formula|formula editor]], and [[Relational database management system|database management]] applications. Several active [[fork (software development)|fork]]s exist.

[[Category:Open-source office suites]]
[[Category:OpenDocument]]
[[Category:Sun Microsystems software]]
[[Category:Office suites for Linux]]
[[Category:Free software programmed in C++]]
[[Category:Cross-platform free software]]
[[Category:Wikipedia categories named after software]]
[[Category:Apache Software Foundation]]</text>
      <sha1>306ig4hjwbnqc8pnprnbn26qlkey1nx</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Ambari</title>
    <ns>0</ns>
    <id>47478005</id>
    <revision>
      <id>845773536</id>
      <parentid>845772828</parentid>
      <timestamp>2018-06-14T01:36:54Z</timestamp>
      <contributor>
        <username>Gouravk</username>
        <id>3846079</id>
      </contributor>
      <comment>Preview info was outdated; removed it. Current status of preview version unknown</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2478">{{Infobox software
| name                   = Ambari
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 2.6.2
| latest release date    = {{release date|2018|05|01}}
| latest preview version =
| latest preview date    =
| programming language   = 
| operating system       = [[Cross-platform]]
| platform               =
| genre                  = [[Distributed computing]]
| license                = [[Apache License]] 2.0
| website                = {{url|//ambari.apache.org}}
}}

'''Apache Ambari''' is a software project of the [[Apache Software Foundation]].&lt;ref&gt;{{cite web|url=https://ambari.apache.org/|title=Ambari|work=[[Apache Software Foundation]]|accessdate=5 December 2016}}&lt;/ref&gt; Ambari enables system administrators to provision, manage and monitor a [[Hadoop]] cluster, and also to integrate Hadoop with the existing enterprise infrastructure. Ambari was a sub-project of Hadoop but is now a [[Apache Software Foundation#Projects|top-level]] project in its own right.

Ambari is used by companies including [[IBM]], [[Hortonworks]],  [[Cardinal Health]], [[EBay]], [[Expedia]], [[Kayak]], [[Lending club]], [[Neustar]], [[Pandora]], [[Priceline.com]], [[Samsung]], [[Shutterfly]], and [[Spotify]].{{citation needed|date=December 2015}}

== Apache Ambari Architecture ==

Ambari evolved as a solution to the problems faced by the developers in handling huge Hadoop clusters. As Hadoop started to increase its scalability, more and more application layers covered its architecture making it bulky and unmanageable.{{cn|date=January 2018}}

The architecture of Apache Ambari includes two major components: Ambari Server and Ambari Agent. Ambari Server is responsible for interacting with the agents installed on the nodes, while Ambari Agents update the status of every node with the help of various operational metrics.{{cn|date=January 2018}}

The Ambari infrastructure consists of REST APIs that help automate the operations running on the cluster. These APIs keep track of the health of the cluster, as well as integrating with operational tools and additional packages.{{cn|date=August 2017}}

==References==
{{Reflist}}
{{Portal|Java}}

==External links==
*{{Official website|//ambari.apache.org}}

{{Apache}}

{{DEFAULTSORT:Apache Ambari}}
[[Category:Apache Software Foundation|Ambari]]
[[Category:Configuration management]]
[[Category:Hadoop]]</text>
      <sha1>rugh6jd0tdvh3x10rsrbpuzffwf7vbv</sha1>
    </revision>
  </page>
  <page>
    <title>Gremlin (programming language)</title>
    <ns>0</ns>
    <id>33800942</id>
    <revision>
      <id>848208943</id>
      <parentid>822500256</parentid>
      <timestamp>2018-06-30T13:04:08Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <comment>WP:HEADERS</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13577">{{Infobox programming language
| name = Gremlin
| logo = [[File:Gremlin (programming language).png|255px]]
| year = {{Start date and age|2009}}
| designer = Marko A. Rodriguez
| developer = Apache TinkerPop of the [[Apache Software Foundation]]
| written_in = [[Java (programming language)]]
| latest release version = Gremlin 3.2.4&lt;ref name="Gremlin Releases"&gt;{{cite web|url=http://tinkerpop.apache.org/|title=Gremlin 3.2.4|accessdate=February 8, 2017}}&lt;/ref&gt;
| dialects = Gremlin-Java8, Gremlin-Groovy, Gremlin-Python, Gremlin-Scala, Gremlin-Clojure, Gremlin-PHP, Gremlin-JavaScript, Gremlin-Typeset
| operating_system = [[Cross-platform]] (multi-platform)
| license = [[Apache License]]
| website =  [http://tinkerpop.apache.org/ Official Site]
| status = Active
| influenced_by = [[Regular expression]], [[XPath]], Ripple, [[SPARQL]], [[SQL]], [[Java (programming language)|Java]]/[[Java virtual machine|JVM]]
}}

'''Gremlin''' is a [[graph traversal]] language and [[virtual machine]] developed by Apache TinkerPop of the [[Apache Software Foundation]]. Gremlin works for both [[Online transaction processing|OLTP]]-based graph databases as well as [[Online analytical processing|OLAP]]-based graph processors. Gremlin's [[Automata theory|automata]] and [[Functional programming|functional language]] foundation enable Gremlin to naturally support [[Imperative programming|imperative]] and [[Declarative programming|declarative]] querying, host language agnosticism, user-defined [[Domain-specific language|domain specific languages]], an extensible compiler/optimizer, single- and multi-machine execution models, hybrid depth- and breadth-first evaluation, as well as [[Turing Complete]]ness.&lt;ref name="Gremlin Machine and Language"&gt;{{cite book|eprint=1508.03843|title=The Gremlin Graph Traversal Machine and Language|pages=1|last1= Rodriguez|first1=Marko A.|class=cs.DB|year=2015|doi=10.1145/2815072.2815073|chapter=The Gremlin graph traversal machine and language (invited talk)|isbn=9781450339025}}&lt;/ref&gt;

As an explanatory analogy, Apache TinkerPop and Gremlin are to [[graph databases]] what the [[Java Database Connectivity|JDBC]] and [[SQL]] are to [[RDBMS|relational databases]]. Likewise, the Gremlin traversal machine is to graph computing as what the [[Java virtual machine]] is to general purpose computing.&lt;ref name="The Benefits of the Gremlin Graph Traversal Machine"&gt;{{cite web|url=http://www.datastax.com/dev/blog/the-benefits-of-the-gremlin-graph-traversal-machine|title=The Benefits of the Gremlin Graph Traversal Machine|accessdate=September 17, 2015}}&lt;/ref&gt;

== History ==
* 2009-10-30 the project is born, and immediately named "TinkerPop"
* 2009-12-25 v0.1 is the first release
* 2011-05-21 v1.0 is released
* 2012-05-24 v2.0 is released
* 2015-01-16 TinkerPop becomes an Apache Incubator project
* 2015-07-09 v3.0.0-incubating is released
* 2016-05-23 Apache TinkerPop becomes a top-level project
* 2016-07-18 v3.1.3 and v3.2.1 are first releases as Apache TinkerPop
* 2017-12-17 v3.3.1 is released

== Vendor integration ==

Gremlin is an [[Apache License|Apache2-licensed]] graph traversal language that can be used by graph system vendors. There are typically two types of graph system vendors: OLTP [[graph databases]] and OLAP graph processors. The table below outlines those graph vendors that support Gremlin.

{| class="wikitable"
|-
! Vendor
! Graph System
|-
| [[Neo4j]]
| graph database
|-
| [[OrientDB]]
| graph database
|-
| [[DataStax]] Enterprise (5.0+)
| graph database
|-
| [[Apache Hadoop|Hadoop]] ([[Apache Giraph|Giraph]])
| graph processor
|-
| [[Apache Hadoop|Hadoop]] ([[Apache Spark|Spark]])
| graph processor
|-
| [[InfiniteGraph]]
| graph database
|-
| [[JanusGraph]]
| graph database
|-
| [[Cosmos DB]]
| graph database
|-
| [[Amazon Neptune]]
| graph database
|-
| [[Ontotext]] GraphDB
| [[Triplestore]]
|}

== Traversal examples ==
The following examples of Gremlin queries and responses in a Gremlin-Groovy environment are relative to a graph representation of the [http://grouplens.org/datasets/movielens/ MovieLens] dataset.&lt;ref name="NoSQLNow Slides"&gt;{{cite web|url=http://www.slideshare.net/slidarko/the-gremlin-traversal-language|title=The Gremlin Graph Traversal Language|accessdate=August 22, 2015}}&lt;/ref&gt; The dataset includes users who rate movies. Users each have one occupation, and each movie has one or more categories associated with it. The MovieLens graph schema is detailed below.
&lt;source lang="cypher"&gt;
user--rated[stars:0-5]--&gt;movie
user--occupation--&gt;occupation
movie--category--&gt;category
&lt;/source&gt;

=== Simple traversals ===

{{pull quote|For each vertex in the graph, emit its label, then group and count each distinct label.}}

&lt;source lang="groovy"&gt;
gremlin&gt; g.V().label().groupCount()
==&gt;[occupation:21, movie:3883, category:18, user:6040]
&lt;/source&gt;

{{pull quote|What year was the oldest movie made?}}

&lt;source lang="groovy"&gt;
gremlin&gt; g.V().hasLabel('movie').values('year').min()
==&gt;1919
&lt;/source&gt;

{{pull quote|What is Die Hard's average rating?}}

&lt;source lang="groovy"&gt;
gremlin&gt; g.V().has('movie','name','Die Hard').inE('rated').values('stars').mean()
==&gt;4.121848739495798
&lt;/source&gt;

=== Projection traversals ===

{{pull quote|For each category, emit a map of its name and the number of movies it represents.}}

&lt;source lang="groovy"&gt;
gremlin&gt; g.V().hasLabel('category').as('a','b').
           select('a','b').
             by('name').
             by(inE('category').count())
==&gt;[a:Animation, b:105]
==&gt;[a:Children's, b:251]
==&gt;[a:Comedy, b:1200]
==&gt;[a:Adventure, b:283]
==&gt;[a:Fantasy, b:68]
==&gt;[a:Romance, b:471]
==&gt;[a:Drama, b:1603]
==&gt;[a:Action, b:503]
==&gt;[a:Crime, b:211]
==&gt;[a:Thriller, b:492]
==&gt;[a:Horror, b:343]
==&gt;[a:Sci-Fi, b:276]
==&gt;[a:Documentary, b:127]
==&gt;[a:War, b:143]
==&gt;[a:Musical, b:114]
==&gt;[a:Mystery, b:106]
==&gt;[a:Film-Noir, b:44]
==&gt;[a:Western, b:68]
&lt;/source&gt;

{{pull quote|For each movie with at least 11 ratings, emit a map of its name and average rating. Sort the maps in decreasing order by their average rating. Emit the first 10 maps (i.e. top 10).}}

&lt;source lang="groovy"&gt;
gremlin&gt; g.V().hasLabel('movie').as('a','b').
           where(inE('rated').count().is(gt(10))).
           select('a','b').
             by('name').
             by(inE('rated').values('stars').mean()).
           order().by(select('b'),decr).
           limit(10)
==&gt;[a:Sanjuro, b:4.608695652173913]
==&gt;[a:Seven Samurai (The Magnificent Seven), b:4.560509554140127]
==&gt;[a:Shawshank Redemption, The, b:4.554557700942973]
==&gt;[a:Godfather, The, b:4.524966261808367]
==&gt;[a:Close Shave, A, b:4.52054794520548]
==&gt;[a:Usual Suspects, The, b:4.517106001121705]
==&gt;[a:Schindler's List, b:4.510416666666667]
==&gt;[a:Wrong Trousers, The, b:4.507936507936508]
==&gt;[a:Sunset Blvd. (a.k.a. Sunset Boulevard), b:4.491489361702127]
==&gt;[a:Raiders of the Lost Ark, b:4.47772]
&lt;/source&gt;

=== Declarative pattern matching traversals ===

Gremlin supports declarative graph pattern matching similar to [[SPARQL]]. For instance, the following query below uses Gremlin's ''match()''-step.

{{pull quote|What 80's action movies do 30-something programmers like? Group count the movies by their name and sort the group count map in decreasing order by value.  Clip the map to the top 10 and emit the map entries.}}

&lt;source lang="groovy"&gt;
gremlin&gt; g.V().
           match(
             __.as('a').hasLabel('movie'),
             __.as('a').out('category').has('name','Action'),
             __.as('a').has('year',between(1980,1990)),
             __.as('a').inE('rated').as('b'),
             __.as('b').has('stars',5),
             __.as('b').outV().as('c'),
             __.as('c').out('occupation').has('name','programmer'),
             __.as('c').has('age',between(30,40))).
           select('a').groupCount().by('name').
           order(local).by(valueDecr).
           limit(local,10)
==&gt;Raiders of the Lost Ark=26
==&gt;Star Wars Episode V - The Empire Strikes Back=26
==&gt;Terminator, The=23
==&gt;Star Wars Episode VI - Return of the Jedi=22
==&gt;Princess Bride, The=19
==&gt;Aliens=18
==&gt;Boat, The (Das Boot)=11
==&gt;Indiana Jones and the Last Crusade=11
==&gt;Star Trek The Wrath of Khan=10
==&gt;Abyss, The=9
&lt;/source&gt;

=== OLAP traversal ===

{{pull quote|Which movies are most central in the ''implicit'' 5-stars graph?}}

&lt;source lang="groovy"&gt;
gremlin&gt; g = graph.traversal(computer(SparkGraphComputer))
==&gt;graphtraversalsource[hadoopgraph[gryoinputformat-&gt;gryooutputformat], sparkgraphcomputer]
gremlin&gt; g.V().repeat(outE('rated').has('stars', 5).inV().
                 groupCount('m').by('name').
                 inE('rated').has('stars', 5).outV()).
               times(4).cap('m')
==&gt;Star Wars Episode IV - A New Hope	  35405394353105332
==&gt;American Beauty	  31943228282020585
==&gt;Raiders of the Lost Ark	31224779793238499
==&gt;Star Wars Episode V - The Empire Strikes Back  30434677119726223
==&gt;Godfather, The	30258518523013057
==&gt;Shawshank Redemption, The	28297717387901031
==&gt;Schindler's List	27539336654199309
==&gt;Silence of the Lambs, The	26736276376806173
==&gt;Fargo	 26531050311325270
==&gt;Matrix, The	 26395118239203191
&lt;/source&gt;

== Gremlin graph traversal machine ==

Gremlin is a [[virtual machine]] composed of an [[instruction set]] as well as an execution engine. An analogy is drawn between Gremlin and [[Java (programming language)|Java]].

{| class="wikitable"
|-
! Java Ecosystem
! Gremlin Ecosystem
|-
| [[Groovy (programming language)|Apache Groovy programming language]]
| Gremlin-Groovy
|-
| [[Scala (programming language)|Scala programming language]]
| Gremlin-Scala
|-
| [[Clojure (programming language)|Clojure programming language]]
| Gremlin-Clojure
|-
| ...
| ...
|-
| [[Java (programming language)|Java programming language]]
| Gremlin-Java8
|-
| Java instruction set
| Gremlin step library
|-
| [[Java virtual machine]]
| Gremlin traversal machine
|}

=== Gremlin steps (instruction set) ===

The following traversal is a Gremlin traversal in the Gremlin-Java8 dialect.

&lt;source lang="java"&gt;
g.V().as("a").out("knows").as("b").
  select("a","b").
    by("name").
    by("age")
&lt;/source&gt;
The Gremlin language (i.e. the [[Fluent interface|fluent-style]] of expressing a graph traversal) can be represented in any host language that supports [[function composition]] and [[Nested function|function nesting]]. Due to this simple requirement, there exists various Gremlin dialects including Gremlin-Groovy, Gremlin-Scala, Gremlin-Clojure, etc. The above Gremlin-Java8 traversal is ultimately compiled down to a step sequence called a ''traversal''. A string representation of the traversal above provided below.
&lt;source lang="java"&gt;
[GraphStep([],vertex)@[a], VertexStep(OUT,[knows],vertex)@[b], SelectStep([a, b],[value(name), value(age)])]
&lt;/source&gt;
The ''steps'' are the primitives of the Gremlin graph traversal machine. They are the parameterized instructions that the machine ultimately executes. The Gremlin [[instruction set]] is approximately 30 steps. These steps are sufficient to provide general purpose computing and what is typically required to express the common motifs of any graph traversal query.

Given that Gremlin is a language, an instruction set, and a virtual machine, it is possible to design another traversal language that compiles to the Gremlin traversal machine (analogous to how Scala compiles to the [[Java virtual machine|JVM]]). For instance, the popular [[SPARQL]] graph pattern match language can be compiled to execute on the Gremlin machine. The following SPARQL query
&lt;source lang="sparql"&gt;
SELECT ?a ?b ?c
WHERE {
  ?a a Person .
  ?a ex:knows ?b .
  ?a ex:created ?c .
  ?b ex:created ?c .
  ?b ex:age ? d .
    FILTER(?d &lt; 30)
}
&lt;/source&gt;
would compile to
&lt;source lang="java"&gt;
[GraphStep([],vertex), MatchStep(AND,[[MatchStartStep(a), LabelStep, IsStep(eq(Person)), MatchEndStep], [MatchStartStep(a), VertexStep(OUT,[knows],vertex), MatchEndStep(b)], [MatchStartStep(a), VertexStep(OUT,[created],vertex), MatchEndStep(c)], [MatchStartStep(b), VertexStep(OUT,[created],vertex), MatchEndStep(c)], [MatchStartStep(b), PropertiesStep([age],value), MatchEndStep(d)], [MatchStartStep(d), IsStep(gt(30)), MatchEndStep]]), SelectStep([a, b, c])].
&lt;/source&gt;
In Gremlin-Java8, the SPARQL query above would be represented as below and compile to the identical Gremlin step sequence (i.e. traversal).
&lt;source lang="java"&gt;
g.V().match(
  as("a").label().is("person"),
  as("a").out("knows").as("b"),
  as("a").out("created").as("c"),
  as("b").out("created").as("c"),
  as("b").values("age").as("d"),
  as("d").is(gt(30))).
    select("a","b","c")
&lt;/source&gt;

=== Gremlin Machine (virtual machine) ===

The Gremlin graph traversal machine can execute on a single machine or across a multi-machine compute cluster. Execution agnosticism allows Gremlin to run over both [[graph databases]] (OLTP) and graph processors (OLAP).

== References ==
{{reflist}}

== External links ==
# [http://www.tinkerpop.com Apache TinkerPop Homepage]
# [http://gremlindocs.com/ GremlinDocs.com (TinkerPop2)]
# [http://sql2gremlin.com/ sql2gremlin.com (TinkerPop2)]
# Rodriguez, M.A., "[https://arxiv.org/abs/1508.03843 The Gremlin Graph Traversal Machine and Language]," Proceedings of the ACM Database Programming Languages Conference, October, 2015.

{{Apache}}

[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Free software programmed in Scala]]
[[Category:Declarative programming languages]]</text>
      <sha1>sklty6xtlac8ldndyy65zye2av3c3at</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OpenWebBeans</title>
    <ns>0</ns>
    <id>48452444</id>
    <revision>
      <id>730651146</id>
      <parentid>707901113</parentid>
      <timestamp>2016-07-20T12:22:25Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <comment>remove advert</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2092">{{ Infobox Software
| name                   = Apache OpenWebBeans 
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.6.3
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Contexts and Dependency Injection|CDI Container]] 
| license                = [[Apache License]] 2.0
| website                = http://openwebbeans.apache.org
}}

'''OpenWebBeans''' is an [[open source]], embeddable and lightweight CDI Container, released under the [[Apache Software License|Apache 2.0 License]]. OpenWebBeans is an ASL-licensed implementation of Context and Dependency Injection for Java EE Platform Specification which is defined by [[JSR-299]],&lt;ref&gt;[https://jcp.org/en/jsr/detail?id=299 JSR-299 Specification]&lt;/ref&gt; [[JSR-346]],&lt;ref&gt;[https://jcp.org/en/jsr/detail?id=346 JSR-346 Specification]&lt;/ref&gt; and [[JSR-365]],.&lt;ref&gt;[https://www.jcp.org/en/jsr/detail?id=365 JSR-365 Specification]&lt;/ref&gt; OpenWebBeans has been integrated with Java EE application servers such as [[Apache Geronimo|Geronimo]],&lt;ref&gt;[http://geronimo.apache.org Apache Geronimo]&lt;/ref&gt; and [[Apache TomEE]]&lt;ref&gt;[http://tomee.apache.org Apache TomEE]&lt;/ref&gt;

==History==
OpenWebBeans was founded by Gurkan Erdogdu in October 2008. It is one of implementation of the Context and Dependency Injection specification. The incubator proposal can be found here,.&lt;ref&gt;[http://wiki.apache.org/incubator/OpenWebBeansProposal OpenWebBeans Incubator Proposal]&lt;/ref&gt; It was graduated in December 2009 to become a top level ASF project.

==See also==
*[[Contexts and Dependency Injection|Context and Dependency Injection (CDI)]]

==References==
{{reflist}}

==External links==
* {{official website|http://openwebbeans.apache.org}}

{{DEFAULTSORT:Apache Openwebbeans}}
[[Category:Apache Software Foundation|OpenWebBeans]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>tak5ocmquinmj1eyzcgaznza541qxi4</sha1>
    </revision>
  </page>
  <page>
    <title>Jakarta Cactus</title>
    <ns>0</ns>
    <id>7985307</id>
    <revision>
      <id>811207712</id>
      <parentid>782295328</parentid>
      <timestamp>2017-11-20T04:41:54Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead. #IABot (v1.6.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2084">{{Multiple issues|
{{refimprove|date=November 2015}}
{{notability|Products|date=November 2015}}
}}

{{ Infobox Software
| name                   = Jakarta Cactus
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| status                 = Retired
| latest release version = 1.8.1
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Unit Test]]
| license                = [[Apache License]] 2.0
| website                = http://jakarta.apache.org/cactus/
}}
'''Cactus''' is a simple test framework for unit testing server-side [[Java (programming language)|Java]] code ([[Servlets]], [[Enterprise JavaBean|EJB]]s, [[Tag library|Tag libs]], ...) from the [[Jakarta Project]]. The intent of Cactus is to lower the cost of writing tests for server-side code. It uses [[JUnit]] and extends it. Cactus implements an in-container strategy, meaning that tests are executed inside the container.

== Project status ==
The Jakarta Cactus project was retired on August 5, 2011.

The Jakarta Cactus project announced the new 1.8.1 version [http://jakarta.apache.org/cactus/changes-report.html] on January 18, 2009. Version 1.8.1 still does not support [[JUnit]] 4.x, although a workaround of sorts is documented [https://issues.apache.org/jira/browse/CACTUS-252]. As of 1.8, Cactus uses [https://web.archive.org/web/20080514003156/http://cargo.codehaus.org/ Cargo] for all server-related manipulation.
As of 08/05/2011, cactus has been retired. http://jakarta.apache.org/cactus/mock_vs_cactus.html

== External links ==
*[http://jakarta.apache.org/cactus/ Official Page of Jakarta Cactus]
*[https://web.archive.org/web/20080514003156/http://cargo.codehaus.org/ Official Page of the Codehaus Cargo Project]

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Unit testing frameworks]]
[[Category:Java development tools]]</text>
      <sha1>oi6x9ryk6s8v239jjeyprxth8ejz1n2</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Calcite</title>
    <ns>0</ns>
    <id>49368574</id>
    <revision>
      <id>837809561</id>
      <parentid>825795295</parentid>
      <timestamp>2018-04-23T05:02:39Z</timestamp>
      <contributor>
        <ip>115.64.177.97</ip>
      </contributor>
      <comment>Update latest stable release</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2588">{{Infobox software
| name = Apache Calcite
| logo = [[File:Apache Calcite Logo.png|frameless|Apache Calcite Logo]]
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| status = Active
| released = {{Start date and age|2014|06|27}}&lt;ref&gt;{{cite web |url=https://mail-archives.apache.org/mod_mbox/optiq-dev/201406.mbox/%3CCABivuanCAHphdi4U3UC+b3c0OZt+dBk+HVVWBREex191SnXyxw@mail.gmail.com%3E |title=Optiq-0.8 release announcement|author=Julian Hyde|publisher=Apache Software Foundation}}&lt;/ref&gt;
| latest release version = 1.16.0
| latest release date = {{release date|df=yes|2018|03|19}}&lt;ref&gt;{{cite web|url=https://calcite.apache.org/news/2018/03/19/release-1.16.0/|accessdate=23 April 2018|title=Release 1.16.0|date=24 March 2017|first=Jesús|last=Camacho}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[SQL database]]
| license = [[Apache License]] 2.0
| website = {{URL|http://calcite.apache.org/}}
}}

'''Apache Calcite''' is an [[Open-source software|open source]] framework for
building databases and data management systems.
It includes a [[SQL]] [[parser]],
an [[API]] for building expressions in [[relational algebra]],
and a [[Query optimization|query planning]] engine.
As a framework, Calcite does not store its own data or [[metadata]],
but instead allows external data and metadata to be accessed by means of [[Plug-in (computing)|plug-ins]].

Several other [[Apache Software Foundation|Apache]] projects use Calcite.&lt;ref&gt;{{cite web |url=http://calcite.apache.org/docs/powered_by.html|title=Powered by Calcite|publisher=Apache Software Foundation}}&lt;/ref&gt;
[[Apache Hive|Hive]] uses Calcite for cost-based query optimization;&lt;ref&gt;Julian Hyde. [http://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-hive-014 "Cost-based query optimization in Apache Hive 0.14"], ''[http://www.hortonworks.com Hortonworks]'', 24 September 2014.&lt;/ref&gt;
[[Apache Drill|Drill]] and [[Apache Kylin|Kylin]] use Calcite for SQL parsing and optimization;
[[Apache Samza|Samza]] and [[Apache Storm|Storm]] use Calcite for streaming SQL.
{{As of|2016|8}}, [[Apache Apex|Apex]], [[Apache Phoenix|Phoenix]] and [[Apache Flink|Flink]] have projects under development that use Calcite.

==References==
{{Reflist}}

{{Apache}}

[[Category:Relational database management systems]]
[[Category:Apache Software Foundation|Calcite]]
[[Category:Software using the Apache license]]
[[Category:Free software programmed in Java (programming language)]]</text>
      <sha1>qx2fqrfq0rb6h1uh3a14a7o1asifcl0</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Phoenix</title>
    <ns>0</ns>
    <id>43866463</id>
    <revision>
      <id>843464238</id>
      <parentid>800656306</parentid>
      <timestamp>2018-05-29T09:34:11Z</timestamp>
      <contributor>
        <username>AbdealiJK</username>
        <id>26436539</id>
      </contributor>
      <comment>Update infobox info</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3678">{{Use dmy dates|date=October 2013}}
{{Infobox software
| name = Apache Phoenix
| logo = [[File:Phoenix-bird-trans.png|frameless|Apache Phoenix Logo]]
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| released = {{Start date and age|2014|04|15}}
| latest release version = 4.14.0
| latest release date = {{release date|df=yes|2018|05|26}}
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| repo = https://github.com/apache/phoenix
| programming language = [[Java (programming language)|Java]]
| genre = [[SQL database]]
| license = [[Apache License]] 2.0
| website = {{URL|http://phoenix.apache.org/}}
}}

'''Apache Phoenix''' is an [[Open-source software|open source]], [[Massively parallel (computing)|massively parallel]], [[relational database]] engine supporting [[OLTP]] for Hadoop using [[Apache HBase]] as its backing store. Phoenix provides a [[Java Database Connectivity|JDBC]] driver that hides the intricacies of the noSQL store enabling users to create, delete, and alter SQL tables, views, indexes, and sequences; insert and delete rows singly and in bulk; and query data through [[SQL]].&lt;ref&gt;James Taylor. [https://www.youtube.com/watch?v=f4Nmh5KM6gI "Apache Phoenix Transforming HBase into a SQL database"], ''[http://hadoopsummit.org/ HadoopSummit]'', 4 June 2014.&lt;/ref&gt; Phoenix compiles queries and other statements into native noSQL store APIs rather than using [[MapReduce]] enabling the building of low latency applications on top of noSQL stores.&lt;ref&gt;Istvan Szegedi. [http://bighadoop.wordpress.com/2014/05/17/apache-phoenix-an-sql-driver-for-hbase/ "Apache Phoenix – an SQL Driver for HBase"], ''[http://bighadoop.wordpress.com/ BigHadoop]'', 17 May 2014.&lt;/ref&gt;

==History==
Phoenix began as an internal project by the company [[salesforce.com]] out of a need to support a higher level, well understood, SQL language. It was originally open-sourced on [[GitHub]]&lt;ref&gt;Abel Avram. [http://www.infoq.com/news/2013/01/Phoenix-HBase-SQL "Phoenix: Running SQL Queries on Apache HBase"], ''[http://www.infoq.com/ InfoQ]'', 31 January 2013.&lt;/ref&gt; and became a top-level [[Apache Software Foundation|Apache]] project on 22 May 2014.&lt;ref&gt;Adam Seligman. [https://developer.salesforce.com/blogs/developer-relations/2014/05/apache-phoenix-small-step-big-data.html "Apache Phoenix: A small step for big data"], ''[https://developer.salesforce.com/ Salesforce.com Developer]'', 28 May 2014.&lt;/ref&gt; Apache Phoenix is included in the [[Hortonworks]] distribution for HDP 2.1 and above,&lt;ref&gt;Hortonworks. [http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.3/bk_installing_manually_book/content/rpm-chap-phoenix.html "Chapter 7. Installing Phoenix"], ''[http://hortonworks.com/ Hortonworks]'', 2 July 2014.&lt;/ref&gt; is available as part of [[Cloudera]] labs,&lt;ref&gt;Srikanth Srungarapu. [http://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs "Apache Phoenix Joins Cloudera Labs"], ''[http://www.cloudera.com/ Cloudera]'', 6 May 2015.&lt;/ref&gt; and is part of the [[Apache Hadoop|Hadoop]]  ecosystem.&lt;ref&gt;Serdar Yegulalp. [http://www.infoworld.com/article/2683729/hadoop/10-ways-to-query-hadoop-with-sql.html "10 ways to query Hadoop with SQL"], "[http://www.infoworld.com/blog/infoworld-tech-watch/]", 16 September 2014.&lt;/ref&gt;

==See also==
*[[Apache HBase]]
*[[Apache Hadoop]]

==References==
{{Reflist}}
==External links==
*[http://phoenix.apache.org/ Official Apache Phoenix homepage]
*[http://blogs.apache.org/phoenix/ Official Apache Phoenix blog]

{{Apache}}

[[Category:Free software]]
[[Category:Apache Software Foundation|Phoenix]]
[[Category:Relational database management systems]]</text>
      <sha1>tu790s23mqy20ep9tpfde5islftbmby</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Brooklyn</title>
    <ns>0</ns>
    <id>47275266</id>
    <revision>
      <id>831888883</id>
      <parentid>770449347</parentid>
      <timestamp>2018-03-22T17:53:36Z</timestamp>
      <contributor>
        <ip>181.44.11.167</ip>
      </contributor>
      <comment>Update last version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2346">{{third-party|date=July 2015}}
{{Infobox Software
| name                   = Apache Brooklyn
| logo                   = [[File:Apache-brooklyn-logo.png|frameless|Brooklyn Logo]]
| caption                =
| developer              = [[Apache Software Foundation]], Cloudsoft
| status                 = Active
| released               = {{release date|2012|04}}&lt;ref name="what-is-apache-brooklyn"&gt;Presentation of Apache Brooklyn at ApacheCon (organized by Linux Foundation) - [http://events.linuxfoundation.org/sites/events/files/slides/Apache%20Brooklyn%20-%20what%20it%20is.pdf Apache Brooklyn - what it is.pdf ]&lt;/ref&gt;
| latest release version = v0.12.0
| latest release date    = {{release date|2017|9}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Linux]], [[macOS]], [[Windows]]
| size                   = 
| programming language   = [[Java (programming language)|Java]], [[Javascript (programming language)|Javascript]], [[Groovy (programming language)|Groovy]]
| genre                  = [[Cloud computing]] [[Orchestration_(computing)|Orchestration]]
| license                = [[Apache License]] 2.0 
| website                = {{URL|https://brooklyn.apache.org}}
}}

'''Apache Brooklyn''' is an [[Open-source software|open-source]] framework for modeling, deploying and managing distributed applications defined using declarative [[YAML]] blueprints. The design is influenced by [[Autonomic computing]] and promise theory and implements the [[OASIS_(organization)|OASIS]] [[Cloud Application Management for Platforms|CAMP]] (''Cloud Application Management for Platforms'') and [[OASIS TOSCA|TOSCA]] (''Topology and Orchestration Specification for Cloud Applications'') standards.&lt;ref&gt;{{cite web |url=https://brooklyn.apache.org/learnmore/theory.html |title=The Theory behind Brooklyn |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=2017-02-10 |website=apache.org |publisher=Apache Foundation |accessdate=2017-03-15}}&lt;/ref&gt;

==References==
{{Reflist|30em}}

==External links==
* https://brooklyn.apache.org/learnmore/theory.html
* https://brooklyn.apache.org/learnmore/features/index.html

{{Apache}}

[[Category:Configuration management]]
[[Category:Apache Software Foundation|Brooklyn]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Software distribution]]</text>
      <sha1>d7swokzg3qxvdjlil3o0zy2mftgd1e0</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Chemistry</title>
    <ns>0</ns>
    <id>35624581</id>
    <revision>
      <id>785201826</id>
      <parentid>706279311</parentid>
      <timestamp>2017-06-12T05:35:40Z</timestamp>
      <contributor>
        <username>Kiwi128</username>
        <id>12235997</id>
      </contributor>
      <comment>/* External links */ chemistry.apache.org supports HTTPS</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1412">{{refimprove|date=January 2013}}
'''Apache Chemistry''' is a project of the [[Apache Software Foundation]] ('''ASF''') which provides  open source [[Content Management Interoperability Services]] (CMIS) for Python, Java, PHP and .NET.&lt;ref&gt;[http://chemistry.apache.org chemistry.apache.org]&lt;/ref&gt;

Apache Chemistry is becoming a top-level project(TLP) of the [[Apache Software Foundation]] ('''ASF''').
Before becoming a TLP, Chemistry was just an incubating project, guided in its growth by the Incubator, like all Apache projects when they begin life in the [[Apache Software Foundation|ASF]].

== Sub-projects ==

* [[OpenCMIS]] - CMIS client and server libraries for Java
* cmislib - CMIS client library for Python
* phpclient - CMIS client library for PHP
* DotCMIS - CMIS client library for .NET
* ObjectiveCMIS - CMIS client library for Objective-C

== References ==
{{Reflist}}

== External links ==
* [//chemistry.apache.org/ Apache Chemistry Home Page]
* [//chemistry.apache.org/java/opencmis.html Apache Chemistry - OpenCMIS]
* [//chemistry.apache.org/python/cmislib.html Apache Chemistry - cmislib]
* [//chemistry.apache.org/php/phpclient.html Apache Chemistry - phpclient]
* [//chemistry.apache.org/dotnet/dotcmis.html Apache Chemistry - DotCMIS]
* [//chemistry.apache.org/objective-c/objectivecmis.html Apache Chemistry - ObjectiveCMIS]

{{Apache}}

[[Category:Apache Software Foundation|Chemistry]]</text>
      <sha1>jizdbmr0bw8v7a6g5yxsvrg6mevz5il</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Commons Logging</title>
    <ns>0</ns>
    <id>49404535</id>
    <revision>
      <id>772789570</id>
      <parentid>732416491</parentid>
      <timestamp>2017-03-29T10:08:34Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor/>
      <comment>/* Example */Typo fixing, replaced: existant → existent using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6314">{{Infobox software
| name                   = Apache Commons Logging
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| released               = 
| latest release version = 1.2
| latest release date    = {{release date|2015|07}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Logging Tool]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://commons.apache.org/proper/commons-logging/}}
}}

'''Apache Commons Logging''' (previously known as '''Jakarta Commons Logging''', '''JCL''') is a [[Java platform|Java]]-based [[data logging|logging]] utility and a programming model for logging and for other toolkits. It provides [[Application programming interface|API]]s, log implementations, and [[Adapter pattern|wrapper]] implementations over some other tools.&lt;ref name=":ac"&gt;{{Cite web|url = http://commons.apache.org/proper/commons-logging/|title = commons logging|date = |accessdate = 12 February 2016|website = Apache.org|publisher = Apache|last = |first = }}&lt;/ref&gt;&lt;ref name=":ijcliwas_2"&gt;{{Cite book|title = Integrating Jakarta Commons Logging with IBM WebSphere Application Server V5|url=http://www-01.ibm.com/support/docview.wss?uid=swg27004610&amp;aid=1|last = Zavala|first = D.A.|last2 = Lau| first2 = Y.C.| publisher = IBM corporation|year = 2004|location = |volume = |pages = 2}}&lt;/ref&gt;&lt;ref name=":ac_c"&gt;{{Cite web|url = http://commons.apache.org/proper/commons-logging/guide.html|title = contents|date = |accessdate = 12 February 2016|website = Apache.org|publisher = Apache|last = |first = }}&lt;/ref&gt;

== JCL log level ==
The following table defines the log levels and messages in JCL, in decreasing order of severity. The left column lists the log level designation in and the right column provides a brief description of each log level.

{| class="wikitable"
|-
!'''Level'''
!'''Description'''
|-
| '''fatal'''
| Severe errors that cause premature termination. Expect these to be immediately visible on a status console.
|-
| '''error'''
| Other runtime errors or unexpected conditions. Expect these to be immediately visible on a status console.
|-
| '''warn'''
| Use of deprecated APIs, poor use of API, 'almost' errors, other runtime situations that are undesirable or unexpected, but not necessarily "wrong". Expect these to be immediately visible on a status console.
|-
| '''info'''
| Interesting runtime events (startup/shutdown). Expect these to be immediately visible on a console, so be conservative and keep to a minimum.
|-
| '''debug'''
| Detailed information on the flow through the system. Expect these to be written to logs only.
|-
| '''trace'''
| Most detailed information. Expect these to be written to logs only.
|}&lt;ref name=":ac_c"/&gt;&lt;ref name=":ajc_120"&gt;{{Cite book|title = Apache Jakarta Commons - Reusable Java Components|last = Iverson|first = W.|publisher = Pearson Education, Inc.|year = 2005|location = Crawfordsville, Indiana, USA|volume = |pages = 120–122}}&lt;/ref&gt;

==Configuration==
Two basic abstractions, Log and LogFactory, are used in JCL.&lt;ref name=":ac_c"/&gt;

==Example==
Sample code may look like as follows:
&lt;source lang="Java"&gt;
package com.cascadetg.ch09;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.commons.logging.impl.Jdk14Logger;

public class LogGenerator
{
  // Note that you pass in an instance of this class to the
  // log generator. This allows you to find the messages
  // generated by this class.
  private static Log log = LogFactory.getLog(LogGenerator.class);

  public static void configJDKLogger()
  {
    try
    {
      ((Jdk14Logger)log).getLogger().setLevel(
java.util.logging.Level.ALL);
      ((Jdk14Logger)log).getLogger().addHandler(
(java.util.logging.FileHandler)Class
        .forName("java.util.logging.FileHandler")
        .newInstance());

      System.out.println("Added JDK 1.4 file handler");
    } catch (Exception e)
    {
      System.out.println("Unable to load JDK 1.4 logging.");
      e.printStackTrace();
    }
  }

  public static void main(String[] args)
  {
    configJDKLogger();
    System.setErr(System.out);

    System.out.println();
    System.out.println("Test fatal log");

    try
    {
      String foo = null;
      int x = 0 / (new Integer(foo)).intValue();
    } catch (Exception e)
    {
    log.fatal(e.getMessage(), e);
    }

    System.out.println();
    System.out.println("Test error log");

    try
    {
      Object foo = null;
      foo.toString();
    } catch (Exception e)
    {
      log.error(e.getMessage(), e);
    }

    System.out.println();
    System.out.println("Test warn log");
    try
    {
      Class.forName("com.cascadetg.NonexistantClass");
    } catch (Exception e)
  {
    log.warn("Can't find a non-existent class!");
  }

    System.out.println();
    System.out.println("Test info log");

    log.info("Starting app!");
    log.info("Quitting app!");

    System.out.println();
    System.out.println("Test debug log");

    if (1 &gt; 2)
    {
      log.debug("1 &gt; 2 evaluated true");
      if (10 % 2 == 0)
        log.debug("10 % 2 is 0");
      else
        log.debug("10 % 2 is not 0");
    } else
    {
      log.debug("1 &gt; 2 evaluated false");
    }

    System.out.println();
    System.out.println("Test trace log");

    log.trace("Calling trace method.");
    log.trace("Calling trace method.");
    log.trace("Calling trace method.");
    log.trace("Calling trace method.");
    log.trace("Calling trace method.");

    System.out.println();
    System.out.println("Log test complete.");

  }
}
&lt;/source&gt;&lt;ref name=":ajc_120"/&gt;

==See also==
{{Portal|Java|Free software}}
*[[log4j]]
*[[Chainsaw (log file viewer)]]
*{{GitHub|https://github.com/apache/commons-logging/commits?author=costinm}}

==References==
{{Reflist}}

==External links==
*{{Official website|https://commons.apache.org/proper/commons-logging/}}

{{Apache}}

[[Category:Apache Software Foundation|Commons Logging]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Log file formats]]
[[Category:Software using the Apache license]]</text>
      <sha1>qlov7cdgw0iwovf7fcizm141vxt2a82</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Hadoop</title>
    <ns>0</ns>
    <id>5919308</id>
    <revision>
      <id>845978062</id>
      <parentid>845876334</parentid>
      <timestamp>2018-06-15T12:49:54Z</timestamp>
      <contributor>
        <username>Montanabernese</username>
        <id>33949039</id>
      </contributor>
      <comment>New Section added: Difference between Hadoop 2 and Hadoop 3</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="63347">{{Multiple issues|
{{Advert|date=October 2013}}
{{Buzzword|date=October 2013}}
{{Technical|date=May 2017}}}}
{{Infobox software
| name                   = Apache Hadoop
| logo                   = [[File:Hadoop logo new.svg|frameless|Hadoop Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| released               = {{Start date and age|2011|12|10}}&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/releases.html#27+December%2C+2011%3A+release+1.0.0+available |title=Hadoop Releases &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-12-06}}&lt;/ref&gt;
| latest release version = 3.0.0
| latest release date    = {{release date|2017|12|13}}&lt;ref name="homepage" /&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Distributed file system]]
&lt;!--| posix compliant        = Not [[POSIX]]-compliant--&gt;
| license                = [[Apache License]] 2.0
| website                = {{Official URL}}
}}

'''Apache Hadoop''' ({{IPAc-en|pron|h|ə|ˈ|d|u:|p}}) is a collection of [[open source|open-source]] software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a [[software framework]] for [[Clustered file system|distributed storage]] and processing of [[big data]] using the [[MapReduce]] [[programming model]]. Originally designed for [[computer cluster]]s built from [[commodity hardware]]&lt;ref&gt;{{cite web |url=http://www.silicon.co.uk/workspace/doug-cutting-big-data-is-not-a-bubble-96694 |title=Doug Cutting: Big Data Is No Bubble |last=Judge |first=Peter |date=2012-10-22 |website=silicon.co.uk |access-date=2018-03-11}}&lt;/ref&gt;{{emdash}}still the common use{{emdash}}it has also found use on clusters of higher-end hardware.&lt;ref&gt;{{cite web |url=https://www.datanami.com/2014/05/12/hadoop-ibm-power/ |title=Why Hadoop on IBM Power |last=Woodie |first=Alex |date=2014-05-12 |website=datanami.com |publisher=Datanami |access-date=2018-03-11}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.hpcwire.com/2014/10/15/cray-launches-hadoop-hpc-airspace/ |title=Cray Launches Hadoop into HPC Airspace |last=Hemsoth |first=Nicole |date=2014-10-15 |website=hpcwire.com |access-date=2018-03-11}}&lt;/ref&gt; All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.&lt;ref name="homepage"&gt;{{Cite web|title= Welcome to Apache Hadoop!|url= http://hadoop.apache.org|website= hadoop.apache.org|accessdate = 2016-08-25}}&lt;/ref&gt;

The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers [[JAR (file format)|packaged code]] into nodes to process the data in parallel. This approach takes advantage of [[data locality]],&lt;ref&gt;{{cite web |url=http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ |title=What is the Hadoop Distributed File System (HDFS)? &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=ibm.com |publisher=[[IBM]] |accessdate=2014-10-30 }}&lt;/ref&gt; where nodes manipulate the data they have access to. This allows the dataset to be [[distributed processing|processed]] faster and more efficiently than it would be in a more conventional [[supercomputer architecture]] that relies on a [[parallel file system]] where computation and data are distributed via high-speed networking.&lt;ref&gt;{{cite web |url=http://www.datascienceassn.org/content/data-locality-hpc-vs-hadoop-vs-spark |title=Data Locality: HPC vs. Hadoop vs. Spark |last1=Malak |first1=Michael |date=2014-09-19 |website=datascienceassn.org |publisher=Data Science Association |accessdate=2014-10-30 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Characterization and Optimization of Memory-Resident MapReduce on HPC Systems|url=http://ieeexplore.ieee.org/document/6877311/|publisher=IEEE|format=pdf|date=October 2014}}&lt;/ref&gt;

The base Apache Hadoop framework is composed of the following modules:

* ''Hadoop Common'' – contains libraries and utilities needed by other Hadoop modules;
* ''Hadoop Distributed File System (HDFS)'' – a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster;
* ''Hadoop YARN'' – introduced in 2012 is a platform responsible for managing computing resources in clusters and using them for scheduling users' applications;&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/yarn/api/records/Resource.html#newInstance(int,%20int) |title=Resource (Apache Hadoop Main 2.5.1 API) &lt;!-- |author=Staff writer(s); no by-line.--&gt; |date=2014-09-12 |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-09-30 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/ |title=Apache Hadoop YARN – Concepts and Applications |last1=Murthy |first1=Arun |date=2012-08-15 |website=hortonworks.com |publisher=Hortonworks |accessdate=2014-09-30 }}&lt;/ref&gt; 
* ''Hadoop MapReduce'' – an implementation of the MapReduce programming model for large-scale data processing.

The term ''Hadoop'' has come to refer not just to the aforementioned base modules and sub-modules, but also to the ''ecosystem'',&lt;ref&gt;{{cite web |url=https://finance.yahoo.com/news/continuuity-raises-10-million-series-120500471.html |title=Continuuity Raises $10 Million Series A Round to Ignite Big Data Application Development Within the Hadoop Ecosystem &lt;!-- |author=Staff writer(s); no by-line.--&gt; |date=2012-11-14 |website=finance.yahoo.com |publisher=[[Marketwired]] |accessdate=2014-10-30 }}&lt;/ref&gt; or collection of additional software packages that can be installed on top of or alongside Hadoop, such as [[Pig (programming tool)|Apache Pig]], [[Apache Hive]], [[Apache HBase]], [[Apache Phoenix]], [[Apache Spark]], [[Apache ZooKeeper]], [[Cloudera Impala]], [[Apache Flume]], [[Apache Sqoop]], [[Apache Oozie]], and [[Apache Storm]].&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/ |title=Hadoop-related projects at |publisher=Hadoop.apache.org |accessdate=2013-10-17 }}&lt;/ref&gt;

Apache Hadoop's MapReduce and HDFS components were inspired by [[Google]] papers on their [[MapReduce]] and [[Google File System]].&lt;ref&gt;{{cite book &lt;!-- |author=Staff writer(s); no by-line.--&gt; |title=Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data |url=https://books.google.com/books?id=axruBQAAQBAJ&amp;pg=PA300|publisher=John Wiley &amp; Sons |page=300 |date=2014-12-19 |isbn=9781118876220 |access-date=2015-01-29 }}&lt;/ref&gt;

The Hadoop framework itself is mostly written in the [[Java (programming language)|Java programming language]], with some native code in [[C (programming language)|C]] and [[Command-line interface|command line]] utilities written as [[shell scripts]]. Though MapReduce Java code is common, any programming language can be used with "Hadoop Streaming" to implement the "map" and "reduce" parts of the user's program.&lt;ref&gt;{{cite web |url=http://www.mail-archive.com/nlpatumd@yahoogroups.com/msg00570.html |title=[nlpatumd&amp;#93; Adventures with Hadoop and Perl |publisher=Mail-archive.com |date=2010-05-02 |accessdate=2013-04-05 }}&lt;/ref&gt; Other projects in the Hadoop ecosystem expose richer user interfaces.

==History==
According to its co-founders, [[Doug Cutting]] and [[Mike Cafarella]], the genesis of Hadoop was the "Google File System" paper that was published in October 2003.&lt;ref&gt;{{Cite news|url=https://www.oreilly.com/ideas/the-next-10-years-of-apache-hadoop|title=The next 10 years of Apache Hadoop|last1=Cutting|first1=Mike|last2=Cafarella|first2=Ben|last3=Lorica|first3=Doug|date=2016-03-31|work=O'Reilly Media|access-date=2017-10-12|language=en}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://research.google.com/archive/gfs.html|title=The Google File System|first1=Sanjay|last1=Ghemawat|first2=Howard|last2=Gobioff|first3=Shun-Tak|last3=Leung}}&lt;/ref&gt; This paper spawned another one from Google{{snd}} "MapReduce: Simplified Data Processing on Large Clusters".&lt;ref&gt;{{cite web|url=http://research.google.com/archive/mapreduce.html|title=MapReduce: Simplified Data Processing on Large Clusters|first1=Jeffrey|last1=Dean|first2=Sanjay|last2=Ghemawat}}&lt;/ref&gt; Development started on the [[Apache Nutch]] project, but was moved to the new Hadoop subproject in January 2006.&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/browse/INFRA-700|title=new mailing lists request: hadoop|last=Cutting|first=Doug|date=28 Jan 2006|website=issues.apache.org|quote=The Lucene PMC has voted to split part of Nutch into a new sub-project named Hadoop}}&lt;/ref&gt; [[Doug Cutting]], who was working at [[Yahoo!]] at the time, named it after his son's toy elephant.&lt;ref&gt;{{cite news |title=Hadoop, a Free Software Program, Finds Uses Beyond Search |first=Ashlee |last=Vance |newspaper=The New York Times |date=2009-03-17 |url=https://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html |accessdate=2010-01-20 | archiveurl= https://web.archive.org/web/20110830130350/http://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html|archivedate=August 30, 2011&lt;!--DASHBot--&gt;| deadurl=no }}&lt;/ref&gt; The initial code that was factored out of Nutch consisted of about 5,000 lines of code for HDFS and about 6,000 lines of code for MapReduce.

The first committer to add to the Hadoop project was Owen O'Malley (in March 2006);&lt;ref&gt;{{cite mailing list|first=Doug|last=Cutting|author-link=Doug Cutting|title=[RESULT] VOTE: add Owen O'Malley as Hadoop committer|mailing-list=hadoop-common-dev|date=30 March 2006|url=http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/200603.mbox/%3C442B27A6.8080500@apache.org%3E}}&lt;/ref&gt; Hadoop 0.1.0 was released in April 2006.&lt;ref&gt;{{cite web|url=https://archive.apache.org/dist/hadoop/core/|title=Index of /dist/hadoop/core|website=archive.apache.org|accessdate=11 December 2017}}&lt;/ref&gt; It continues to evolve through the many contributions that are being made to the project.&lt;ref&gt;{{cite web|url=https://hadoop.apache.org/who.html|title=Who We Are|website=hadoop.apache.org|accessdate=11 December 2017}}&lt;/ref&gt;

===Timeline===
{| class="wikitable" width="100%"
! style="width:6%" | Year || style="width:10%" | Month || Event || Ref.
|-
|2003||October||[[Google File System]] paper released ||&lt;ref&gt;
{{cite web
| title = Google Research Publication: The Google File System
| url = http://research.google.com/archive/gfs.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2004||December||[[MapReduce]]: Simplified Data Processing on Large Clusters||&lt;ref&gt;
{{cite web
| title = Google Research Publication: MapReduce
| url = http://research.google.com/archive/mapreduce.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2006||January||Hadoop subproject created with mailing lists, jira, and wiki||&lt;ref&gt;
{{cite web
| title = [INFRA-700] new mailing lists request: hadoop – ASF JIRA
| url = https://issues.apache.org/jira/browse/INFRA-700
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2006||January||Hadoop is born from Nutch 197 ||&lt;ref&gt;
{{cite web
| title = [HADOOP-1] initial import of code from Nutch – ASF JIRA
| url = https://issues.apache.org/jira/browse/HADOOP-1
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2006||February||NDFS+ MapReduce moved out of Apache Nutch to create Hadoop||&lt;ref name="tom-white-book"&gt;{{cite book
| last =White
| first =Tom
| title =Hadoop: The Definitive Guide
| publisher =O'Reilly
| edition =3rd
| date =2012
| isbn = 9781449328917}}&lt;/ref&gt;
|-
|2006||February||Owen O'Malley's first patch goes into Hadoop||&lt;ref&gt;
{{cite web
| title = [NUTCH-197] NullPointerException in TaskRunner if application jar does not have "lib" directory – ASF JIRA
| url = https://issues.apache.org/jira/browse/NUTCH-197
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2006||February||Hadoop is named after Cutting's son's yellow plush toy||&lt;ref name="datanami.com"&gt;{{cite web
| title = From Spiders to Elephants: The History of Hadoop
| url = http://www.datanami.com/2015/04/15/from-spiders-to-elephants-the-history-of-hadoop/
| accessdate = 2016-03-09
 }}&lt;/ref&gt;
|-
|2006||April||Hadoop 0.1.0 released||&lt;ref&gt;
{{cite web
| title = Index of /dist/hadoop/core
| url = https://archive.apache.org/dist/hadoop/core/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2006||April||Hadoop sorts 1.8&amp;nbsp;TB on 188 nodes in 47.9 hours||&lt;ref name="tom-white-book" /&gt;
|-
|2006||May||Yahoo deploys 300 machine Hadoop cluster||&lt;ref name="tom-white-book" /&gt;
|-
|2006||October||Yahoo Hadoop cluster reaches 600 machines||&lt;ref name="tom-white-book" /&gt;
|-
|2007||April||Yahoo runs two clusters of 1,000 machines||&lt;ref name="tom-white-book" /&gt;
|-
|2007||June||Only three companies on "Powered by Hadoop Page"||&lt;ref name="powered-by-hadoop"&gt;
{{cite web
| title = Hadoop Summit 2009
| url = http://riccomini.name/posts/hadoop/2009-06-13-hadoop-summit-2009/|website=Riccomini.name
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2007||October||First release of Hadoop that includes HBase||&lt;ref&gt;
{{cite web
| title = Apache Hadoop Releases
| url = http://hadoop.apache.org/releases.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2007||October||Yahoo Labs creates Pig, and donates it to the ASF||&lt;ref&gt;{{cite book
| last =Gates
| first =Alan
| title =Programming Pig
| publisher =O'Reilly
| date =2011
| page =10
| isbn = 978-1-4493-0264-1}}&lt;/ref&gt;
|-
|2008||January||YARN JIRA opened||Yarn Jira (Mapreduce 279)
|-
|2008||January||20 companies on "Powered by Hadoop Page"||&lt;ref name="powered-by-hadoop" /&gt;
|-
|2008||February||Yahoo moves its web index onto Hadoop||&lt;ref&gt;
{{cite web
| title = Yahoo! Launches World's Largest Hadoop Production Application | work=hadoopnew –  Yahoo
| url = https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html | archive-url = https://web.archive.org/web/20130526091159/https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html | dead-url = yes | archive-date = 2013-05-26 | accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2008||February||Yahoo! production search index generated by a 10,000-core Hadoop cluster||&lt;ref name="tom-white-book" /&gt;
|-
|2008||March||First Hadoop Summit||&lt;ref&gt;
{{cite web
| title = RE: Hadoop summit / workshop at Yahoo!
| url = http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200802.mbox/%3CDD27C9769EA63D43BE3CF4CCCC12DD977ED49E@SNV-EXVS02.ds.corp.yahoo.com%3E
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2008||April||Hadoop world record fastest system to sort a terabyte of data. Running on a 910-node cluster, Hadoop sorted one terabyte in 209 seconds||&lt;ref name="tom-white-book" /&gt;
|-
|2008||May||Hadoop wins TeraByte Sort (World Record sortbenchmark.org)||&lt;ref&gt;{{cite web|url=http://sortbenchmark.org/YahooHadoop.pdf|format=PDF|title=TeraByte Sort on Apache Hadoop|website=Sortbenchmark.org|accessdate=11 December 2017}}&lt;/ref&gt;
|-
|2008||July||Hadoop wins Terabyte Sort Benchmark||&lt;ref&gt;
{{cite web
| title = Apache Hadoop Wins Terabyte Sort Benchmark | website=Developer.yahoo.com
| url = https://developer.yahoo.com/blogs/hadoop/apache-hadoop-wins-terabyte-sort-benchmark-408.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2008||October||Loading 10&amp;nbsp;TB/day in Yahoo clusters||&lt;ref name="tom-white-book" /&gt;
|-
|2008||October||Cloudera, Hadoop distributor is founded||&lt;ref&gt;
{{cite web
| title = Cloudera |website=Crunchbase.com
| url = https://www.crunchbase.com/organization/cloudera#/entity
| accessdate = 2016-03-09
 }}&lt;/ref&gt;
|-
|2008||November||Google MapReduce implementation sorted one terabyte in 68 seconds||&lt;ref name="tom-white-book" /&gt;
|-
|2009||March||Yahoo runs 17 clusters with 24,000 machines||&lt;ref name="tom-white-book" /&gt;
|-
|2009||April||Hadoop sorts a petabyte||&lt;ref&gt;{{cite web|url=http://sortbenchmark.org/Yahoo2009.pdf|format=PDF|title=Winning a 60 Second Dash with a Yellow Elephant|website=Sortbenchmark.org|accessdate=11 December 2017}}&lt;/ref&gt;
|-
|2009||May||Yahoo! used Hadoop to sort one terabyte in 62 seconds||&lt;ref name="tom-white-book" /&gt;
|-
|2009||June||Second Hadoop Summit||&lt;ref&gt;{{cite web|url=http://www.mollynix.com/images_content/01commdes/hadoopschedulepdf.pdf|format=PDF|title=Events &amp; Media|website=Mollynix.com|accessdate=11 December 2017}}&lt;/ref&gt;
|-
|2009||July||Hadoop Core is renamed Hadoop Common||&lt;ref name="hadoop-apache-org"&gt;
{{cite web
| title = Welcome to Apache™ Hadoop®!|website=Hadoop.apache.org
| url = http://hadoop.apache.org/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2009||July||MapR, Hadoop distributor founded||&lt;ref&gt;
{{cite web
| title = MapR Technologies|website=Crunchbase.com
| url = https://www.crunchbase.com/organization/mapr-technologies#/entity
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2009||July||HDFS now a separate subproject||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2009||July||MapReduce now a separate subproject||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2010||January||Kerberos support added to Hadoop||&lt;ref&gt;{{cite web |url=https://thinkbiganalytics.com/yahoo-updates-from-hadoop-summit-2010/ |title=Yahoo! Updates from Hadoop Summit 2010 |publisher=Think Big Analytics |accessdate=April 25, 2016 |quote=Baldeschwieler announced that Yahoo has released a beta test of Hadoop Security, which uses Kerberos for authentication and allows colocation of business sensitive data within the same cluster.}}&lt;/ref&gt;
|-
|2010||May||Apache HBase Graduates||&lt;ref&gt;
{{cite web
| title = Apache HBase – Apache HBase™ Home
| url = http://hbase.apache.org/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2010||June||Third Hadoop Summit||&lt;ref&gt;
{{cite web
| title = Hadoop Summit 2010 – Agenda is available! | work=hadoopnew –  Yahoo
| url = https://developer.yahoo.com/blogs/hadoop/hadoop-summit-2010-agenda-available-455.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2010||June||Yahoo 4,000 nodes/70 petabytes ||&lt;ref name="hadoop-summit-2010"&gt;
{{cite web
| title = Hadoop Summit 2010
| url = http://perspectives.mvdirona.com/2010/07/hadoop-summit-2010/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2010||June||Facebook 2,300 clusters/40 petabytes ||&lt;ref name="hadoop-summit-2010" /&gt;
|-
|2010||September||Apache Hive Graduates||&lt;ref&gt;
{{cite web
| title = Apache Hive TM
| url = http://hive.apache.org/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2010||September||Apache Pig Graduates||&lt;ref&gt;
{{cite web
| title = Welcome to Apache Pig!
| url = http://pig.apache.org/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2011||January||Apache Zookeeper Graduates ||&lt;ref&gt;
{{cite web
| title = Apache ZooKeeper – Home
| url = http://zookeeper.apache.org/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2011||January||Facebook, LinkedIn, eBay and IBM collectively contribute 200,000 lines of code ||&lt;ref name="contributions-to-apache-hadoop"&gt;
{{cite web
| title = Reality Check: Contributions to Apache Hadoop&amp;nbsp;— Hortonworks
| url = http://hortonworks.com/blog/reality-check-contributions-to-apache-hadoop/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2011||March||Apache Hadoop takes top prize at Media Guardian Innovation Awards||&lt;ref&gt;
{{cite web
| title = Apache Hadoop takes top prize at Media Guardian Innovation Awards |work= The Guardian
| url = https://www.theguardian.com/technology/2011/mar/25/media-guardian-innovation-awards-apache-hadoop
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2011||June||Rob Beardon and Eric Badleschieler spin out Hortonworks out of Yahoo.||&lt;ref name="history-of-hadoop"&gt;
{{cite web
| title = The history of Hadoop: From 4 nodes to the future of data | publisher=Gigaom
| url = https://gigaom.com/2013/03/04/the-history-of-hadoop-from-4-nodes-to-the-future-of-data/
| accessdate = 2016-03-09
| first = Derrick
| last = Harris
 }}
&lt;/ref&gt;
|-
|2011||June||Yahoo has 42K Hadoop nodes and hundreds of petabytes of storage||&lt;ref name="history-of-hadoop" /&gt;
|-
|2011||June||Third Annual Hadoop Summit (1,700 attendees)||&lt;ref&gt;
{{cite web
| title = Hadoop Summit 2011: June 29th, Santa Clara Convention Center | work= hadoopnew&amp;nbsp;–  Yahoo
| url = https://developer.yahoo.com/blogs/hadoop/hadoop-summit-2011-june-29th-santa-clara-convention-5061.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2011||October||Debate over which company had contributed more to Hadoop.||&lt;ref name="contributions-to-apache-hadoop" /&gt;
|-
|2012||January||Hadoop community moves to separate from MapReduce and replace with YARN||&lt;ref name="datanami.com"/&gt;
|-
|2012||June||San Jose Hadoop Summit (2,100 attendees)||&lt;ref&gt;
{{cite web
| title = Fifth Annual Hadoop Summit 2012 Kicks Off with Record Attendance – Hortonworks
| url = http://hortonworks.com/press-releases/fifth-annual-hadoop-summit-2012-kicks-off-with-record-attendance/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2012||November||Apache Hadoop 1.0 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2013||March||Hadoop Summit{{snd}} Amsterdam (500 attendees)||&lt;ref&gt;
{{cite web
| title = Hadoop Summit 2013 Amsterdam – It's A Wrap! – Hortonworks
| url = http://hortonworks.com/blog/hadoop-summit-2013-amsterdam-its-a-wrap/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2013||March||YARN deployed in production at Yahoo||&lt;ref&gt;
{{cite web
| title = Hadoop at Yahoo!: More Than Ever Before
| url = https://developer.yahoo.com/blogs/ydn/hadoop-yahoo-more-ever-54421.html
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2013||June||San Jose Hadoop Summit (2,700 attendees)||&lt;ref&gt;
{{cite web
| title = Hadoop Summit North America 2013 Draws Record Ecosystem Support | work= Business Wire
| url = http://www.businesswire.com/news/home/20130610006449/en/Hadoop-Summit-North-America-2013-Draws-Record
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2013||October||Apache Hadoop 2.2 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2014||February||Apache Hadoop 2.3 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2014||February||Apache Spark top Level Apache Project||&lt;ref&gt;
{{cite web
| title = The Apache Software Foundation Announces Apache™ Spark™ as a Top-Level Project : The Apache Software Foundation Blog
| url = https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2014||April||Hadoop summit Amsterdam (750 attendees)||&lt;ref&gt;
{{cite web
| title = Loved Hadoop Summit Europe 2014 – Hope you did too! – SAP HANA
| url = https://blogs.saphana.com/2014/04/14/loved-hadoop-summit-europe-2014-hope-you-did-too/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2014||June||Apache Hadoop 2.4 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2014||June||San Jose Hadoop Summit (3,200 attendees)||&lt;ref&gt;
{{cite web
| title = Hadoop Summit 2014 – Big Data Keeps Getting Bigger |publisher=Pentaho
| url = http://www.pentaho.com/blog/2014/06/06/hadoop-summit-2014-big-data-keeps-getting-bigger
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2014||August||Apache Hadoop 2.5 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2014||November||Apache Hadoop 2.6 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2015||April||Hadoop Summit Europe||&lt;ref&gt;
{{cite web
| title = Hadoop Summit Europe 2015, 15th–16th April 2015 |publisher= Lanyrd
| url = http://lanyrd.com/2015/hadoopsummit/
| accessdate = 2016-03-09
 }}
&lt;/ref&gt;
|-
|2015||June||Apache Hadoop 2.7 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2017||March||Apache Hadoop 2.8 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2017||November||Apache Hadoop 2.9 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2017||December||Apache Hadoop 3.0 Available||&lt;ref name="hadoop-apache-org" /&gt;
|-
|2018||April||Apache Hadoop 3.1 Available||&lt;ref name="hadoop-apache-org" /&gt;
|}

==Architecture==
{{See also|#Hadoop_distributed_file_system|Apache HBase|MapReduce|l1=Hadoop Distributed File System}}

Hadoop consists of the ''Hadoop Common'' package, which provides file system and operating system level abstractions, a MapReduce engine (either MapReduce/MR1 or YARN/MR2)&lt;ref&gt;{{cite web |url=http://blog.cloudera.com/blog/2012/10/mr2-and-yarn-briefly-explained/ |title=MR2 and YARN Briefly Explained |first=Harsh |last=Chouraria |date=21 October 2012 |website=Cloudera.com|accessdate=23 October 2013 }}&lt;/ref&gt; and the [[#Hadoop distributed file system|Hadoop Distributed File System]] (HDFS). The Hadoop Common package contains the [[JAR (file format)|Java ARchive (JAR)]] files and scripts needed to start Hadoop.

For effective scheduling of work, every Hadoop-compatible file system should provide location awareness – the name of the rack (or, more precisely, of the network switch) where a worker node is. Hadoop applications can use this information to execute code on the node where the data is, and, failing that, on the same rack/switch to reduce backbone traffic. HDFS uses this method when replicating data for data redundancy across multiple racks. This approach reduces the impact of a rack power outage or switch failure; if any of these hardware failures occurs, the data will remain available.&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html |title=HDFS User Guide |publisher=Hadoop.apache.org |accessdate=2014-09-04 }}&lt;/ref&gt;
[[File:Hadoop 1.png|thumb|upright=1.2|right|alt=Hadoop cluster|A multi-node Hadoop cluster]]

A small Hadoop cluster includes a single master and multiple worker nodes. The master node consists of a Job Tracker, Task Tracker, NameNode, and DataNode. A slave or ''worker node'' acts as both a DataNode and TaskTracker, though it is possible to have data-only and compute-only worker nodes. These are normally used only in nonstandard applications.&lt;ref name="michael-noll.com_2"&gt;{{cite web |title=Running Hadoop on Ubuntu Linux System(Multi-Node Cluster) |url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ }}&lt;/ref&gt;

Hadoop requires [[JRE|Java Runtime Environment]] (JRE) 1.6 or higher. The standard startup and shutdown scripts require that [[Secure Shell]] (SSH) be set up between nodes in the cluster.&lt;ref name="michael-noll.com_1"&gt;{{cite web |title=Running Hadoop on Ubuntu Linux (Single-Node Cluster) |url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#prerequisites |accessdate=6 June 2013 }}&lt;/ref&gt;

In a larger cluster, HDFS nodes are managed through a dedicated NameNode server to host the file system index, and a secondary NameNode that can generate snapshots of the namenode's memory structures, thereby preventing file-system corruption and loss of data. Similarly, a standalone JobTracker server can manage job scheduling across nodes. When Hadoop MapReduce is used with an alternate file system, the NameNode, secondary NameNode, and DataNode architecture of HDFS are replaced by the file-system-specific equivalents.

===File systems===
===={{Anchor|HDFS}}Hadoop distributed file system====
The HDFS is a distributed, scalable, and portable [[distributed file system|file system]] written in Java for the Hadoop framework. Some consider it to instead be a [[Distributed data store|data store]] due to its lack of [[POSIX]] compliance,&lt;ref&gt;{{cite web |url=http://www.computerweekly.com/feature/Big-data-storage-Hadoop-storage-basics |title=Big data storage: Hadoop storage basics |last1=Evans |first1=Chris |date=Oct 2013 |website=computerweekly.com |publisher=[[Computer Weekly]] |access-date=21 June 2016 |quote=HDFS is not a file system in the traditional sense and isn't usually directly mounted for a user to view }}&lt;/ref&gt; but it does provide shell commands and Java application programming interface (API) [[Method (computer programming)|methods]] that are similar to other file systems.&lt;ref&gt;{{cite web |url=http://www.dummies.com/how-to/content/managing-files-with-the-hadoop-file-system-command.html |title=Managing Files with the Hadoop File System Commands |last1=deRoos |first1=Dirk |website=dummies.com |publisher=[[For Dummies]] |access-date=21 June 2016}}&lt;/ref&gt; A Hadoop cluster has nominally a single namenode plus a cluster of datanodes, although [[redundancy (engineering)|redundancy]] options are available for the namenode due to its criticality. Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses [[TCP/IP]] [[Internet socket|sockets]] for communication. Clients use [[remote procedure call]]s (RPC) to communicate with each other.

HDFS stores large files (typically in the range of gigabytes to terabytes&lt;ref&gt;
{{cite web |title=HDFS Architecture |url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Large_Data_Sets |accessdate=1 September 2013 }}
&lt;/ref&gt;) across multiple machines. It achieves reliability by [[Replication (computer science)|replicating]] the data across multiple hosts, and hence theoretically does not require [[RAID|redundant array of independent disks (RAID)]] storage on hosts (but to increase input-output (I/O) performance some RAID configurations are still useful). With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully POSIX-compliant, because the requirements for a POSIX file-system differ from the target goals of a Hadoop application. The trade-off of not having a fully POSIX-compliant file-system is increased performance for data [[throughput]] and support for non-POSIX operations such as Append.&lt;ref name="openlibrary1"&gt;
{{Cite journal
|publisher = Amazon.com
|first = Yaniv |last = Pessach
|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations
|title = Distributed Storage
|edition = Distributed Storage: Concepts, Algorithms, and Implementations
|publication-date = 2013
|postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}
}}&lt;/ref&gt;

HDFS added the high-availability capabilities, as announced for version 2.0 in May 2012,&lt;ref name="failover"&gt;{{cite web |title=Version 2.0 provides for manual failover and they are working on automatic failover: |url=https://hadoop.apache.org/releases.html#23+May%2C+2012%3A+Release+2.0.0-alpha+available  |accessdate= 30 July 2013 |publisher=Hadoop.apache.org }}&lt;/ref&gt; letting the main metadata server (the NameNode) manually fail-over onto a backup. The project has also started developing automatic [[fail-over]]s.

The HDFS file system includes a so-called ''secondary namenode'', a misleading term that some might incorrectly interpret as a backup namenode when the primary namenode goes offline. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple namespaces served by separate namenodes. Moreover, there are some issues in HDFS such as small file issues, scalability problems, Single Point of Failure (SPoF), and bottlenecks in huge metadata requests.
One advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location. For example: if node A contains data (a, b, c) and node X contains data (x, y, z), the job tracker schedules node A to perform map or reduce tasks on (a, b, c) and node X would be scheduled to perform map or reduce tasks on (x, y, z). This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems, this advantage is not always available. This can have a significant impact on job-completion times as demonstrated with data-intensive jobs.&lt;ref&gt;{{cite web |url=
http://www.eng.auburn.edu/~xqin/pubs/hcw10.pdf |format=PDF |title= Improving MapReduce performance through data placement in heterogeneous Hadoop Clusters |date=April 2010 |publisher=Eng.auburn.ed }}&lt;/ref&gt;

HDFS was designed for mostly immutable files and may not be suitable for systems requiring concurrent write-operations.&lt;ref name="openlibrary1" /&gt;

HDFS can be [[Mount (computing)|mounted]] directly with a [[Filesystem in Userspace]] (FUSE) [[virtual file system]] on [[Linux]] and some other [[Unix]] systems.

File access can be achieved through the native Java API, the [[Thrift (protocol)|Thrift]] API (generates a client in a number of languages e.g. C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, [[Cocoa (API)|Cocoa]], Smalltalk, and [[OCaml]]), the [[command-line interface]], the HDFS-UI [[web application]] over [[HTTP]], or via 3rd-party network client libraries.&lt;ref&gt;{{cite web |url=https://wiki.apache.org/hadoop/MountableHDFS |title=Mounting HDFS |accessdate=2016-08-05 }}&lt;/ref&gt;

HDFS is designed for portability across various hardware platforms and for compatibility with a variety of underlying operating systems. The HDFS design introduces portability limitations that result in some performance bottlenecks, since the Java implementation cannot use features that are exclusive to the platform on which HDFS is running.&lt;ref&gt;{{cite web |url=
http://www.jeffshafer.com/publications/papers/shafer_ispass10.pdf |format=PDF |title=The Hadoop Distributed Filesystem: Balancing Portability and Performance |last1=Shafer |first1=Jeffrey |last2=Rixner |first2=Scott |last3=Cox |first3=Alan |publisher=Rice University| accessdate=2016-09-19 }}&lt;/ref&gt; Due to its widespread integration into enterprise-level infrastructure, monitoring HDFS performance at scale has become an increasingly important issue. Monitoring end-to-end performance requires tracking metrics from datanodes, namenodes, and the underlying operating system.&lt;ref&gt;{{cite web |url=
https://www.datadoghq.com/blog/monitor-hadoop-metrics/#toc-hdfs-metrics2 |title=How to Collect Hadoop Performance Metrics |last1=Mouzakitis |first1=Evan| accessdate=2016-10-24 }}&lt;/ref&gt; There are currently several monitoring platforms to track HDFS performance, including HortonWorks, Cloudera, and Datadog.

====Other file systems====
Hadoop works directly with any distributed file system that can be mounted by the underlying operating system by simply using a &lt;code&gt;file://&lt;/code&gt; URL; however, this comes at a price – the loss of locality. To reduce network traffic, Hadoop needs to know which servers are closest to the data, information that Hadoop-specific file system bridges can provide.

In May 2011, the list of supported file systems bundled with Apache Hadoop were:

* HDFS: Hadoop's own rack-aware file system.&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Rack_Awareness |title=HDFS Users Guide&amp;nbsp;– Rack Awareness |publisher=Hadoop.apache.org |accessdate=2013-10-17 }}&lt;/ref&gt; This is designed to scale to tens of petabytes of storage and runs on top of the file systems of the underlying [[operating system]]s.
* [[FTP]] file system: This stores all its data on remotely accessible FTP servers.
* [[Amazon Simple Storage Service|Amazon S3 (Simple Storage Service)]] object storage: This is targeted at clusters hosted on the [[Amazon Elastic Compute Cloud]] server-on-demand infrastructure. There is no rack-awareness in this file system, as it is all remote.
* [http://azure.microsoft.com/en-us/documentation/articles/hdinsight-use-blob-storage Windows Azure Storage Blobs (WASB)] file system: This is an extension of HDFS that allows distributions of Hadoop to access data in Azure blob stores without moving the data permanently into the cluster.

A number of third-party file system bridges have also been written, none of which are currently in Hadoop distributions. However, some commercial distributions of Hadoop ship with an alternative file system as the default{{snd}}specifically IBM and MapR.

* In 2009, [[IBM]] discussed running Hadoop over the [[IBM General Parallel File System]].&lt;ref&gt;{{cite web |url=http://www.usenix.org/events/hotcloud09/tech/full_papers/ananthanarayanan.pdf |title= Cloud analytics: Do we really need to reinvent the storage stack? |date=June 2009 |publisher=IBM }}&lt;/ref&gt; The source code was published in October 2009.&lt;ref&gt;{{cite web |url=https://issues.apache.org/jira/browse/HADOOP-6330 |title=HADOOP-6330: Integrating IBM General Parallel File System implementation of Hadoop Filesystem interface |date=2009-10-23 |publisher=IBM }}&lt;/ref&gt;
* In April 2010, [[Parascale]] published the source code to run Hadoop against the Parascale file system.&lt;ref&gt;{{cite web |url=https://issues.apache.org/jira/browse/HADOOP-6704 |title=HADOOP-6704: add support for Parascale filesystem |date=2010-04-14 |publisher=Parascale }}&lt;/ref&gt;
* In April 2010, [[Appistry]] released a Hadoop file system driver for use with its own CloudIQ Storage product.&lt;ref&gt;{{cite web |url=http://resources.appistry.com/news-and-events/press/06072010-appistry-cloudiq-storage-now-generally-available |title=HDFS with CloudIQ Storage |date=2010-07-06 |publisher=Appistry,Inc. }}&lt;/ref&gt;
* In June 2010, [[Hewlett-Packard|HP]] discussed a location-aware [[IBRIX Fusion]] file system driver.&lt;ref&gt;{{cite web |url=http://www.slideshare.net/steve_l/high-availability-hadoop |title=High Availability Hadoop |date=2010-06-09 |publisher=HP }}&lt;/ref&gt;
* In May 2011, [[MapR|MapR Technologies Inc.]] announced the availability of an alternative file system for Hadoop, [[MapR FS]], which replaced the HDFS file system with a full random-access read/write file system.

===JobTracker and TaskTracker: the MapReduce engine===
{{Main|MapReduce}}

Atop the file systems comes the [[MapReduce]] Engine, which consists of one ''JobTracker'', to which client applications submit MapReduce jobs. The JobTracker pushes work to available ''TaskTracker'' nodes in the cluster, striving to keep the work as close to the data as possible. With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack. This reduces network traffic on the main backbone network. If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns a separate [[Java virtual machine]] (JVM) process to prevent the TaskTracker itself from failing if the running job crashes its JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by [[Jetty (web server)|Jetty]] and can be viewed from a web browser.

Known limitations of this approach are:

# The allocation of work to TaskTrackers is very simple. Every TaskTracker has a number of available ''slots'' (such as "4 slots"). Every active map or reduce task takes up one slot. The Job Tracker allocates work to the tracker nearest to the data with an available slot. There is no consideration of the current [[load (computing)|system load]] of the allocated machine, and hence its actual availability.
# If one TaskTracker is very slow, it can delay the entire MapReduce job{{snd}} especially towards the end, when everything can end up waiting for the slowest task. With speculative execution enabled, however, a single task can be executed on multiple slave nodes.

====Scheduling====
By default Hadoop uses [[FIFO (computing and electronics)|FIFO]] scheduling, and optionally 5 scheduling priorities to schedule jobs from a work queue.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/common/docs/current/commands_manual.html|title=Commands Guide|date=17 August 2011|website=Web.archive.org|accessdate=11 December 2017|deadurl=bot: unknown|archiveurl=https://web.archive.org/web/20110817053520/http://hadoop.apache.org/common/docs/current/commands_manual.html#job|archivedate=17 August 2011|df=}}&lt;/ref&gt; In version 0.19 the job scheduler was refactored out of the JobTracker, while adding the ability to use an alternate scheduler (such as the ''Fair scheduler'' or the ''Capacity scheduler'', described next).&lt;ref&gt;{{cite web |title=Refactor the scheduler out of the JobTracker |url=https://issues.apache.org/jira/browse/HADOOP-3412 |work=Hadoop Common |publisher=Apache Software Foundation |accessdate=9 June 2012 }}&lt;/ref&gt;

=====Fair scheduler=====
The fair scheduler was developed by [[Facebook]].&lt;ref&gt;{{cite web |url=http://www.ibm.com/developerworks/library/os-hadoop-scheduling/ |title=Scheduling in Hadoop |first=M. Tim |last=Jones |date=6 December 2011 |website=ibm.com |publisher=[[IBM]] |accessdate=20 November 2013 }}&lt;/ref&gt; The goal of the fair scheduler is to provide fast response times for small jobs and [[Quality of service]] (QoS) for production jobs. The fair scheduler has three basic concepts.&lt;ref&gt;{{cite web|url=https://svn.apache.org/repos/asf/hadoop/common/branches/MAPREDUCE-233/src/contrib/fairscheduler/designdoc/fair_scheduler_design_doc.pdf|title=Hadoop Fair Scheduler Design Document|website=apache.org|accessdate=12 October 2017}}&lt;/ref&gt;
# Jobs are grouped into [[Pool (computer science)|pools]].
# Each pool is assigned a guaranteed minimum share.
# Excess capacity is split between jobs.

By default, jobs that are uncategorized go into a default pool. Pools have to specify the minimum number of map slots, reduce slots, as well as a limit on the number of running jobs.

=====Capacity scheduler=====
The capacity scheduler was developed by Yahoo. The capacity scheduler supports several features that are similar to those of the fair scheduler.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/docs/stable1/capacity_scheduler.html|title=CapacityScheduler Guide|website=Hadoop.apache.org|accessdate=31 December 2015}}&lt;/ref&gt;

# Queues are allocated a fraction of the total resource capacity.
# Free resources are allocated to queues beyond their total capacity.
# Within a queue, a job with a high level of priority has access to the queue's resources.

There is no [[preemption (computing)|preemption]] once a job is running.

===Difference between Hadoop 1 vs Hadoop 2 (YARN)===
The biggest difference between Hadoop 1 and Hadoop 2 is YARN technology. In the first version of Hadoop, the core components included Hadoop Common, HDFS, and MapReduce, but the second version of Hadoop came out with a new technology called YARN which was an acronym for '''Yet Another Resource Negotiator (YARN)'''.

It is an open source resource management technology which is deployed on a Hadoop cluster. YARN strives to allocate the resources to various applications effectively. It runs two daemons, which take care of two different tasks: ''job tracking'' and ''progress monitoring''.

These two daemons are called the ''resource manager'' and the ''application master'' respectively. The resource manager allocates resources to various applications, and the application master monitors the execution of the process.

===Difference between Hadoop 2 vs Hadoop 3===
There are important features provided by Hadoop 3. For example, while there is one single ''namenode'' in Hadoop 2, Hadoop 3 enables having multiple name nodes, which solves the single point of failure problem.  

In Hadoop 3, there are containers working in principle of [[Docker_(software)|Docker]], which reduces time spent on application development.

One of the biggest changes is that Hadoop 3 decreases storage overhead with erasure coding. 

Also, Hadoop 3 permits usage of GPU hardware within the cluster, which is a very substantial benefit to execute Deep Learning algorithms on a Hadoop cluster.&lt;ref&gt;{{cite web |url=https://it.hortonworks.com/blog/hadoop-3-adds-value-hadoop-2/ |title=How Apache Hadoop 3 Adds Value Over Apache Hadoop 2 |website=hortonworks.com |accessdate=2018-06-11}}&lt;/ref&gt;

===Other applications===
The HDFS file system is not restricted to MapReduce jobs. It can be used for other applications, many of which are under development at Apache. The list includes the [[HBase]] database, the [[Apache Mahout]] [[machine learning]] system, and the [[Apache Hive]] [[Data Warehouse]] system. Hadoop can, in theory, be used for any sort of work that is batch-oriented rather than real-time, is very data-intensive, and benefits from parallel processing of data. It can also be used to complement a real-time system, such as [[lambda architecture]], Apache Storm, Flink and Spark Streaming.&lt;ref&gt;{{cite web |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7530084 |format=PDF |title= Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming |date=May 2016 |publisher=IEEE }}&lt;/ref&gt;

{{As of|2009|10}}, commercial applications of Hadoop&lt;ref&gt;{{cite web |date=10 October 2009 |url=http://www.dbms2.com/2009/10/10/enterprises-using-hadoo/ |title="How 30+ enterprises are using Hadoop", in DBMS2 |publisher=Dbms2.com |accessdate=2013-10-17 }}&lt;/ref&gt; included:-

* log and/or clickstream analysis of various kinds
* marketing analytics
* machine learning and/or sophisticated data mining
* image processing
* processing of XML messages
* web crawling and/or text processing
* general archiving, including of relational/tabular data, e.g. for compliance

==Prominent use cases==

On February 19, 2008, Yahoo! Inc. launched what they claimed was the world's largest Hadoop production application. The Yahoo! Search Webmap is a Hadoop application that runs on a Linux cluster with more than 10,000 [[Multi-core|cores]] and produced data that was used in every Yahoo! web search query.&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html|title=Yahoo! Launches World's Largest Hadoop Production Application|date=19 February 2008|work=Yahoo|accessdate=31 December 2015}}&lt;/ref&gt; There are multiple Hadoop clusters at Yahoo! and no HDFS file systems or MapReduce jobs are split across multiple data centers. Every Hadoop cluster node bootstraps the Linux image, including the Hadoop distribution. Work that the clusters perform is known to include the index calculations for the Yahoo! search engine. In June 2009, Yahoo! made the source code of its Hadoop version available to the open-source community.&lt;ref&gt;{{cite web |url=http://developer.yahoo.com/hadoop/ |title=Hadoop and Distributed Computing at Yahoo! |publisher=Yahoo! |date=2011-04-20 |accessdate=2013-10-17 }}&lt;/ref&gt;

In 2010, Facebook claimed that they had the largest Hadoop cluster in the world with 21 [[Petabyte|PB]] of storage.&lt;ref&gt;{{cite web |url=http://hadoopblog.blogspot.com/2010/05/facebook-has-worlds-largest-hadoop.html |title=HDFS: Facebook has the world's largest Hadoop cluster! |publisher=Hadoopblog.blogspot.com |date=2010-05-09 |accessdate=2012-05-23 }}&lt;/ref&gt; In June 2012, they announced the data had grown to 100 PB&lt;ref&gt;{{cite web |url=http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920 |title=Under the Hood: Hadoop Distributed File system reliability with Namenode and Avatarnode |publisher=Facebook |accessdate=2012-09-13 }}&lt;/ref&gt; and later that year they announced that the data was growing by roughly half a PB per day.&lt;ref&gt;{{cite web |url=https://www.facebook.com/notes/facebook-engineering/under-the-hood-scheduling-mapreduce-jobs-more-efficiently-with-corona/10151142560538920 |title=Under the Hood: Scheduling MapReduce jobs more efficiently with Corona |publisher=Facebook |accessdate=2012-11-09 }}&lt;/ref&gt;

{{As of|2013}}, Hadoop adoption had become widespread: more than half of the Fortune 50 used Hadoop.&lt;ref&gt;{{cite press release &lt;!-- |author=Staff writer(s); no by-line.--&gt; |title=Altior's AltraSTAR – Hadoop Storage Accelerator and Optimizer Now Certified on CDH4 (Cloudera's Distribution Including Apache Hadoop Version 4) |url=http://www.prnewswire.com/news-releases/altiors-altrastar---hadoop-storage-accelerator-and-optimizer-now-certified-on-cdh4-clouderas-distribution-including-apache-hadoop-version-4-183906141.html |location=Eatontown, NJ |publisher=Altior Inc. |date=2012-12-18 |accessdate=2013-10-30 }}&lt;/ref&gt;

==Hadoop hosting in the cloud==
Hadoop can be deployed in a traditional onsite datacenter as well as in [[Cloud computing|the cloud]].&lt;ref&gt;{{cite web|url=http://azure.microsoft.com/en-us/solutions/hadoop/|title=Hadoop - Microsoft Azure|website=azure.microsoft.com|accessdate=11 December 2017}}&lt;/ref&gt; The cloud allows organizations to deploy Hadoop without the need to acquire hardware or specific setup expertise.&lt;ref&gt;{{cite web |url=http://azure.microsoft.com/en-us/solutions/hadoop/ |title=Hadoop |publisher=Azure.microsoft.com |accessdate=2014-07-22 }}&lt;/ref&gt; Vendors who currently have an offer for the cloud include [[Microsoft]], [[Amazon.com|Amazon]], [https://www.ctl.io/ CenturyLink Cloud] [[IBM|, IBM]],&lt;ref&gt;{{cite web|url=http://www-03.ibm.com/software/products/en/ibm-biginsights-on-cloud|title=IBM BigInsights on Cloud|date=1 January 2016|website=03.ibm.com|accessdate=11 December 2017}}&lt;/ref&gt; Google and [[Oracle Corporation|Oracle]].&lt;ref&gt;{{cite web |url=http://searchoracle.techtarget.com/feature/Oracles-cloud-analytics-platform-comprises-several-tools|title=Oracle's cloud analytics platform comprises several tools|accessdate=8 April 2016}}&lt;/ref&gt;

===On Microsoft Azure===
Azure HDInsight&lt;ref name="azure.microsoft.com"&gt;{{cite web |url=http://azure.microsoft.com/en-us/services/hdinsight/ |title=HDInsight &amp;#124; Cloud Hadoop |publisher=Azure.microsoft.com |accessdate=2014-07-22 }}&lt;/ref&gt; is a service that deploys Hadoop on [[Microsoft Azure]]. HDInsight uses Hortonworks HDP and was jointly developed for HDI with Hortonworks. HDI allows programming extensions with .NET (in addition to Java). HDInsight also supports the creation of Hadoop clusters using Linux with Ubuntu.&lt;ref name="azure.microsoft.com" /&gt; By deploying HDInsight in the cloud, organizations can spin up the number of nodes they want and only get charged for the compute and storage that is used.&lt;ref name="azure.microsoft.com" /&gt; [[Hortonworks]] implementations can also move data from the on-premises datacenter to the cloud for backup, development/test, and bursting scenarios.&lt;ref name="azure.microsoft.com" /&gt;
It is also possible to run Cloudera or Hortonworks Hadoop clusters on Azure Virtual Machines.

===On Amazon EC2/S3 services===
It is possible to run Hadoop on [[Amazon Elastic Compute Cloud]] (EC2) and [[Amazon Simple Storage Service]] (S3).&lt;ref&gt;{{cite web |last=Varia |first=Jinesh (@jinman) |title=Taking Massive Distributed Computing to the Common Man – Hadoop on Amazon EC2/S3 |url=http://aws.typepad.com/aws/2008/02/taking-massive.html |work=Amazon Web Services Blog |publisher=Amazon.com |accessdate=9 June 2012 }}&lt;/ref&gt; As an example, [[The New York Times]] used in 2007 100 Amazon EC2 instances and a Hadoop application to process 4&amp;nbsp;TB of raw image [[TIFF]] data (stored in S3) into 11 million finished [[PDF]]s in the space of 24 hours at a computation cost of about $240 (not including bandwidth).&lt;ref&gt;{{cite news |url=https://open.blogs.nytimes.com/2007/11/01/self-service-prorated-super-computing-fun/ |work=The New York Times |title=Self-service, Prorated Super Computing Fun! |first=Derek |last=Gottfrid |date=1 November 2007 |accessdate=4 May 2010}}&lt;/ref&gt;
&lt;!-- Really article is from 2007 and Hadoop if from 2011 --&gt;

There is support for the S3 object store in the Apache Hadoop releases, though this is below what one expects from a traditional POSIX filesystem. Specifically, operations such as rename() and delete() on directories are not atomic, and can take time proportional to the number of entries and the amount of data in them.

===On Amazon Elastic MapReduce===
Elastic MapReduce (EMR)&lt;ref&gt;{{cite web |url=http://aws.amazon.com/elasticmapreduce/ |title=AWS &amp;#124; Amazon Elastic MapReduce (EMR) &amp;#124; Hadoop MapReduce in the Cloud |publisher=Aws.amazon.com |accessdate=2014-07-22 }}&lt;/ref&gt; was introduced by Amazon.com in April 2009. Provisioning of the Hadoop cluster, running and terminating jobs, and handling data transfer between EC2(VM) and S3(Object Storage) are automated by Elastic MapReduce. Apache Hive, which is built on top of Hadoop for providing data warehouse services, is also offered in Elastic MapReduce.&lt;ref&gt;{{cite web |url=http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf |title=Amazon Elastic MapReduce Developer Guide |format=PDF |accessdate=2013-10-17 }}&lt;/ref&gt; Support for using Spot Instances&lt;ref&gt;{{cite web |url=http://aws.amazon.com/ec2/spot-instances/ |title=Amazon EC2 Spot Instances |publisher=Aws.amazon.com |accessdate=2014-07-22 }}&lt;/ref&gt; was later added in August 2011.&lt;ref&gt;{{cite web |url=http://aws.amazon.com/about-aws/whats-new/2011/08/18/amazon-elastic-mapreduce-now-supports-spot-instances/ |title=Amazon Elastic MapReduce Now Supports Spot Instances |publisher=Amazon.com |date=2011-08-18 |accessdate=2013-10-17 }}&lt;/ref&gt; Elastic MapReduce is fault-tolerant for slave failures,&lt;ref&gt;{{cite web |url=http://aws.amazon.com/elasticmapreduce/faqs/#cluster-10 |title=Amazon Elastic MapReduce FAQs |publisher=Amazon.com |accessdate=2013-10-17 }}&lt;/ref&gt; and it is recommended to only run the Task Instance Group on spot instances to take advantage of the lower cost while maintaining availability.&lt;ref&gt;{{Youtube |id=66rfnFA0jpM |title=Using Spot Instances with EMR }}&lt;/ref&gt;

===On CenturyLink Cloud (CLC)===
CenturyLink Cloud&lt;ref&gt;{{cite web|url=http://ctl.io|title=Cloud Computing Services and Managed Services – CenturyLink Cloud|website=Ctl.io|accessdate=11 December 2017}}&lt;/ref&gt; offers Hadoop via both a managed and un-managed model.&lt;ref&gt;{{cite web|url=https://www.ctl.io/managed-services/cloudera/|title=Managed Cloudera|website=Ctl.io|accessdate=11 December 2017}}&lt;/ref&gt; CLC also offers customers several managed Cloudera Blueprints, the newest managed service in the CenturyLink Cloud big data portfolio, which also includes Cassandra and MongoDB solutions.&lt;ref&gt;{{cite web|url=https://www.ctl.io/blog/post/hadoop-simplified-managed-cloudera-centurylink-cloud/|title=Hadoop Simplified: Managed Cloudera|website=Ctl.io|accessdate=11 December 2017}}&lt;/ref&gt;

===On Google Cloud Platform===
There are multiple ways to run the Hadoop ecosystem on [[Google Cloud Platform]] ranging from self-managed to Google-managed.&lt;ref&gt;{{cite web|url=https://cloud.google.com/hadoop/|title=Apache Spark and Apache Hadoop on Google Cloud Platform Documentation  -  Apache Hadoop on Google Cloud Platform|website=Google Cloud Platform|accessdate=11 December 2017}}&lt;/ref&gt;
* [[Google Cloud Dataproc]]: a managed Spark and Hadoop service&lt;ref&gt;{{cite web|url=https://cloud.google.com/dataproc/|title=Cloud Dataproc - Cloud-native Hadoop &amp; Spark  -  Google Cloud Platform|website=Google Cloud Platform|accessdate=11 December 2017}}&lt;/ref&gt;
* [https://cloud.google.com/hadoop/bdutil command line tools (bdutil)]: a collection of shell scripts to manually create and manage Spark and Hadoop clusters&lt;ref&gt;{{cite web|url=https://cloud.google.com/hadoop/setting-up-a-hadoop-cluster|title=Quickstarts - Google Cloud Dataproc Documentation - Google Cloud Platform|website=Google Cloud Platform|accessdate=11 December 2017}}&lt;/ref&gt;
* third party Hadoop distributions:
** Cloudera – using the Cloudera Director Plugin for Google Cloud Platform&lt;ref&gt;{{cite web|url=https://vision.cloudera.com/cloudera-now-certified-on-google-cloud-platform/|title=Cloudera now Certified on Google Cloud Platform - Cloudera VISION|date=17 August 2015|website=Vision.cloudera.com|accessdate=11 December 2017}}&lt;/ref&gt;
** Hortonworks – using bdutil support for Hortonworks HDP&lt;ref&gt;{{cite web|url=http://hortonworks.com/blog/hdp-google-cloud-platform/|title=HDP on Google Cloud Platform|date=22 January 2015|website=Hortonworks.com|accessdate=11 December 2017}}&lt;/ref&gt;
** MapR – using bdutil support for MapR&lt;ref&gt;{{cite web|url=https://www.mapr.com/resources/mapr-google-cloud-platform|title=MapR Google Cloud Platform|website=Mapr.com|accessdate=11 December 2017}}&lt;/ref&gt;

Google also offers connectors for using other Google Cloud Platform products with Hadoop, such as a [https://www.mapr.com/resources/mapr-google-cloud-platform Google Cloud Storage connector] for using [[Google Storage|Google Cloud Storage]] and a [https://cloud.google.com/hadoop/bigquery-connector Google BigQuery connector] for using [[BigQuery|Google BigQuery]].

=== On Oracle Cloud Platform ===
Hadoop is available on [[Oracle Cloud Platform|Oracle Cloud]] through the [https://cloud.oracle.com/bigdata Oracle Big Data Cloud]. It provides big data services and deployment models. Big Data Cloud Service and Big Data SQL Cloud Service can be combined with Oracle’s analytics platform. These services can be deployed in Public Cloud and in Hybrid Cloud as Cloud At Customer.

==Commercial support==
&lt;!--
Please don't go overboard in marketing here, as it will only be edited out. Use external citations rather than press releases, and be aware of Wikipedia's rules regarding conflict of interest and external links, WP:COI and WP:EL specifically
--&gt;
A number of companies offer commercial implementations or support for Hadoop.&lt;ref&gt;{{cite web |url=http://gigaom.com/cloud/why-we-need-more-hadoop-innovation/ |title=Why the Pace of Hadoop Innovation Has to Pick Up |publisher=Gigaom.com |date=2011-04-25 |accessdate=2013-10-17 }}&lt;/ref&gt;

===Branding===
The Apache Software Foundation has stated that only software officially released by the Apache Hadoop Project can be called ''Apache Hadoop'' or ''Distributions of Apache Hadoop''.&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/Defining%20Hadoop |title=Defining Hadoop |publisher=Wiki.apache.org |date=2013-03-30 |accessdate=2013-10-17 }}&lt;/ref&gt; The naming of products and derivative works from other vendors and the term "compatible" are somewhat controversial within the Hadoop developer community.&lt;ref&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/hadoop-general/201105.mbox/%3C4DC91392.2010308@apache.org%3E |title=Defining Hadoop Compatibility: revisited |publisher=Mail-archives.apache.org |date=2011-05-10 |accessdate=2013-10-17 }}&lt;/ref&gt;

==Papers==
Some papers influenced the birth and growth of Hadoop and big data processing. Some of these are:
* Jeffrey Dean, Sanjay Ghemawat (2004) [https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/index.html MapReduce: Simplified Data Processing on Large Clusters], Google. This paper inspired Doug Cutting to develop an open-source implementation of the Map-Reduce framework. He named it Hadoop, after his son's toy elephant.
* Michael Franklin, Alon Halevy, David Maier (2005) [http://www.eecs.berkeley.edu/~franklin/Papers/dataspaceSR.pdf From Databases to Dataspaces: A New Abstraction for Information Management]. The authors highlight the need for storage systems to accept all data formats and to provide APIs for data access that evolve based on the storage system's understanding of the data.
* Fay Chang et al.&lt;!-- Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber --&gt; (2006) [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf Bigtable: A Distributed Storage System for Structured Data], Google.
* Robert Kallman et al.&lt;!-- Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alexander Rasin, Stanley Zdonik, Evan P. C. Jones, Samuel Madden, Michael Stonebraker, Yang Zhang, John Hugg, Daniel J. Abadi --&gt; (2008) [http://www.vldb.org/pvldb/1/1454211.pdf H-store: a high-performance, distributed main memory transaction processing system]

==See also==
{{Portal|Free software}}
* [[Apache Accumulo]] – Secure [[Bigtable]]&lt;ref&gt;{{cite web |url=https://accumulo.apache.org/1.4/user_manual/Security.html |title=Apache Accumulo User Manual: Security &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |accessdate=2014-12-03 }}&lt;/ref&gt;
* [[Apache Cassandra]] – A column-oriented database that supports access from Hadoop
* [[Apache CouchDB]] is a database that uses JSON for documents, JavaScript for MapReduce queries, and regular HTTP for an API
* [[Big data]]
* [[Cloud computing]]
* [[Data Intensive Computing]]
* [[HPCC]] – [[LexisNexis]] Risk Solutions High Performance Computing Cluster
* [[Hypertable]] – HBase alternative
* [[Sector/Sphere]] – Open source distributed storage and processing
* [[Simple Linux Utility for Resource Management]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    = Chuck
| last1     = Lam
| date      = July 28, 2010
| title     = Hadoop in Action
| edition   = 1st
| publisher = [[Manning Publications]]
| page     = 325
| isbn      = 1-935-18219-6
| url       =
}}
*{{Cite book
| first1    = Jason
| last1     = Venner
| date      = June 22, 2009
| title     = Pro Hadoop
| edition   = 1st
| publisher = [[Apress]]
| page     = 440
| isbn      = 1-430-21942-4
| url       = http://www.apress.com/book/view/1430219424
}}
*{{Cite book
| first1    = Tom
| last1     = White
| date      = June 16, 2009
| title     = Hadoop: The Definitive Guide
| edition   = 1st
| publisher = [[O'Reilly Media]]
| page      = 524
| isbn      = 0-596-52197-9
| url       = http://oreilly.com/catalog/9780596521974
}}
{{Refend}}

==External links==
* {{Official website}}
* [http://apiwave.com/java/api/org.apache.hadoop Apache Hadoop popular APIs in GitHub]
* [http://www.stanford.edu/class/ee380/Abstracts/111116.html Introducing Apache Hadoop: The Modern Data Operating System] – a lecture given at [[Stanford University]] by Co-Founder and CTO of Cloudera, Amr Awadallah ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=111116-ee380-300.asx video archive]) ([https://www.youtube.com/watch?v=d2xeNpfzsYI YouTube])
* [http://www.se-radio.net/2010/03/episode-157-hadoop-with-philip-zeyliger/ Hadoop with Philip Zeyliger, Software Engineering Radio, IEEE Computer Society, March 8 2010]
*[http://online.sju.edu/resource/engineering-technology/key-role-hadoop-plays-in-business-intelligence The Key Role Hadoop Plays in Business Intelligence and Data Warehousing]

{{Apache}}
{{File systems}}
{{Authority control}}

{{DEFAULTSORT:Hadoop}}
[[Category:Apache Software Foundation]]
[[Category:Big data products]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Free software for cloud computing]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Hadoop|*]]
[[Category:Software using the Apache license]]</text>
      <sha1>rbs1bafihgetlckhuhsjk8d1f9141he</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Ivy</title>
    <ns>0</ns>
    <id>11243260</id>
    <revision>
      <id>783946361</id>
      <parentid>782980033</parentid>
      <timestamp>2017-06-05T16:24:16Z</timestamp>
      <contributor>
        <ip>87.141.63.99</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3958">{{Infobox software
| name                   = Apache Ivy
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 2.4.0
| latest release date    = {{release date|2014|12|26}} [http://ant.apache.org/ivy/history/2.4.0/release-notes.html]
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               = [[Java platform|Java]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Library dependency]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://ant.apache.org/ivy}}
}}

'''Apache Ivy''' is a [[transitive relation|transitive]] [[package manager]]. It is a sub-project of the [[Apache Ant]] project, with which Ivy works to resolve project dependencies. An external [[XML]] file defines project dependencies and lists the resources necessary to build a project. Ivy then resolves and downloads resources from an artifact repository: either a private repository or one publicly available on the [[Internet]].

To some degree, it competes with [[Apache Maven]], which also manages dependencies. However, Maven is a complete build tool, whereas Ivy focuses purely on managing transitive dependencies.

Newer build tools and [[continuous integration]] servers regularly support or include Ivy:
* [[SBT (software)|sbt]], or "simple build tool," the primary build tool for [[Scala (programming language)|Scala]] projects, incorporates Ivy for its dependency management (not anymore starting from sbt 1.0).
* [[Grails (framework)|Grails]] (until anticipated 3.0 release in 2014)&lt;ref&gt;{{cite web |url=http://grails.org/Roadmap |title=Grails roadmap |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=grails.org |accessdate=5 February 2014}}&lt;/ref&gt;
* [[gradle]] (until replaced by an internal dependency resolution engine in release 1.0)&lt;ref&gt;{{cite web |url=http://www.gradle.org/docs/1.0/release-notes#powerful-dependency-management |title=Gradle 1.0 Release Notes}}&lt;/ref&gt;
* [[Jenkins (software)|Jenkins]]

==Features==
* Managing project dependencies
* XML-driven declaration of project dependencies and JAR repositories
* Automatic retrieval of [[transitive relation|transitive]] dependency definitions and resources
* Automatic integration to publicly available artifact repositories
* Resolution of dependency closures
* Configurable project state definitions, which allow for multiple dependency-set definitions
* Publishing of artifacts into a local enterprise repository

==History==
[[Jayasoft]] first created Ivy in September, 2004, with [[Xavier Hanin]] serving as the principal architect and developer of the project. Jayasoft moved hosting of Ivy (then at version 1.4.1) to [[Apache Incubator]] in October 2006. Since then, the project has undergone package renaming to reflect its association with the [[Apache Software Foundation]]. Package names prefixes of the form &lt;code&gt;fr.jayasoft.ivy&lt;/code&gt; have become &lt;code&gt;org.apache.ivy&lt;/code&gt; prefixes.

Ivy graduated from the [[Apache Incubator]] in October, 2007. As of 2009 it functions as a sub-project of [[Apache Ant]].

==See also==
*[[Apache Maven]], an alternative dependency management and build tool

==References==
{{Reflist}}

* Steve Loughran, Erik Hatcher: &lt;cite&gt;Ant in Action&lt;/cite&gt;, Manning Publications Company, {{ISBN|1-932394-80-X}}
*{{Cite book
|title= JUnit in Action
|publisher= Manning
|year= 2011
|edition= 2nd
|isbn= 978-1-935182-02-3
|pages= 145–147
}}

==External links==
* {{Official|http://ant.apache.org/ivy/}}
* [http://www.jaya.free.fr/ Archival Jayasoft website]
* [http://public.dhe.ibm.com/software/dw/java/j-ap05068-a4.pdf Automation for the people: Manage dependencies with Ivy] by Paul Duvall
{{apache}}

[[Category:Java development tools]]
[[Category:Apache Software Foundation|Ivy]]
[[Category:Build automation]]</text>
      <sha1>4nuc66uoz1jxwnln3n93gvdioqdi7ws</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Mesos</title>
    <ns>0</ns>
    <id>45031048</id>
    <revision>
      <id>846360674</id>
      <parentid>835908961</parentid>
      <timestamp>2018-06-18T07:55:10Z</timestamp>
      <contributor>
        <ip>88.76.18.254</ip>
      </contributor>
      <comment>release update.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9595">{{Infobox software
| name                   = Apache Mesos
| logo                   = Apache-Mesos-logo.jpg
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 1.6.0
| latest release date    = {{release date|2018|05|11}}
| programming language   = [[C++]]
| genre                  = [[Computer cluster#Cluster management|Cluster management software]]
| license                = [[Apache License]] 2.0
| website                = {{URL|mesos.apache.org}}
}}

'''Apache Mesos''' is an [[open-source]]  project to manage [[computer cluster]]s. It was developed at the [[University of California, Berkeley]].

==History==
Mesos began as a research project in the UC Berkeley RAD Lab by then PhD students Benjamin Hindman, Andy Konwinski, and [[Matei Zaharia]], as well as professor [[Ion Stoica]]. The students started working on the project as part of a course taught by [[David Culler]]. It was originally named ''Nexus'' but due to a conflict with another university's project, was renamed to Mesos.&lt;ref name="hug-meetup"&gt;{{cite web|last1=Zaharia|first1=Matei|title=HUG Meetup August 2010: Mesos: A Flexible Cluster Resource manager - Part 1|url=https://www.youtube.com/watch?v=lE3jR6nM3bw|website=youtube.com|accessdate=13 January 2015}}&lt;/ref&gt;

Mesos was first presented in 2009 (while still named Nexus) by Andy Konwinski at HotCloud '09 in a talk accompanying the first paper published about the project.&lt;ref&gt;[http://usenix.org/event/hotcloud09/tech/full_papers/hindman.pdf A Common Substrate for Cluster Computing]&lt;/ref&gt; Later in 2011 it was presented in a more mature state in a talk by Zaharia at the [[Usenix]] Symposium on Networked Systems Design and Implementation conference about the paper "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center" by Benjamin Hindman, Andy Konwinski, Zaharia, [[Ali Ghodsi]], Anthony D. Joseph, [[Randy Katz]], [[Scott Shenker]], [[Ion Stoica]].&lt;ref name="mesos-paper"&gt;{{cite journal|last1=Hindman|first1=Benjamin|last2=Konwinski|first2=Andy|last3=Zaharia|first3=Matei|last4=Ghodsi|first4=Ali|last5=Joseph|first5=Anthony|last6=Katz|first6=Randy|last7=Shenker|first7=Scott|last8=Stoica|first8=Ion|title=Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center|journal=NSDI|date=2011|volume=11|page=22-22|url=http://people.csail.mit.edu/matei/papers/2011/nsdi_mesos.pdf|accessdate=12 January 2015|format=PDF}}&lt;/ref&gt;

On July 27, 2016, the [[Apache Software Foundation]] announced version 1.&lt;ref&gt;{{Cite news |title= The Apache Software Foundation Announces Apache Mesos v1.0 |work= Press release |date= July 27, 2016 |url= https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces97 |access-date= February 24, 2017 }}&lt;/ref&gt; It added the ability to centrally supply [[Docker (software)|Docker]], [[CoreOS|rkt]] and appc instances.&lt;ref&gt;http://siliconangle.com/blog/2016/07/27/mesos-1-0-brings-a-new-container-runtime-and-more-third-party-integrations/&lt;/ref&gt;

==Technology==

Mesos uses Linux [[Cgroups]] to provide isolation for [[CPU]], [[Virtual memory|memory]], [[I/O]] and [[file system]].&lt;ref name="isolation"&gt;{{cite web|title=Open-Source Datacenter Computing with Apache Mesos|url=https://opensource.com/business/14/9/open-source-datacenter-computing-apache-mesos|first=Sachin P.|last=Bappalige|website=[[Red Hat#Opensource.com|OpenSource.com]]|publisher=[[Red Hat]]|date=2014-09-15|accessdate=2016-12-10}}&lt;/ref&gt;
Mesos is comparable to [[Google]]'s Borg scheduler, a highly secretive platform used internally to manage and distribute Google's services.&lt;ref name="wired-mesos-borg" /&gt;

===Apache Aurora===
[https://aurora.apache.org/ Apache Aurora] is a Mesos framework for both long-running services and cron jobs, originally developed by Twitter starting in 2010 and open sourced in late 2013.&lt;ref&gt;{{cite web|title=All about Apache Aurora|url=https://blog.twitter.com/2015/all-about-apache-aurora|publisher=[[Twitter]]|accessdate=20 May 2015}}&lt;/ref&gt; It can scale to tens of thousands of servers, and holds many similarities to Google's Borg&lt;ref&gt;{{cite web|title=Large-scale cluster management at Google with Borg|url=http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/43438.pdf|publisher=[[Google]]|format=PDF|accessdate=20 May 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Twitter's Aurora and How It Relates to Google's Borg|url=http://thenewstack.io/twitters-aurora-relates-googles-borg-part-1/|accessdate=20 May 2015}}&lt;/ref&gt; including its rich [[Domain-specific language|DSL]] for configuring services.

===Chronos===
[https://mesos.github.io/chronos/ Chronos] is a distributed cron-like system, that is elastic and can also express dependencies between jobs.&lt;ref&gt;{{cite web|title=Chronos|url=https://github.com/mesos/chronos|website=GitHub.com|publisher=[[GitHub]]|accessdate=30 March 2015}}&lt;/ref&gt;

===Marathon===
[https://mesosphere.github.io/marathon/ Marathon] is promoted for [[platform as a service]] or container orchestration system scaling to thousands of physical servers. It is fully [[REST]] based and allows canary style deploys and deployment topologies. It is written in the programming language [[Scala (programming language)|Scala]].&lt;ref&gt;{{cite web|title=Marathon|url=https://mesosphere.github.io/marathon/|website=Mesosphere.GitHub.io|publisher=[[Mesosphere (software)|Mesosphere]]|date=2014|accessdate=30 March 2015}}&lt;/ref&gt;

==Users==
Social networking site Twitter began using Mesos and Apache Aurora in 2010, after Hindman gave a presentation to a group of Twitter engineers.&lt;ref name="wired-mesos-borg"&gt;{{cite web|last1=Metz|first1=Cade|title=Return of the Borg: How Twitter Rebuilt Google’s Secret Weapon|url=https://www.wired.com/2013/03/google-borg-twitter-mesos/|website=wired.com|publisher=Wired|accessdate=12 January 2015}}&lt;/ref&gt;

Airbnb said in July 2013 that it uses Mesos to run data processing systems like [[Apache Hadoop]] and [[Apache Spark]].&lt;ref&gt;{{cite web|last1=Harris|first1=Derrick|title=Airbnb is engineering itself into a data-driven company|url=https://gigaom.com/2013/07/29/airbnb-is-engineering-itself-into-a-data-driven-company/|website=gigaom.com|accessdate=12 January 2015}}&lt;/ref&gt;

The Internet auction website [[eBay]] stated in April 2014 that it used Mesos to run [[continuous integration]] on a per-developer basis. They accomplish this by using a custom Mesos plugin that allows developers to launch their own private [[Jenkins (software)|Jenkins]] instance.&lt;ref&gt;{{cite web|last1=The eBay PAAS Team|title=Delivering eBay’s CI Solution with Apache Mesos - Part I|url=http://www.ebaytechblog.com/2014/04/04/delivering-ebays-ci-solution-with-apache-mesos-part-i/|website=EbayTechBlog.com|publisher=eBay|accessdate=12 January 2015}}&lt;/ref&gt;

In April 2015, it was announced that [[Apple Inc.|Apple]] service [[Siri]] is using its own Mesos framework called Jarvis.&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Apple Details How It Rebuilt Siri on Mesos|url=https://mesosphere.com/blog/2015/04/23/apple-details-j-a-r-v-i-s-the-mesos-framework-that-runs-siri/|website=Mesosphere.com|publisher=Mesosphere|date=2015-04-23|accessdate=2015-04-27}}&lt;/ref&gt;

In August 2015, it was announced that [[Verizon]] selected Mesosphere's DC/OS, which is based on open source Apache Mesos, for data center service orchestration.&lt;ref&gt;{{cite web|title=Verizon selects Mesosphere DCOS as nationwide platform for data center service orchestration
|url=http://www.verizon.com/about/news/verizon-selects-mesosphere-dcos-nationwide-platform-data-center-service-orchestration|publisher=Verizon|accessdate=21 August 2015}}&lt;/ref&gt;

In November 2015, [[Yelp]] announced they had been using Mesos and Marathon for a year and a half for production services.&lt;ref&gt;{{Cite web|url=http://engineeringblog.yelp.com/2015/11/introducing-paasta-an-open-platform-as-a-service.html|title=Introducing PaaSTA: An Open, Distributed, Platform as a Service|website=engineeringblog.yelp.com|access-date=2016-07-12}}&lt;/ref&gt;

==Commercial support==
Software startup [[Mesosphere Inc]] sells the [[Datacenter Operating System]], a [[distributed operating system]], based on Apache Mesos.&lt;ref name="mesosphere-product"&gt;{{cite web|title=The Mesosphere DCOS|url=http://mesosphere.com/product/|website=mesosphere.com|accessdate=13 January 2015}}&lt;/ref&gt; In September 2015, [[Microsoft]] announced a commercial partnership with Mesosphere to build container scheduling and orchestration services for [[Microsoft Azure]].&lt;ref&gt;{{cite web|url=http://www.zdnet.com/article/new-azure-container-service-to-bring-together-mesos-docker-and-azure-cloud/|title=New Azure Container Service to bring together Mesos, Docker and Azure cloud|author=[[Mary Jo Foley]]|work=[[ZDNet]]|date=September 29, 2015}}&lt;/ref&gt; In October 2015, [[Oracle Corporation|Oracle]] announced support for Mesos through [[Oracle Cloud|Oracle Container Cloud Service]].&lt;ref&gt;{{Cite web|url=https://www.oracle.com/corporate/pressrelease/iaas-102715.html|title=Oracle Updates Oracle Cloud Infrastructure Services|website=www.oracle.com|language=en-US|access-date=2018-02-06}}&lt;/ref&gt;

==References==
{{Reflist|30em}}

==External links==
{{Portal|Free software}}
* {{Official website|https://mesos.apache.org/}}
* [https://mesosphere.com/ Mesosphere]

{{Linux Containers}}

[[Category:Apache Software Foundation projects|Mesos]]
[[Category:Apache Software Foundation|Mesos]]
[[Category:Cloud infrastructure]]
[[Category:Free software for cloud computing]]
[[Category:Linux Containerization]]
[[Category:Software using the Apache license]]</text>
      <sha1>gnw04tookhrhyjq80ngmnuhq4gz0bq7</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ODE</title>
    <ns>0</ns>
    <id>15603215</id>
    <revision>
      <id>785718695</id>
      <parentid>785718660</parentid>
      <timestamp>2017-06-15T01:29:03Z</timestamp>
      <contributor>
        <username>Kiwi128</username>
        <id>12235997</id>
      </contributor>
      <minor/>
      <comment>rm another set of quotes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4422">{{Context|date=October 2009}}

{{Infobox software
| name                   = Apache ODE
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version =  1.3.6
| latest release date    = {{release date|2013|10|12|}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Workflow engine]], [[Middleware]]
| standard               = [[WS-BPEL]], [[WSDL]], [[SOAP]], [[Java Business Integration|JBI]]
| license                = [[Apache License]] 2.0
| website                = {{url|//ode.apache.org/}}
}}

'''Apache ODE''' ('''Apache Orchestration Director Engine''') executes one or more [[business processes]] which have been expressed in the Web Services Business Process Execution Language ([[WS-BPEL]]). It principally communicates with one or more [[Web services]], sending and receiving messages, manipulating data and handling exceptions (errors) as defined by any given process. The engine is capable of running both long and short living processes to coordinate all the services that make up a service or application ([[Orchestration (computing)|orchestration]]).

WS-BPEL itself is based upon the [[XML]] language and includes a number of ways in which business processes can be expressed. These include conditional clauses, repeating loops, calls to web services and the exchange of messages. Where interfaces with web services are required, it makes use of Web Services Description Language ([[WSDL]]) to express them. Messages can be handled in a flexible way by reading either part or all of the message into variables, which can then be used for onward communication.

The engine has two communication layers, with which it interacts with the outside world:&lt;ref&gt;{{cite web
| accessdate = 2011-05-16
| location = http://ddweerasiri.blogspot.com/
| publisher = Denis's Blog
| title = How to deploy an Axis2 Web service programatically in ODE during the initialization of ODE Runtime
| quote = Apache ODE (Orchestration Director Engine) executes business processes written following the WS-BPEL standard. It has two communication layers. One is Axis2 integration layer and the other one is based on JBI standard. Those integration layers are used by ODE BPEL Engine Runtime for interact with the outside world. Axis2 integration layer supports for communicate via Web Service interactions. JBI integration layer supports for communicate via JBI messages.
| url = http://ddweerasiri.blogspot.com/2009/05/how-to-deploy-axis2-web-service.html}}&lt;/ref&gt;
* [[Apache Axis2]] integration layer: supports the communication over [[Web services]].
* Layer based on the [[Java Business Integration|JBI]] standard: supports communication via JBI messages.

==Features==
{{prose|section|date=March 2016}}
* Side-by-side support for both the WS-BPEL 2.0 [[OASIS (organization)|OASIS]] standard and the legacy BPEL4WS 1.1 vendor specification.
* Supports 2 communication layers: one based on [[Axis2]] (Web Services http transport) and another one based on the [[Java Business Integration|JBI]] standard (using [[ServiceMix]]).
* Support for the HTTP WSDL binding, allowing invocation of [[REST]]-style web services.
* Possibility to map process variables externally to a database table of your choice.
* High level API to the engine that allows you to integrate the core with virtually any communication layer.
* Hot-deployment of your processes.
* Compiled approach to [[BPEL]] that provides detailed analysis and validation at the command line or at deployment.
* Management interface for processes, instances and messages.

==Embedding==
Apache ODE is embedded and an important part of the [[Jboss]] projects '''RiftSaw''' ([[WS-BPEL]] 2.0 engine) and also in the follow-up '''[[Jboss Switchyard|Switchyard]]''', which is a service delivery [[Software framework|framework]] for service-oriented applications.

==See also==
{{Portal|Free Software}}
*[[WS-BPEL]]

==References==
{{Reflist|2}}

==External links==
*[//ode.apache.org/ Apache ODE home page]
*[//www.jboss.org/riftsaw RiftSaw]
*[//www.jboss.org/switchyard SwitchYard]
{{apache}}

&lt;!--Interwikies--&gt;

&lt;!--Categories--&gt;
[[Category:Apache Software Foundation|ODE]]
[[Category:Beta software]]</text>
      <sha1>ns3fsiq1mbyvdftc4d2l3rjluxti7v9</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Shale</title>
    <ns>0</ns>
    <id>9079196</id>
    <revision>
      <id>707624920</id>
      <parentid>706279779</parentid>
      <timestamp>2016-02-29T22:15:09Z</timestamp>
      <contributor>
        <username>LE666</username>
        <id>25989282</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1854">{{no footnotes|date=August 2012}}
{{ref improve|date=August 2012}}
{{Infobox software
| name = Apache Shale
| logo = 
| screenshot =
| caption =
| developer = [[Apache Software Foundation]]
| status = Retired
| latest release version = 1.0.4
| latest release date = {{release date|2007|12|19}}
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[web framework|Web Framework]]
| license = [[Apache License]] 2.0
| website = {{URL|http://shale.apache.org/}}
}}
'''Shale''' is a [[web application framework]] maintained by the [[Apache Software Foundation]]. It is fundamentally based on [[JavaServer Faces]]. As of May 2009 Apache Shale has been retired and moved to the [[Apache Attic]].

==See also==
{{Portal|Java}}
* [[Apache Struts]]

==References==
{{refbegin}}
* {{citation |url=http://www.ibm.com/developerworks/library/j-shale0228/ |title=All Hail Shale: Shale isn't Struts |first=Brett D. |last=McLaughlin |work=[[DeveloperWorks]] |date=February 28, 2006 |accessdate=August 3, 2012 }}
* {{citation |url=http://www.itworld.com/development/69162/apache-shale-web-framework-project-retired |title=Apache Shale Web framework project retired |first=Paul |last=Krill |work=[[InfoWorld]] |date=June 11, 2009 |accessdate=August 3, 2012 }}
{{refend}}

==External links==
* [http://shale.apache.org/ Shale project homepage]
* [http://www.jsfcentral.com/articles/mcclanahan-05-05.html JSF Central Interviews Craig McClanahan about Shale]
* [http://www.javaworld.com/javaworld/jw-06-2009/061109-apache-shale-web-framework-project.html Apache Shale Web framework project retired]

{{Application frameworks}}
{{apache}}

{{web-software-stub}}

[[Category:Apache Software Foundation|Shale]]
[[Category:JavaServer Faces]]
[[Category:Web frameworks]]</text>
      <sha1>nbeqke4et3blhmpxij4o5mqq2hp6kez</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Spark</title>
    <ns>0</ns>
    <id>42164234</id>
    <revision>
      <id>846671867</id>
      <parentid>845780723</parentid>
      <timestamp>2018-06-20T06:17:39Z</timestamp>
      <contributor>
        <username>Manchunkumar</username>
        <id>34032005</id>
      </contributor>
      <minor/>
      <comment>/* Overview */ added reference for Apache Spark Data Frames</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26246">{{Infobox Software
| name                   = Apache Spark
| logo                   = [[File:Spark-logo-192x100px.png|frameless|Spark Logo]]
| caption                =
| author                 = [[Matei Zaharia]]
| developer              = [[Apache Software Foundation]], [[UC Berkeley]] AMPLab, [[Databricks]]
| status                 = Active
| released               = [https://github.com/apache/spark/releases/tag/v1.0.0 {{Start date and age|2014|05|26}}]
| latest release version = v2.3.1
| latest release date    = {{Start date and age|2018|06|08}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Microsoft Windows]], [[macOS]], [[Linux]]
| size                   = 
| programming language   = [[Scala (programming language)|Scala]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[R (programming language)|R]]&lt;ref&gt;{{cite web |url=https://spark.apache.org/releases/spark-release-2-0-0.html |title=Spark Release 2.0.0 |quote=MLlib in R: SparkR now offers MLlib APIs [..] Python: PySpark now offers many more MLlib algorithms"}}&lt;/ref&gt;
| genre                  = Data analytics, [[machine learning]] algorithms
| license                = [[Apache License]] 2.0 
| website                = {{URL|https://spark.apache.org}}|repo=https://github.com/apache/spark|language=Scala, Java, SQL, Python, R}}

'''Apache Spark''' is an [[Open-source software|open-source]] [[Cluster computing|cluster-computing]] [[Software framework|framework]]. Originally developed at the [[UC Berkeley|University of California, Berkeley]]'s [[AMPLab]], the Spark [[codebase]] was later donated to the [[Apache Software Foundation]], which has maintained it since. Spark provides an [[application programming interface|interface]] for programming entire clusters with implicit [[data parallelism]] and [[fault tolerance]].

==Overview==
Apache Spark has as its architectural foundation the resilient distributed dataset (RDD), a read-only [[multiset]] of data items distributed over a cluster of machines, that is maintained in a [[fault-tolerant computing|fault-tolerant]] way.&lt;ref name="hc10"&gt;{{cite conference |first1=Matei |last1=Zaharia |first2=Mosharaf |last2=Chowdhury |first3=Michael J. |last3=Franklin |first4=Scott |last4=Shenker |first5=Ion |last5=Stoica |title=Spark: Cluster Computing with Working Sets |conference=USENIX Workshop on Hot Topics in Cloud Computing (HotCloud) |url=https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Spark-Cluster-Computing-with-Working-Sets.pdf}}&lt;/ref&gt; In Spark 1.x, the RDD was the primary [[application programming interface]] (API), but as of Spark 2.x use of the Dataset API is encouraged&lt;ref&gt;{{cite web |url=https://spark.apache.org/docs/2.2.0/quick-start.html |title=Spark 2.2.0 Quick Start |author=&lt;!--Not stated--&gt; |date=2017-07-11 |website=apache.org |access-date=2017-10-19 |quote=we highly recommend you to switch to use Dataset, which has better performance than RDD}}&lt;/ref&gt; even though the RDD API is not [[deprecated]].&lt;ref&gt;{{cite web |url=https://spark.apache.org/docs/2.2.0/api/scala/index.html#deprecated-list |title=Spark 2.2.0 deprecation list |author=&lt;!--Not stated--&gt; |date=2017-07-11 |website=apache.org |access-date=2017-10-10}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html |title=A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets: When to use them and why |last=Damji |first=Jules |date=2016-07-14 |website=databricks.com |access-date=2017-10-19}}&lt;/ref&gt; The RDD technology still underlies the Dataset API.&lt;ref&gt;{{cite book |last=Chambers |first=Bill |date=2017-08-10 |title=Spark: The Definitive Guide |url=http://techbus.safaribooksonline.com/book/operating-systems-and-server-administration/apache/9781491912201/12dot-resilient-distributed-datasets-rdds/about_rdds_html |publisher=[[O'Reilly Media]] |chapter=12 |quote=virtually all Spark code you run, where DataFrames or Datasets, compiles down to an RDD }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.janbasktraining.com/blog/what-is-spark/ |title= What is Apache Spark? Spark Tutorial Guide for Beginner |website=janbasktraining.com |access-date=2018-04-13}}&lt;/ref&gt;

Spark and its RDDs were developed in 2012 in response to limitations in the [[MapReduce]] cluster computing [[Programming paradigm|paradigm]], which forces a particular linear [[dataflow]] structure on distributed programs: MapReduce programs read input data from disk, [[Map (parallel pattern)|map]] a function across the data, [[Fold (higher-order function)|reduce]] the results of the map, and store reduction results on disk. Spark's RDDs function as a [[working set]] for distributed programs that offers a (deliberately) restricted form of distributed [[shared memory]].&lt;ref&gt;{{cite conference |first1=Matei |last1=Zaharia |first2=Mosharaf |last2=Chowdhury |first3=Tathagata |last3=Das |first4=Ankur |last4=Dave |first5=Justin |last5=Ma, |first6=Murphy |last6=McCauley |first7=Michael |last7=J. |first8=Scott |last8=Shenker |first9=Ion |last9=Stoica |date=2010 |title=Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing |url=https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf |conference=USENIX Symp. Networked Systems Design and Implementation}}&lt;/ref&gt;

Spark facilitates the implementation of both [[iterative algorithm]]s, that visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated [[database]]-style querying of data. The [[latency (engineering)|latency]] of such applications may be reduced by several orders of magnitude compared to a MapReduce implementation (as was common in [[Apache Hadoop]] stacks).{{r|hc10}}&lt;ref&gt;{{cite journal|first1=Reynold| last1=Xin| first2=Josh |last2=Rosen| first3=Matei| last3=Zaharia| first4=Michael| last4=Franklin| first5=Scott| last5=Shenker| first6=Ion| last6=Stoica|title=Shark: SQL and Rich Analytics at Scale| conference=SIGMOD 2013|date=June 2013| url=https://amplab.cs.berkeley.edu/wp-content/uploads/2013/02/shark_sigmod2013.pdf}}&lt;/ref&gt;
Among the class of iterative algorithms are the training algorithms for [[machine learning]] systems, which formed the initial impetus for developing Apache Spark.&lt;ref&gt;{{cite web |title=4 reasons why Spark could jolt Hadoop into hyperdrive |first=Derrick |last=Harris |date=28 June 2014 |url=https://gigaom.com/2014/06/28/4-reasons-why-spark-could-jolt-hadoop-into-hyperdrive/ |website=[[Gigaom]]}}&lt;/ref&gt;

Apache Spark requires a [[cluster manager]] and a [[Clustered file system|distributed storage system]]. For cluster management, Spark supports standalone (native Spark cluster), [[Apache Hadoop|Hadoop YARN]], or [[Apache Mesos]].&lt;ref&gt;{{cite web |url=https://spark.apache.org/docs/1.2.0/cluster-overview.html#cluster-manager-types |title=Cluster Mode Overview - Spark 1.2.0 Documentation - Cluster Manager Types |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=2014-12-18 |website=apache.org |publisher=Apache Foundation |accessdate=2015-01-18}}&lt;/ref&gt; For distributed storage, Spark can interface with a wide variety, including [[Apache Hadoop#Hadoop distributed file system|Hadoop Distributed File System (HDFS)]],&lt;ref&gt;[https://amplab.cs.berkeley.edu/software/ Figure showing Spark in relation to other open-source Software projects including Hadoop]&lt;/ref&gt; [[MapR#MapR converged data platform|MapR File System (MapR-FS)]],&lt;ref&gt;[http://doc.mapr.com/display/MapR/Ecosystem+Support+Matrix MapR ecosystem support matrix]&lt;/ref&gt; [[Apache Cassandra|Cassandra]],&lt;ref&gt;{{cite mailing list |url=http://mail-archives.apache.org/mod_mbox/cassandra-user/201409.mbox/%3CCABNXB2DE5Apmvn1nNg79+VdPCSZiCsGdt=ZB4s4OF_5JzS60iA@mail.gmail.com%3E |title=Re: cassandra + spark / pyspark |date=2014-09-10 |accessdate=2014-11-21 |mailinglist=Cassandra User |last=Doan |first=DuyHai }}&lt;/ref&gt; [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Amazon S3]], [[Apache Kudu|Kudu]], or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per [[CPU core]].

===Spark Core===
Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic [[I/O interface|I/O]] functionalities, exposed through an application programming interface (for [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Scala (programming language)|Scala]], and [[R (programming language)|R]]) centered on the RDD [[Abstraction (computer science)|abstraction]] (the Java API is available for other JVM languages, but is also usable for some other non-JVM languages, such as [[Julia (programming language)|Julia]],&lt;ref&gt;https://github.com/dfdx/Spark.jl&lt;/ref&gt; that can connect to the JVM). This interface mirrors a [[functional programming|functional]]/[[higher-order programming|higher-order]] model of programming: a "driver" program invokes parallel operations such as map, [[Filter (computer science)|filter]] or reduce on an RDD by passing a function to Spark, which then schedules the function's execution in parallel on the cluster.{{r|hc10}} These operations, and additional ones such as [[Join (database)|joins]], take RDDs as input and produce new RDDs. RDDs are [[Immutable object|immutable]] and their operations are [[lazy evaluation|lazy]]; fault-tolerance is achieved by keeping track of the "lineage" of each RDD (the sequence of operations that produced it) so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, Java, or Scala objects.

Besides the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: ''broadcast variables'' reference read-only data that needs to be available on all nodes, while ''accumulators'' can be used to program reductions in an [[imperative programming|imperative]] style.{{r|hc10}}

A typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each {{mono|map}}, {{mono|flatMap}} (a variant of {{mono|map}}) and {{mono|reduceByKey}} takes an [[anonymous function]] that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.

&lt;source lang="scala"&gt;
val conf = new SparkConf().setAppName("wiki_test") // create a spark config object
val sc = new SparkContext(conf) // Create a spark context
val data = sc.textFile("/path/to/somedir") // Read files from "somedir" into an RDD of (filename, content) pairs.
val tokens = data.flatMap(_.split(" ")) // Split each file into a list of tokens (words).
val wordFreq = tokens.map((_, 1)).reduceByKey(_ + _) // Add a count of one to each token, then sum the counts per word type.
wordFreq.sortBy(s =&gt; -s._2).map(x =&gt; (x._2, x._1)).top(10) // Get the top 10 words. Swap word and count to sort by count.
&lt;/source&gt;

===Spark SQL===
Spark [[SQL]] is a component on top of Spark Core that introduced a data abstraction called DataFrames,{{efn|Called SchemaRDDs before Spark 1.3&lt;ref&gt;https://spark.apache.org/releases/spark-release-1-3-0.html&lt;/ref&gt;}} which provides support for structured and [[semi-structured data]]. Spark SQL provides a [[domain-specific language]] (DSL) to manipulate DataFrames in [[Scala (programming language)|Scala]], [[Java (programming language)|Java]], or [[Python (programming language)|Python]]. It also provides SQL language support, with [[command-line interface]]s and [[Open Database Connectivity|ODBC]]/[[Java Database Connectivity|JDBC]] server. Although DataFrames lack the compile-time type-checking afforded by RDDs, as of Spark 2.0, the strongly typed DataSet is fully supported by Spark SQL as well.

&lt;source lang="scala"&gt;
import org.apache.spark.sql.SQLContext

val url = "jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword" // URL for your database server.
val sqlContext = new org.apache.spark.sql.SQLContext(sc) // Create a sql context object

val df = sqlContext
  .read
  .format("jdbc")
  .option("url", url)
  .option("dbtable", "people")
  .load()

df.printSchema() // Looks the schema of this DataFrame.
val countsByAge = df.groupBy("age").count() // Counts people by age
&lt;/source&gt;

===Spark Streaming===
Spark Streaming uses Spark Core's fast scheduling capability to perform [[Event stream processing|streaming analytics]]. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of [[lambda architecture]].&lt;ref&gt;{{Cite web|url=https://www.pluralsight.com/courses/spark-kafka-cassandra-applying-lambda-architecture|title=Applying the Lambda Architecture with Spark, Kafka, and Cassandra {{!}} Pluralsight|website=www.pluralsight.com|access-date=2016-11-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://blog.cloudera.com/blog/2014/08/building-lambda-architecture-with-spark-streaming/ |title=Building Lambda Architecture with Spark Streaming |last1=Shapira |first1=Gwen |date=29 August 2014 |website=cloudera.com |publisher=Cloudera |access-date=17 June 2016 |quote=re-use the same aggregates we wrote for our batch application on a real-time data stream}}&lt;/ref&gt; However, this convenience comes with the penalty of latency equal to the mini-batch duration. Other streaming data engines that process event by event rather than in mini-batches include [[Storm (event processor)|Storm]] and the streaming component of [[Apache Flink|Flink]].&lt;ref&gt;{{cite web |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7530084 |format=PDF |title= Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming |date=May 2016 |publisher=IEEE }}&lt;/ref&gt; Spark Streaming has support built-in to consume from [[Apache Kafka|Kafka]], [[Apache Flume|Flume]], [[Twitter#Implementation|Twitter]], [[ZeroMQ]], [[Amazon Web Services#Database|Kinesis]], and [[Network socket|TCP/IP sockets]].&lt;ref&gt;{{cite web |url=https://www.sigmoid.com/getting-data-into-spark-streaming/ |title=Getting Data into Spark Streaming |last1=Kharbanda |first1=Arush |date=17 March 2015 |website=sigmoid.com |publisher=Sigmoid (Sunnyvale, California IT product company) |access-date=7 July 2016}}&lt;/ref&gt;

In Spark 2.x, a separate technology based on Datasets, called Structured Streaming, that has a higher-level interface is also provided to support streaming.&lt;ref&gt;{{cite web |url=https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html |title=Structured Streaming In Apache Spark: A new high-level API for streaming |last=Zaharia |first=Matei |date=2016-07-28 |website=databricks.com |access-date=2017-10-19}}&lt;/ref&gt;

===MLlib Machine Learning Library===
Spark MLlib is a [[Distributed computing|distributed]] machine learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by [[Apache Mahout]] (according to benchmarks done by the MLlib developers against the [[Linear regression|alternating least squares]] (ALS) implementations, and before Mahout itself gained a Spark interface), and [[Scale (computing)|scales]] better than [[Vowpal Wabbit]].&lt;ref&gt;{{cite web |url=http://www.slideshare.net/chaochen5496/mlllib-sparkmeetup8613finalreduced/68 |title=Spark Meetup: MLbase, Distributed Machine Learning with Spark |last1=Sparks |first1=Evan |last2=Talwalkar |first2=Ameet |date=2013-08-06 |website=slideshare.net |publisher=Spark User Meetup, San Francisco, California |accessdate=10 February 2014}}&lt;/ref&gt; Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning [[Pipeline (software)|pipelines]], including:

* [[summary statistics]], [[Correlation and dependence|correlations]], [[stratified sampling]], [[hypothesis testing]], random data generation&lt;ref&gt;{{Cite web|title = MLlib {{!}} Apache Spark|url = http://spark.apache.org/mllib/|website = spark.apache.org|access-date = 2016-01-18}}&lt;/ref&gt;
* [[Statistical classification|classification]] and [[Regression analysis|regression]]: [[support vector machines]], [[logistic regression]], [[linear regression]], decision trees, [[Naive Bayes classifier|naive Bayes classification]]
* [[collaborative filtering]] techniques including alternating least squares (ALS)
* [[Cluster analysis|cluster analysis methods]] including [[K-means clustering|k-means]], and [[latent Dirichlet allocation]] (LDA)
* [[dimensionality reduction|dimensionality reduction techniques]] such as [[singular value decomposition]] (SVD), and [[principal component analysis]] (PCA)
* [[feature extraction]] and [[Data transformation (statistics)|transformation]] functions
* [[optimization (mathematics)|optimization]] algorithms such as [[stochastic gradient descent]], [[limited-memory BFGS]] (L-BFGS)

===GraphX===
GraphX is a distributed [[Graph (abstract data type)|graph processing]] framework on top of Apache Spark. Because it is based on RDDs, which are immutable, graphs are immutable and thus GraphX is unsuitable for graphs that need to be updated, let alone in a transactional manner like a [[graph database]].&lt;ref&gt;{{cite web |url=http://www.slideshare.net/SparkSummit/finding-graph-isomorphisms-in-graphx-and-graphframes/11 |title=Finding Graph Isomorphisms In GraphX And GraphFrames: Graph Processing vs. Graph Database |last1=Malak |first1=Michael |date=14 June 2016 |website=slideshare.net |publisher=sparksummit.org |access-date=11 July 2016}}&lt;/ref&gt; GraphX provides two separate APIs for implementation of massively parallel algorithms (such as [[PageRank]]): a [[Graph database#Distributed processing|Pregel]] abstraction, and a more general MapReduce style API.&lt;ref&gt;{{cite book |last=Malak |first=Michael |date=1 July 2016 |title=Spark GraphX in Action |url=https://books.google.com/books?id=8XcPjwEACAAJ |publisher=Manning |page=89 |isbn=9781617292521 |quote=Pregel and its little sibling aggregateMessages() are the cornerstones of graph processing in GraphX. ... algorithms that require more flexibility for the terminating condition have to be implemented using aggregateMessages()}}&lt;/ref&gt; Unlike its predecessor Bagel, which was formally deprecated in Spark 1.6, GraphX has full support for property graphs (graphs where properties can be attached to edges and vertices).&lt;ref&gt;{{cite web |url=http://www.slideshare.net/SparkSummit/finding-graph-isomorphisms-in-graphx-and-graphframes/15 |title=Finding Graph Isomorphisms In GraphX And GraphFrames: Graph Processing vs. Graph Database |last1=Malak |first1=Michael |date=14 June 2016 |website=slideshare.net |publisher=sparksummit.org |access-date=11 July 2016}}&lt;/ref&gt;

GraphX can be viewed as being the Spark in-memory version of [[Apache Giraph]], which utilized Hadoop disk-based MapReduce.&lt;ref&gt;{{cite book |last=Malak |first=Michael |date=1 July 2016 |title=Spark GraphX in Action |url=https://books.google.com/books?id=8XcPjwEACAAJ |publisher=Manning |page=9 |isbn=9781617292521 |quote=Giraph is limited to slow Hadoop Map/Reduce}}&lt;/ref&gt;

Like Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.&lt;ref&gt;{{cite journal|first1=Joseph| last1=Gonzalez| first2=Reynold |last2=Xin| first3=Ankur| last3=Dave| first4=Daniel| last4=Crankshaw| first5=Michael| last5=Franklin| first6=Ion| last6=Stoica|title=GraphX: Graph Processing in a Distributed Dataflow Framework| conference=OSDI 2014|date=Oct 2014| url=https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf}}&lt;/ref&gt;

==History==
Spark was initially started by [[Matei Zaharia]] at UC Berkeley's AMPLab in 2009, and open sourced in 2010 under a [[BSD licenses|BSD license]].&lt;ref name=":0"&gt;{{Cite news|url=https://www.computerweekly.com/feature/Apache-Spark-speeds-up-big-data-decision-making|title=Apache Spark speeds up big data decision-making|last=Clark|first=Lindsay|date=|work=ComputerWeekly.com|access-date=2018-05-16|language=en-GB}}&lt;/ref&gt;

In 2013, the project was donated to the Apache Software Foundation and switched its license to [[Apache License|Apache 2.0]]. In February 2014, Spark became a [[Apache Software Foundation#Projects|Top-Level Apache Project]].&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |accessdate=4 March 2014}}&lt;/ref&gt;

In November 2014, Spark founder M. Zaharia's company [[Databricks]] set a new world record in large scale sorting using Spark.&lt;ref&gt;[http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html Spark officially sets a new record in large-scale sorting]&lt;/ref&gt;&lt;ref name=":0" /&gt;

Spark had in excess of 1000 contributors in 2015,&lt;ref&gt;[https://www.openhub.net/p/apache-spark Open HUB Spark development activity]&lt;/ref&gt; making it one of the most active projects in the Apache Software Foundation&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |accessdate=4 March 2014}}&lt;/ref&gt; and one of the most active open source [[big data]] projects.

Given the popularity of the platform by 2014, paid programs like [[General_Assembly_(school)|General Assembly]] and free fellowships like [[The Data Incubator]] have started offering customized training courses&lt;ref&gt;{{cite news
|title=NY gets new bootcamp for data scientists: It’s free, but harder to get into than Harvard
|newspaper=Venture Beat
|access-date=2016-02-21
|url=https://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/
}}&lt;/ref&gt;

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date
|-
| {{Version|o|0.5}}
| 2012-06-12
| 0.5.1
| 2012-10-07
|-
| {{Version|o|0.6}}
| 2012-10-14
| 0.6.2
| 2013-02-07&lt;ref&gt;{{cite web |url=http://spark.apache.org/news/ |title=Spark News |author=&lt;!--Not stated--&gt; |website=apache.org |access-date=2017-03-30}}&lt;/ref&gt;
|-
| {{Version|o|0.7}}
| 2013-02-27
| 0.7.3
| 2013-07-16
|-
| {{Version|o|0.8}}
| 2013-09-25
| 0.8.1
| 2013-12-19
|-
| {{Version|o|0.9}}
| 2014-02-02
| 0.9.2
| 2014-07-23
|-
| {{Version|o|1.0}}
| 2014-05-26
| 1.0.2
| 2014-08-05
|-
| {{Version|o|1.1}}
| 2014-09-11
| 1.1.1
| 2014-11-26
|-
| {{Version|o|1.2}}
| 2014-12-18
| 1.2.2
| 2015-04-17
|-
| {{Version|o|1.3}}
| 2015-03-13
| 1.3.1
| 2015-04-17
|-
| {{Version|o|1.4}}
| 2015-06-11
| 1.4.1
| 2015-07-15
|-
| {{Version|o|1.5}}
| 2015-09-09
| 1.5.2
| 2015-11-09
|-
| {{Version|co|1.6}}
| 2016-01-04
| 1.6.3
| 2016-11-07
|-
| {{Version|co|2.0}}
| 2016-07-26
| 2.0.2
| 2016-11-14
|-
| {{Version|co|2.1}}
| 2016-12-28
| 2.1.2
| 2017-10-09
|-
| {{Version|co|2.2}}
| 2017-07-11
| 2.2.1
| 2017-12-01
|-
| {{Version|c|2.3}}
| 2018-02-28
| 2.3.1
| 2018-06-08
|-
| colspan="4" | &lt;small&gt;{{Version |l |show=111110}}&lt;/small&gt;
|}
&lt;!-- o=Old version; co=Older version, still supported; c=Latest version; p=Latest preview version (same as "Planned-Future"?) --&gt;

==Hosting Spark in the cloud==
Spark can be deployed in a traditional [[On-premises software|on-premises]] [[data center]] as well as in the [[Cloud computing|cloud]].&lt;ref&gt;{{cite web |url=https://www.infoworld.com/article/2950491/big-data/why-apache-spark-is-spiking-in-the-cloud.html |title=Why Spark is spiking in the cloud |last=Yegulalp |first=Serdar |date=2015-07-22 |website=infoworld.com |publisher=InfoWorld |access-date=2018-04-26}}&lt;/ref&gt; The cloud allows organizations to deploy Spark without the need to acquire hardware or specific setup expertise. Enterprise Strategy Group (esg-global.com) found 43% of respondents considering cloud as their primary deployment for Spark.&lt;ref&gt;{{Cite web|url=https://aws.amazon.com/big-data/what-is-spark/|title=What is Apache Spark? {{!}} AWS|website=Amazon Web Services, Inc.|language=en-US|access-date=2018-04-26}}&lt;/ref&gt; The top reasons customers perceived the cloud as an advantage for Spark are faster time to deployment, better availability, more frequent feature/functionality updates, more elasticity, more geographic coverage, and costs linked to actual utilization. Vendors who currently have an offer for the cloud include [[Databricks]], [[Amazon Web Services|AWS]], [[Microsoft Azure]], [[Google Cloud Platform|Google Cloud]], [[Oracle Cloud#Platform as a Service (PaaS)|Oracle Cloud]], and [[Bluemix|IBM Bluemix]].

==See also==
* [[List of concurrent and parallel programming languages#APIs.2FFrameworks|List of concurrent and parallel programming APIs/Frameworks]]

==Notes==
{{Notelist}}

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}
* [https://spark.apache.org/sql/ Spark SQL]
* [https://spark.apache.org/streaming/ Spark Streaming]
* [https://spark.apache.org/mllib/ MLlib machine learning library]
* [https://spark.apache.org/graphx/ GraphX graph processing library]

{{Apache}}
{{Parallel computing}}

{{DEFAULTSORT:Spark}}
[[Category:Apache Software Foundation]]
[[Category:Big data products]]
[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Free software programmed in Scala]]
[[Category:Hadoop]]
[[Category:Java platform]]
[[Category:Software using the Apache license]]
[[Category:University of California, Berkeley]]
[[Category:Articles with example Scala code]]</text>
      <sha1>rkrob301xdxy45vik6ad5bwv0uk6627</sha1>
    </revision>
  </page>
  <page>
    <title>Apache SystemML</title>
    <ns>0</ns>
    <id>49338480</id>
    <revision>
      <id>789525799</id>
      <parentid>781492305</parentid>
      <timestamp>2017-07-07T21:56:27Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4330">{{Infobox Software
| name = Apache SystemML
| logo = [[File:SystemML-Logo-Black.png|frameless|Logo for the Apache SystemML project.]]
| author = [[Shivakumar Vaithyanathan]]
| developer = [[Apache Software Foundation]], [[IBM]]
| status = Active
| released = {{Start date and age|2015|11|02}}
| operating system = [[Linux]], [[macOS]], [[Windows]]
| programming language = [[Java (programming language)|Java]]
| genre = [[Machine Learning]], [[Deep Learning]]
| license = [[Apache License]] 2.0 
| website = {{URL|http://systemml.apache.org/}}
}}

'''Apache SystemML''' is a flexible machine learning system that automatically scales to [[Apache Spark|Spark]] and [[Apache Hadoop|Hadoop]] clusters. SystemML's
distinguishing characteristics are:

# Algorithm customizability via R-like and Python-like languages.
# Multiple execution modes, including Standalone, [[Apache Spark|Spark]] Batch, [[Apache Spark|Spark]] MLContext, [[Apache Hadoop|Hadoop]] Batch, and JMLC.
# Automatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.

==History==
SystemML was created in 2010 by researchers at the [[IBM Almaden Research Center]] led by IBM Fellow Shivakumar Vaithyanathan. It was observed that data scientists would write machine learning algorithms in languages such as [[R (programming language)|R]] and [[Python (programming language)|Python]] for small data. When it came time to scale to big data, a systems programmer would be needed to scale the algorithm in a language such as [[Scala (programming language)|Scala]]. This process typically involved days or weeks per iteration, and errors would occur translating the algorithms to operate on big data. SystemML seeks to simplify this process. A primary goal of SystemML is to automatically scale an algorithm written in an R-like or Python-like language to operate on big data, generating the same answer without the error-prone, multi-iterative translation approach.

On June 15, 2015, at the Spark Summit in San Francisco, Beth Smith, General Manager of IBM Analytics, announced that IBM was open-sourcing SystemML as part of IBM's major commitment to [[Apache Spark]] and Spark-related projects. SystemML became publicly available on [[GitHub]] on August 27, 2015 and became an [[Apache Incubator]] project on November 2, 2015. On May 17, 2017, the Apache Software Foundation Board approved the graduation of Apache SystemML as an Apache Top Level Project.

==External links==
*[http://systemml.apache.org/ Apache SystemML website]
*[http://researcher.watson.ibm.com/researcher/view_group.php?id=3174 IBM Research - SystemML]
*[http://www.spark.tc/q-a-with-shiv-vaithyanathan-creator-of-systemml-and-ibm-fellow/ Q &amp; A with Shiv Vaithyanathan, Creator of SystemML and IBM Fellow]
*[http://www.spark.tc/a-universal-translator-for-big-data-and-machine-learning/ A Universal Translator for Big Data and Machine Learning]
*[https://www.youtube.com/watch?v=WkYqjWL1xzk SystemML: Declarative Machine Learning at Scale presentation by Fred Reiss]
*[http://researcher.watson.ibm.com/researcher/files/us-ytian/systemML.pdf SystemML: Declarative Machine Learning on MapReduce]
*[http://www.vldb.org/pvldb/vol7/p553-boehm.pdf Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML]
*[https://web.archive.org/web/20150218102423/http://sites.computer.org/debull/A14sept/p52.pdf SystemML’s Optimizer: Plan Generation for Large-Scale Machine Learning Programs]
*[http://www.zdnet.com/article/ibms-systemml-machine-learning-system-becomes-apache-incubator-project/ IBM's SystemML machine learning system becomes Apache Incubator project]
*[http://www.theinquirer.net/inquirer/news/2413132/ibm-donates-machine-learning-tech-to-apache-spark-open-source-community IBM donates machine learning tech to Apache Spark open source community]
*[http://www.eweek.com/developer/ibms-systemml-moves-forward-as-apache-incubator-project.html IBM's SystemML Moves Forward as Apache Incubator Project]

==See also==
*[[Comparison of deep learning software]]

{{Apache}}

[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation|SystemML]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Big data products]]</text>
      <sha1>g7a47rr90efgot2i2kbzjisaxuevrdl</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Taverna</title>
    <ns>0</ns>
    <id>8283256</id>
    <revision>
      <id>830041974</id>
      <parentid>826556980</parentid>
      <timestamp>2018-03-12T11:26:44Z</timestamp>
      <contributor>
        <username>Renamed user 2560613081</username>
        <id>16847332</id>
      </contributor>
      <comment>/* top */ Cleanup. Removed erroneous parameters from the infobox. For a list of supported parameters please consult [[Template:Infobox software/doc]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15579">{{ Infobox Software
| name                   = Apache Taverna
| logo                   = Taverna-wheel-logo.png
| screenshot             = Dragon-workflow.png
| caption                = Taverna Workbench
| developer              = [[Apache Software Foundation]] ([[myGrid]] for 2.x)
| latest release version = 3.1
| latest release date    = {{release date|2016|07|01}}
| operating system       = [[Linux]], [[Mac OS X]], [[Microsoft Windows]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Scientific workflow system]]
| license                = [[Apache License]] 2.0 ([[LGPL]] for 2.x)
| website                = {{URL|taverna.incubator.apache.org/}}
}}'''Apache Taverna''' is an [[open source software]] tool for designing and executing [[workflow]]s, initially created by the [[myGrid]] project under the name ''Taverna Workbench'', now a project under the [[Apache incubator]]. Taverna allows users to integrate many different software components, including [[WSDL]] SOAP or REST [[Web service]]s, such as those provided by the [[National Center for Biotechnology Information]], the [[European Bioinformatics Institute]], the [[DNA Data Bank of Japan|DNA Databank of Japan (DDBJ)]], [[Soaplab|SoapLab]], [[BioMOBY]] and [[EMBOSS]]. The set of available services is not finite and users can import new service descriptions into the Taverna Workbench.&lt;ref&gt;{{Cite journal | last1 = Belhajjame | first1 = K. | last2 = Wolstencroft | first2 = K. | last3 = Corcho | first3 = O. | last4 = Oinn | first4 = T. | last5 = Tanoh | first5 = F. | last6 = William | first6 = A. | last7 = Goble | first7 = C. | doi = 10.1109/CCGRID.2008.17 | title = Metadata Management in the Taverna Workflow System | pages = 651–656 | year = 2008 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal 
| last1 = Li | first1 = P. 
| last2 = Castrillo | first2 = J. I. 
| last3 = Velarde | first3 = G. 
| last4 = Wassink | first4 = I. 
| last5 = Soiland-Reyes | first5 = S. 
| last6 = Owen | first6 = S. 
| last7 = Withers | first7 = D. 
| last8 = Oinn | first8 = T. 
| last9 = Pocock | first9 = M. R. 
| last10 = Goble | first10 = C. A. 
| last11 = Oliver | first11 = S. G. 
| last12 = Kell | first12 = D. B. 
| title = Performing statistical analyses on quantitative data in Taverna workflows: An example using R and maxdBrowse to identify differentially-expressed genes from microarray data 
| doi = 10.1186/1471-2105-9-334 
| journal = BMC Bioinformatics 
| volume = 9 
| pages = 334 
| year = 2008 
| pmid = 18687127 
| pmc =2528018 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal 
| last1 = Oinn | first1 = T. 
| last2 = Addis | first2 = M. 
| last3 = Ferris | first3 = J. 
| last4 = Marvin | first4 = D. 
| last5 = Senger | first5 = M. 
| last6 = Greenwood | first6 = M. 
| last7 = Carver | first7 = T. 
| last8 = Glover | first8 = K. 
| last9 = Pocock | first9 = M. R. 
| last10 = Wipat 
| doi = 10.1093/bioinformatics/bth361 | first10 = A. 
| last11 = Li | first11 = P. 
| title = Taverna: A tool for the composition and enactment of bioinformatics workflows 
| journal = Bioinformatics 
| volume = 20 
| issue = 17 
| pages = 3045–3054 
| year = 2004 
| pmid = 15201187 
| pmc = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Oinn | first1 = T. | last2 = Greenwood | first2 = M. | last3 = Addis | first3 = M. | last4 = Alpdemir | first4 = M. N. | last5 = Ferris | first5 = J. | last6 = Glover | first6 = K. | last7 = Goble | first7 = C. | authorlink7 = Carole Goble| last8 = Goderis | first8 = A. | last9 = Hull | first9 = D. | doi = 10.1002/cpe.993 | last10 = Marvin | first10 = D. | last11 = Li | first11 = P. | last12 = Lord | first12 = P. | last13 = Pocock | first13 = M. R. | last14 = Senger | first14 = M. | last15 = Stevens | first15 = R. | last16 = Wipat | first16 = A. | last17 = Wroe | first17 = C. | title = Taverna: Lessons in creating a workflow environment for the life sciences | journal = Concurrency and Computation: Practice and Experience | volume = 18 | issue = 10 | pages = 1067–1100 | year = 2006 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal
 | last1 = Hull | first1 = D.
| authorlink = 
 | last2 = Wolstencroft | first2 = K.
 | last3 = Stevens | first3 = R.
 | authorlink3 = Robert David Stevens
 | last4 = Goble | first4 = C. A.
 | authorlink4 = Carole Goble
 | last5 = Pocock | first5 = M. R.
 | last6 = Li | first6 = P.
 | last7 = Oinn | first7 = T.
 | title = Taverna: A tool for building and running workflows of services
 | doi = 10.1093/nar/gkl320
 | journal = [[Nucleic Acids Research]]
 | volume = 34
 | issue = Web Server issue
 | pages = W729–W732
 | year = 2006
 | pmc = 1538887
 | pmid = 16845108
}} {{open access}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal 
| last1 = Kawas | first1 = E. 
| last2 = Senger | first2 = M. 
| last3 = Wilkinson | first3 = M. D. 
| title = BioMoby extensions to the Taverna workflow management and enactment software 
| journal = BMC Bioinformatics 
| volume = 7 
| pages = 523 
| year = 2006 
| doi = 10.1186/1471-2105-7-523 
| pmid = 17137515 
| pmc =1693925 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal 
| last1 = Sroka | first1 = J. 
| last2 = Kaczor | first2 = G. 
| last3 = Tyszkiewicz | first3 = J. 
| last4 = Kierzek | first4 = A. 
| title = XQTav: An XQuery processor for Taverna environment 
| doi = 10.1093/bioinformatics/btl101 
| journal = Bioinformatics 
| volume = 22 
| issue = 10 
| pages = 1280–1281 
| year = 2006 
| pmid = 16551662 
| pmc = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal 
| last1 = Wolstencroft | first1 = K. 
| last2 = Haines | first2 = R. 
| last3 = Fellows | first3 = D. 
| last4 = Williams | first4 = A. 
| last5 = Withers | first5 = D. 
| last6 = Owen | first6 = S. 
| last7 = Soiland-Reyes | first7 = S. 
| last8 = Dunlop | first8 = I. 
| last9 = Nenadic | first9 = A. 
| last10 = Fisher | first10 = P. 
| last11 = Bhagat | first11 = J. 
| last12 = Belhajjame | first12 = K. 
| last13 = Bacall | first13 = F. 
| last14 = Hardisty | first14 = A.
| last15 = Nieva De La Hidalga | first15 = A. 
| last16 = Balcazar Vargas | first16 = M. P. 
| last17 = Sufi | first17 = S. 
| last18 = Goble | first18 = C. 
| title = The Taverna workflow suite: Designing and executing workflows of Web Services on the desktop, web or in the cloud 
| doi = 10.1093/nar/gkt328 
| journal = Nucleic Acids Research 
| volume = 41 
| issue = Web Server issue 
| pages = W557–W561 
| year = 2013 
| pmid = 23640334 
| pmc =3692062 
}}&lt;/ref&gt;

Taverna Workbench provides a desktop authoring environment and enactment engine for scientific workflows. The Taverna workflow engine is also available separately, as a Java API, command line tool or as a server.

Taverna is used by users in many domains, such as [[bioinformatics]],&lt;ref&gt;{{Cite journal | last1 = Stevens | first1 = R. D. | authorlink1 = Robert David Stevens| last2 = Robinson | first2 = A. J. | last3 = Goble | first3 = C. A. | authorlink3 = Carole Goble| title = [[MyGrid]]: Personalised bioinformatics on the information grid | doi = 10.1093/bioinformatics/btg1041 | journal = Bioinformatics | volume = 19 | pages = i302–i304 | year = 2003 | pmid =  12855473| pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Stevens | first1 = R. D. | authorlink1 = Robert David Stevens| last2 = Tipney | first2 = H. J. | last3 = Wroe | first3 = C. J. | last4 = Oinn | first4 = T. M. | last5 = Senger | first5 = M. | last6 = Lord | first6 = P. W. | last7 = Goble | first7 = C. A. | authorlink7 = Carole Goble| last8 = Brass | first8 = A. | authorlink8 = Andy Brass| last9 = Tassabehji | first9 = M. 
| title = Exploring Williams-Beuren syndrome using myGrid | doi = 10.1093/bioinformatics/bth944 | journal = Bioinformatics | volume = 20 | pages = i303–i310 | year = 2004 | pmid =  15262813| pmc = }}&lt;/ref&gt; [[cheminformatics]],&lt;ref&gt;{{Cite journal 
| last1 = Truszkowski | first1 = A. 
| last2 = Jayaseelan | first2 = K. 
| last3 = Neumann | first3 = S. 
| last4 = Willighagen | first4 = E. L. 
| last5 = Zielesny | first5 = A. 
| last6 = Steinbeck | first6 = C. 
| doi = 10.1186/1758-2946-3-54 
| title = New developments on the cheminformatics open workflow environment CDK-Taverna 
| journal = Journal of Cheminformatics 
| volume = 3 
| pages = 54 
| year = 2011 
| pmid = 22166170 
| pmc =3292505 
}}&lt;/ref&gt; [[medicine]], [[astronomy]],&lt;ref&gt;{{Cite book | last1 = Hook | first1 = R. N. | last2 = Romaniello | first2 = M. | last3 = Ullgrén | first3 = M. | last4 = Järveläinen | first4 = P. | last5 = Maisala | first5 = S. | last6 = Oittinen | first6 = T. | last7 = Savolainen | first7 = V. | last8 = Solin | first8 = O. | last9 = Tyynelä | first9 = J. | last10 = Peron | doi = 10.1007/978-3-540-76963-7_23 | first10 = M. | last11 = Izzo | first11 = C. | last12 = Licha | first12 = T. | chapter = ESO Reflex: A Graphical Workflow Engine for Running Recipes | title = The 2007 ESO Instrument Calibration Workshop | series = ESO Astrophysics Symposia European Southern Observatory | pages = 169 | year = 2008 | isbn = 978-3-540-76962-0 | pmid =  | pmc = }}&lt;/ref&gt; [[social sciences|social science]], [[music]], and [[digital preservation]].&lt;ref&gt;{{cite web|url=http://www.scape-project.eu/wp-content/uploads/2012/05/SCAPE_D15.1_ONB_v1.0.pdf|author1=Raditsch, M. |author2=Schlarb, S. |author3=Møldrup-Dalum, P. |author4=Medjkoune, L. |title=Web content executable workflows for experimental execution|date=2012}}&lt;/ref&gt;

Some of the services for the use in Taverna workflows can be discovered through the [[BioCatalogue]] - a public, centralised and curated registry of Life Science Web services. Taverna workflows can also be shared with other people through the [[myExperiment]] [[social web]] site for scientists. BioCatalogue and myExperiment are another two product from the [[myGrid]] consortium.

Taverna is used in over 350 organizations around the world, both academic and commercial. As of 2011, there have been over 80,000 downloads of Taverna across different versions.

== Capabilities ==

Taverna workflows can invoke general [[SOAP]]/[[WSDL]] or [[Representational state transfer|REST]] [[Web service]]s, and more specific SADI, BioMart, [[BioMoby]] and [[SoapLab]] Web services. It can also invoke [[R]] statistical services, local Java code, external tools on local and remote machines (via [[Secure Shell|ssh]]), do [[XPath]] and other text manipulation, import a spreadsheet and include sub-workflows.

Taverna Workbench includes the ability to monitor the running of a workflow and to examine the [[Provenance#Data provenance|provenance]] of the data produced, exposing details of the workflow run as a [[World Wide Web Consortium|W3C]] PROV-O [[Resource Description Framework|RDF]] provenance graph,&lt;ref&gt;{{Cite conference | doi = 10.1145/2457317.2457376| chapter = A workflow PROV-corpus based on Taverna and Wings| title = Proceedings of the Joint EDBT/ICDT 2013 Workshops on - EDBT '13| pages = 331| year = 2013| last1 = Belhajjame | first1 = K. | last2 = Zhao | first2 = J. | last3 = Garijo | first3 = D. | last4 = Garrido | first4 = A. | last5 = Soiland-Reyes | first5 = S. | last6 = Alper | first6 = P. | last7 = Corcho | first7 = O. | isbn = 9781450315999}}&lt;/ref&gt; within a structured [[Research Objects|Research Object]] bundle&lt;ref&gt;{{cite web|author1=Soiland-Reyes, S|author2=Gamble, M|author3=Haines, R|title=Research Object Bundle 1.0|url=https://w3id.org/bundle/2014-11-05/|website=researchobject.org|publisher=Zenodo|accessdate=28 January 2015|ref=robundle|format=Specification|doi=10.5281/zenodo.12586|date=2014-11-05}}&lt;/ref&gt;  [[Zip (file format)|ZIP]] file that includes inputs, outputs, intermediate values and the executed workflow definition.&lt;ref&gt;{{cite web|author1=Soiland-Reyes, S|author2=Nenadic, A|title=taverna-prov: Structure of exported provenance|url=https://github.com/apache/incubator-taverna-engine/tree/master/taverna-prov#structure-of-exported-provenance|website=GitHub|accessdate=26 October 2017|ref=tavernaprov|archiveurl=https://github.com/apache/incubator-taverna-engine/blob/b636986aed62c3c381b0fdd60cf8a9596b70fccd/taverna-prov/README.md#structure-of-exported-provenance|archivedate=2017-10-26|date=2017-10-26}}&lt;/ref&gt;

Taverna includes the ability to search for services described in [[BioCatalogue]] to invoke from workflows. However, services do not need to be described within BioCatalogue to be included in workflows as they can be added from a [[WSDL]] [[Web Service]] description or entered as a [[Representational state transfer|REST]] [[Uniform resource locator|URI]] pattern.

Taverna also includes the capability to search for workflows on [[myExperiment]]. The Taverna Workbench can download, modify and run workflows discovered on myExperiment, and also upload created workflows in order to share them with others using the social aspects of myExperiment.

Taverna workflows do not need to be executed within the Taverna Workbench. Workflows can also be run by:

* a command line execution tool
* remote execution server that allow Taverna workflows to be run on other machines, on computational grids, clouds, from Web pages and portals
* online workflow designer and enactor [[OnlineHPC]]

Taverna allows pipelining and streaming of data.&lt;ref&gt;{{cite web|title=Implicit iteration|url=http://dev.mygrid.org.uk/wiki/display/tav250/Implicit+iteration|website=Taverna 2.5 User Manual|publisher=myGrid|accessdate=28 January 2015|date=2014-09-09}}&lt;/ref&gt; This means that services downstream in the workflow can start as soon as the first data item is received, without waiting for the whole data list to become available from upstream services and iterations. Taverna services execute in parallel when possible, as Taverna workflows are primarily data-driven rather than control-driven.&lt;ref&gt;{{cite web|last1=Soiland-Reyes|first1=Stian|title=Parallel service invocations|url=http://taverna.knowledgeblog.org/2010/12/13/parallel-service-invocations/|website=The Taverna Knowledge Blog|publisher=knowledgeblog.org|accessdate=28 January 2015|date=2010-12-13}}&lt;/ref&gt;

[[Image:Taverna 2.1 Splashscreen.png|thumbnail|right|250px|Taverna Workbench 2.1 [[splash screen]]]]

== Open source community ==

Taverna has been an open-source project since 2003,&lt;ref&gt;{{cite web|author1=Soiland-Reyes, S|author2=Sufi, S|author3=Seaborne, S|title=Taverna Proposal|url=https://wiki.apache.org/incubator/TavernaProposal#Inexperience_with_Open_Source|website=Incubator Wiki|publisher=Apache Software Foundation|accessdate=28 January 2015|date=2014-09-23}}&lt;/ref&gt; with contributors from multiple academic and industry institutions. In 2014-10, Taverna became an independent [[Apache incubator]] project,&lt;ref&gt;{{cite web|title=Taverna Project Incubation Status|url=https://incubator.apache.org/projects/taverna.html|website=Apache Incubator|publisher=Apache Software Foundation|accessdate=28 January 2015}}&lt;/ref&gt;
and changed name to ''Apache Taverna (incubating)''. 
The project is developing Apache Taverna 3.x,&lt;ref&gt;{{cite web|url=http://taverna.incubator.apache.org/download/|title=Download Apache Taverna|publisher=Apache Software Foundation|accessdate=28 January 2015}}&lt;/ref&gt; 
which license changed from [[LGPL|LGPL 2.1]] to [[Apache License|Apache License 2.0]].

== External links ==
* [http://taverna.incubator.apache.org/ Apache Taverna] at the [[Apache incubator]]
* [http://www.taverna.org.uk/ Taverna] homepage (archive pre-Apache)

== References ==
{{reflist}}

[[Category:Bioinformatics software]]
[[Category:School of Computer Science, University of Manchester]]
[[Category:Workflow applications]]
[[Category:Apache Software Foundation|Taverna]]
[[Category:Apache Software Foundation projects|Taverna]]
[[Category:Software using the Apache license]]</text>
      <sha1>q7orjh0r0myj15c1f0s4t8fypnv7mbn</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Wave</title>
    <ns>0</ns>
    <id>22992426</id>
    <revision>
      <id>846435964</id>
      <parentid>841926514</parentid>
      <timestamp>2018-06-18T19:40:01Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v485)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29390">{{Infobox software
|name                       = Apache Wave
|logo                       = Apache Wave logo.png
|screenshot                 = Google Wave.png
|caption                    = Google Wave, the previous incarnation of Apache Wave
|collapsible                = 
|author                     = [[Google]]
|developer                  = [[Apache Software Foundation]], Google
|released                   = {{start date|2009|5|27}}
|latest release version     =
|latest release date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|programming language       = [[Java (programming language)|Java]]
|operating system           =
|status         = Retired (January 2018)&lt;ref&gt;http://incubator.apache.org/projects/wave.html&lt;/ref&gt;
|platform                   = [[Web application|Web platform]]
|genre                      = [[Collaborative real-time editor]]
|license                    = [[Apache License]]
|website                    = {{URL|incubator.apache.org/wave/}}
|repo                       = {{URL|https://git-wip-us.apache.org/repos/asf/incubator-wave.git}}
}}
'''Apache Wave''' was a software framework for [[Collaborative real-time editor|real-time collaborative editing]] online. [[Google]] originally developed it as '''Google Wave'''.&lt;ref&gt;{{cite web|url=http://wave.google.com/about.html
|title=Google Wave Overview|author=Google Inc.
|quote=[A] new web application for real-time communication and collaboration.
|year=2009|accessdate=May 2010| archiveurl= https://web.archive.org/web/20100427183005/http://wave.google.com/about.html| archivedate= 27 April 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
It was announced at the [[Google I/O]] conference on May 27, 2009.&lt;ref&gt;[[TechCrunch]] (May 28, 2009):
[https://www.techcrunch.com/2009/05/28/google-wave-drips-with-ambition-can-it-fulfill-googles-grand-web-vision/ Google Wave Drips With Ambition.  A New Communication Platform For A New Web.]&lt;/ref&gt;&lt;ref name="iokeynote"&gt;{{cite web
|url=https://www.youtube.com/watch?v=v_UyVmITiYQ
|title=I/O Conference Google Wave Keynote|author=Google Inc.}}&lt;/ref&gt;

Wave is a [[web application|web-based]] [[computing platform]] and [[communications protocol]] designed to merge key features of [[Media (communication)|communications media]] such as [[email]], [[instant messaging]], [[wiki]]s, and [[Social networking service|social networking]].&lt;ref name="aboutgw"&gt;{{cite web
|url=http://wave.google.com/help/wave/about.html|title=About Google Wave|author=Google Inc.}}&lt;/ref&gt; Communications using the system can be [[Synchronization|synchronous]] or [[Asynchronous communication#Electronically mediated communication|asynchronous]]. Software extensions provide contextual [[spell checker|spelling and grammar checking]], [[machine translation|automated language translation]]&lt;ref name="iokeynote" /&gt; and other features.&lt;ref name="gwdevblog"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2009/05/introducing-google-wave-apis-what-can.html|title=Google Wave Developer Blog|publisher=Google}}&lt;/ref&gt;

Initially released only to developers, a preview release of Google Wave was extended to 100,000 users in September 2009, each allowed to invite additional users. Google accepted most requests submitted starting November 29, 2009, soon after the September extended release of the technical preview. On May 19, 2010, it was released to the general public.&lt;ref&gt;Shankland, Stephen. (2010-05-19) [http://news.cnet.com/8301-30685_3-20005394-264.html Google Wave: Now open to the public | Deep Tech – CNET News]. News.cnet.com. Retrieved on 2010-12-14.&lt;/ref&gt;

On August 4, 2010, Google announced the suspension of stand-alone Wave development and the intent of maintaining the web site at least for the remainder of the year,&lt;ref&gt;[http://googleblog.blogspot.com/2010/08/update-on-google-wave.html Official Google Blog: Update on Google Wave]. Googleblog.blogspot.com (2010-04-08). Retrieved on 2010-12-14.&lt;/ref&gt; and on November 22, 2011, announced that existing Waves would become read-only in January 2012 and all Waves would be deleted in April 2012.&lt;ref&gt;{{cite web|url=http://googleblog.blogspot.com/2011/11/more-spring-cleaning-out-of-season.html |title=Official Blog: More spring cleaning out of season |publisher=Googleblog.blogspot.com |date=2011-11-22 |accessdate=2013-06-15}}&lt;/ref&gt; Development was handed over to the [[Apache Software Foundation]] which started to develop a server-based product called '''Wave in a Box'''.&lt;ref&gt;Meyer, David. (2010-09-03) [http://www.zdnet.co.uk/news/application-development/2010/09/03/google-puts-open-source-wave-in-a-box-40089999/ Google puts open-source Wave in a 'box' | Application Development | ZDNet UK]. Zdnet.co.uk. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;[http://www.idg.se/2.1085/1.355483/google-wave-inte-ute-ur-leken Google Wave inte ute ur leken]. IDG.se. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;Murphy, David. (1970-01-01) [https://www.pcmag.com/article2/0,2817,2368730,00.asp Google Spins Wave Into 'Wave in a Box' for Third-Party Use | News &amp; Opinion]. PCMag.com. Retrieved on 2010-12-14.&lt;/ref&gt; It was retired in January 2018.&lt;ref&gt;{{Cite web|url=http://incubator.apache.org/projects/wave.html|title=Wave Incubation Status - Apache Incubator|website=incubator.apache.org|access-date=2018-04-12}}&lt;/ref&gt;

==History==
[[File:Googlewave.svg|thumb|upright|The original logo while owned by Google]]

===Origin of name===
The science fiction television series ''[[Firefly (TV series)|Firefly]]'' provided the inspiration for the project's name.&lt;ref name=itnewsau&gt;{{cite news
 |first=Nate 
 |last=Cochrane 
 |url=http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx 
 |title=Opinion: Google's wave drowns the bling in Microsoft's Bing 
 |agency=IT News Australia 
 |date=2009-05-29 
 |accessdate=2009-06-03 
 |archiveurl=https://web.archive.org/web/20090603041903/http://www.itnews.com.au/News/104396%2Copinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx 
 |archivedate=3 June 2009 
 |deadurl=no 
}}&lt;/ref&gt; In the series, a ''wave'' is an electronic communication, often consisting of a [[video call]] or video message.&lt;ref name=itnewsau/&gt;  During the developer preview, a number of references were made to the series, such as [[Lars Rasmussen (Software Developer)|Lars Rasmussen]] replying to a message with "shiny", a word used in the series to mean ''cool'' or ''good'', and the crash message of Wave being a popular quotation from the series: "Curse your sudden but inevitable betrayal!"&lt;ref name="iokeynote" /&gt;&lt;ref&gt;Originally said by [[List of characters in the Firefly universe#Hoban Washburne|Wash]] at 6:36, in ''[[Serenity (Firefly episode)|Serenity]]''; [[Firefly (TV series)|Firefly]]: The Complete Series (Blu-ray), 2008, 20th Century Fox.&lt;/ref&gt; Another common error message, "Everything's shiny, Cap'n. Not to fret!" is a quote from [[Kaylee Frye]] in the 2005 motion-picture ''Firefly'' continuation, ''[[Serenity (2005 film)|Serenity]]'', and it is matched with a sign declaring that "This wave is experiencing some turbulence and might explode. If you don't want to explode..." which is another reference to the opening of the film.

During an event in [[Amsterdam]], [[Netherlands]],&lt;ref name="nextweb"&gt;{{cite news | first = Ralf | last = Rottmann | url = https://thenextweb.com/appetite/2009/10/30/breaking-google-wave-opened-federation-today-host/ | title = Google Wave to be opened for federation today! | agency = The Next Web | date = October 30, 2009}}&lt;/ref&gt; it became apparent that the 60-strong team that was then working on Wave in [[Sydney, Australia]] used [[Joss Whedon]]-related references to describe, among others, the sandbox version of Wave called ''[[Dollhouse (TV series)|Dollhouse]]'' after the TV-series by ''Firefly'' producer Joss Whedon, which was aired on Fox in the U.S. The development of external extensions was codenamed "Serenity", after the spaceship used in ''Firefly'' and ''Serenity''.

===Open source===
Google released most of the source code as [[open source software]],&lt;ref name="iokeynote"/&gt; allowing the public to develop its features through extensions.&lt;ref name="iokeynote" /&gt;  Google allowed third parties to build their own Wave services (be it private or commercial) because it wanted the [[Google Wave Federation Protocol|Wave protocol]] to replace the [[e-mail]] protocol.&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt; Initially, Google was the only Wave service provider, but it was hoped that other service providers would launch their own Wave services, possibly designing their own unique web-based clients as is common with many email service providers.  The possibility also existed for native Wave clients to be made, as demonstrated with their [[command-line interface|CLI]]-based console client.&lt;ref name="osreleasenext"&gt;{{cite web|url=https://groups.google.com/group/wave-protocol/browse_thread/thread/618ff4e9ef477e80?pli=1|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}&lt;/ref&gt;

Google released initial open-source components of Wave:&lt;ref name="osrelease1"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2009/07/google-wave-federation-protocol-and.html|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}&lt;/ref&gt;
# the [[operational transformation]] (OT) code,
# the underlying wave model, and
# a basic client/server prototype that uses the wave protocol

In addition, Google provided some detail about later phases of the open-source release:&lt;ref name="osreleasenext" /&gt;
# wave model code that is a simplified version of Google's production code and is tied to the OT code; this code will evolve into the shared code base that Google will use and expects that others will too
# a testing and verification suite for people who want to do their own implementation (for example, for porting the code to other languages)

===Reception===
{{Wikinews|Google to discontinue social networking application Google Wave}}
During the initial launch of Google Wave, invitations were widely sought by users and were sold on auction sites.&lt;ref&gt;[http://mashable.com/2009/09/30/google-wave-invite/ Google Wave Invite Selling for $70 on eBay]&lt;/ref&gt;&lt;!--  However, people were confused as to how to use it.&lt;ref&gt;[http://www.csmonitor.com/Innovation/Horizons/2009/1124/so-youve-got-google-wave-now-what Christian Science Monitor: So you've got Google wave, now what?]&lt;/ref&gt; Google Wave was called an "over-hyped disappointment for the first generation of users"&lt;ref&gt;[http://www.linux-mag.com/id/7653 Linux Mag 2009 Review]&lt;/ref&gt; with "dismal usability"&lt;ref&gt;[http://themilwaukeeseo.com/2009/12/14/the-google-wave-failure/ Google Wave Failure on Milwaukee SEO]&lt;/ref&gt; that "humans may not be able to comprehend."&lt;ref name="EngagetWave"&gt;[https://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]&lt;/ref&gt; --&gt;
Those who received invitations and decided to test Google Wave could not communicate with their contacts on their regular email accounts. The initial spread of Wave was very restricted.

===End of development of ''Google Wave''===
Google Wave initially received positive press coverage for its design&lt;ref&gt;[http://news.bbc.co.uk/1/hi/technology/8282687.stm B.B.C. report introducing Google Wave in September 2009]&lt;/ref&gt; and potential uses.&lt;ref name="EngagetWave"&gt;[https://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]&lt;/ref&gt;&lt;ref&gt;[http://asia.cnet.com/blogs/tokyo-shift/post.htm?id=63015591 CNET Predictions for 2010]&lt;/ref&gt; On August 4, 2010, Google announced Wave would no longer be developed as a stand-alone product due to a lack of interest.&lt;ref name="ZDNet on GW's death"&gt;[http://www.zdnet.com/blog/google/how-will-google-wave-be-reincarnated/2344 ZDNet on GW's death]&lt;/ref&gt; Google's statement surprised many in the industry and user community.

Google later clarified the Wave service would be available until [[Google Docs]] was capable of accessing saved waves.&lt;ref&gt;{{cite web|url=http://www.google.com/support/wave/bin/answer.py?answer=1083134 |title=Status of Google Wave - Google Help |publisher=Google.com |date= |accessdate=2013-06-15}}&lt;/ref&gt;

Response to the news of the end of development came from Wave users in the form of a website.&lt;ref&gt;[http://www.webpronews.com/topnews/2010/08/09/save-google-wave-site-forms '"Save Google Wave" Site Forms']&lt;/ref&gt; Since their announcement in early August, the website has recorded over 49,000 supporter registrations urging Google Wave's continuation.&lt;ref&gt;[http://www.savegooglewave.com Save Google Wave!]. Retrieved on 2011-05-14.&lt;/ref&gt;

In retrospect, the lack of success of Google Wave was attributed among other things to its complicated user interface resulting in a product that merged features of email, instant messengers and wikis but ultimately failed to do anything significantly better than the existing solutions.&lt;ref&gt;[https://arstechnica.com/software/news/2010/08/google-wave-why-we-didnt-use-it.ars Google Wave: why we didn't use it], [[Ars Technica]]&lt;/ref&gt;

Chris Dawson of online technology magazine [[Zdnet]] discussed inconsistencies in the reasoning of Google in deciding to end support for Wave,&lt;ref name="ZDNet on GW's death"/&gt; mentioning its "deep involvement" in developing social media networks, to which many of Wave's capabilities are ideally suited.

===Apache Wave===
Google Wave was accepted by the [[Apache Software Foundation]]'s Incubator program under the project name Apache Wave. The Google Wave Developer blog was updated with news of the change on December 6, 2010.&lt;ref&gt;North, Alex. (2010-12-06) [http://googlewavedev.blogspot.com/2010/12/introducing-apache-wave.html Google Wave Developer Blog: Introducing Apache Wave]. Googlewavedev.blogspot.com. Retrieved on 2010-12-14.&lt;/ref&gt; A Wave Proposal page with details on the project's goals was created on the Apache Foundation's Incubator Wiki.&lt;ref&gt;[http://wiki.apache.org/incubator/WaveProposal WaveProposal – Incubator Wiki]. Wiki.apache.org (2010-11-24). Retrieved on 2010-12-14.&lt;/ref&gt;

====Wave in a Box====
[[File:Wave in a Box logo.png|thumb|75px|The logo for Wave in a Box]]
Wave in a Box is the current server implementation of Apache Wave.  Currently, there are not any demo servers available.&lt;ref name=demo_servers&gt;{{cite web|title=Wave in a Box demo servers|url=http://incubator.apache.org/wave/demo-servers.html|publisher=Apache Software Foundation|accessdate=10 October 2012}}&lt;/ref&gt;

====Crisis and SwellRT====
{{further information|SwellRT}}
In 2016, several discussions took place within the Apache Wave community, aiming to tackle the stagnation and crisis state of the project. The Apache Software Foundation mentor of Apache Wave, Upayavira,&lt;ref&gt;{{Cite web|url=https://incubator.apache.org/projects/wave.html|title=Wave Incubation Status - Apache Incubator|website=incubator.apache.org|access-date=2017-10-28}}&lt;/ref&gt; was concerned on the project stagnation, but framed [[SwellRT]] (a fork which re-engineered Wave into a backend-as-a-service for building apps) as Wave's potential savior.&lt;ref&gt;{{Cite web|url=https://wiki.apache.org/incubator/September2016|title=September2016 - Incubator Wiki|website=wiki.apache.org|access-date=2017-10-28}}&lt;/ref&gt; Eventually, Wave was approved to continue within Apache incubator program, and a copy of SwellRT codebase was placed in the Apache Wave repository in order to grant the Wave community access to it.&lt;ref&gt;{{Cite web|url=https://wiki.apache.org/incubator/December2016|title=December2016 - Incubator Wiki|website=wiki.apache.org|access-date=2017-10-28}}&lt;/ref&gt; In this regard, Intellectual Property of SwellRT was transferred to the Apache Foundation in 2017.&lt;ref&gt;{{Cite web|url=https://wiki.apache.org/incubator/March2017|title=March2017 - Incubator Wiki|website=wiki.apache.org|access-date=2017-10-28}}&lt;/ref&gt; Still this was not sufficient to resurrect Wave's developer community, and SwellRT continued as independent project. 
====Retirement====
The Wave project retired on 2018-01-15, having never left incubator status.&lt;ref name=retirement&gt;{{cite web|title=Wave Incubation Status - Apache Incubator|url=http://incubator.apache.org/projects/wave.html|publisher=Apache Software Foundation|accessdate=17 January 2018}}&lt;/ref&gt;

==Features==
Google Wave was a new [[Internet]] [[communications]] platform. It was written in [[Java (programming language)|Java]] using [[OpenJDK]] and its web interface used the [[Google Web Toolkit]]. Google Wave works like previous messaging systems such as [[email]] and [[Usenet]], but instead of sending a message along with its entire thread of previous messages, or requiring all responses to be stored in each user's inbox for context, message documents (referred to as ''waves'') that contain complete threads of multimedia messages (blips) are perpetually stored on a central server. Waves are shared with collaborators who can be added or removed from the wave at any point during a wave's existence.

Waves, described by Google as "''equal parts conversation and document''", are hosted [[XML]] documents that allow seamless and low latency concurrent modifications.&lt;ref name="ot"&gt;[http://www.waveprotocol.org/whitepapers/operational-transform Google Wave Operational Transformation – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt; Any participant of a wave can reply anywhere within the message, edit any part of the wave, and add participants at any point in the process. Each edit/reply is a blip and users can reply to individual blips within waves. Recipients are notified of changes/replies in all waves in which they are active and, upon opening a wave, may review those changes in chronological order. In addition, waves are live. All replies/edits are visible in real-time, letter-by-letter, as they are typed by the other collaborators. Multiple participants may edit a single wave simultaneously in Google Wave. Thus, waves can function not only as e-mails and [[threaded discussion|threaded conversations]] but also as an [[instant messaging]] service when many participants are online at the same time. A wave may repeatedly shift roles between e-mail and instant messaging depending on the number of users editing it concurrently. The ability to show messages as they are typed can be disabled, similar to conventional instant messaging.&lt;ref name="aboutgw"/&gt;

The ability to modify a wave at any location lets users create collaborative documents, [[collaborative editing|edited]] in a manner akin to [[wiki]]s. Waves can easily link to other waves. In many respects, it is a more advanced forum.&lt;ref&gt;[http://variableghz.com/2009/10/google-wave-review/ Google Wave Review]. VariableGHz (2009-10-13). Retrieved on 2010-12-14.&lt;/ref&gt; It can be read and known to exist by only one person, or by two or more and can also be public, available for reading ''and'' writing to everyone on the Wave.

The history of each wave is stored within it. Collaborators may use a playback feature to observe the order in which it was edited, blips that were added, and who was responsible for what in the wave.&lt;ref name="aboutgw"/&gt;&lt;ref name="gwdevblog"/&gt; The history may also be searched by a user to view and/or modify specific changes, such as specific kinds of changes or messages from a single user.&lt;ref name="iokeynote" /&gt;

==Extension programming interface==
{{anchor|Google Wave extensions}}
Google Wave is extensible through an [[application programming interface]] (API). It provides extensions in the form of ''Gadgets'' and ''Robots'', and is embeddable by dropping interactive windows into a given wave on external sites, such as [[blog]] sites.&lt;ref name="iokeynote" /&gt;&lt;ref name="codepage" /&gt;

The last version of robots API is 2.0.&lt;ref name="robotsapiv2"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2010/03/introducing-robots-api-v2-rise-of.html|title=Introducing Robots API v2: The Rise of Active Robots|publisher=Google}}&lt;/ref&gt;

Google Wave also supports extension installers, which bundle back-end elements (robots and gadgets) and front-end user interface elements into an integrated package. Users may install extensions directly within the Wave client using an extension installer.

===Extensions===
Google Wave extensions are [[Plug-in (computing)|add-ins]] that may be installed on Google Wave to enhance its functionality. They may be [[Internet bot]]s (robots) to automate common tasks, or gadgets to extend or change user interaction features, e.g., posting blips on [[microblog]] feeds or providing RSVP recording mechanisms.&lt;ref name="iokeynote" /&gt;&lt;ref name="aboutgw" /&gt;&lt;ref name="codepage"&gt;{{cite web|url=https://code.google.com/apis/wave/|title=Google Wave API – Google Code|publisher=Google}}&lt;/ref&gt;

Over 150 Google Wave extensions have been developed either in the form of Gadgets or Robots.&lt;ref&gt;[https://wave-samples-gallery.appspot.com/ Google Wave Samples Gallery] {{webarchive|url=https://web.archive.org/web/20100415004922/http://wave-samples-gallery.appspot.com/ |date=2010-04-15 }}. Wave-samples-gallery.appspot.com. Retrieved on 2010-12-14.&lt;/ref&gt;

====Robots====
A robot is an automated participant on a wave. It can read the contents of a wave in which it participates, modify its contents, add or remove participants, and create new blips or new waves. Robots perform actions in response to events. For example, a robot might publish the contents of a wave to a public [[blog]] site and update the wave with user comments.

Robots may be added as participants to the Wave itself. In theory, a robot can be added anywhere a human participant can be involved.

====Gadgets====
Gadget extensions are applications that run within the wave, and to which all participants have access. Robots and Gadgets can be used together, but they generally serve different purposes. A gadget is an application users could participate with, many of which are built on Google’s [[OpenSocial]] platform. A good comparison would be iGoogle gadgets or Facebook applications.

The gadget is triggered based on the user action. They can be best described as applications installed on a mobile phone. For example, a wave might include a [[sudoku]] gadget that lets the wave participants compete to see who can solve the puzzle first.

Gadgets may be added to individual waves and all the participants share and interact with the gadget.

==Federation protocol==
{{Main article|Google Wave Federation Protocol}}
Google Wave provides [[Federation (information technology)|federation]] using an extension of [[XMPP|Extensible Messaging and Presence Protocol]] (XMPP), the [[open standard|open]] [[Google Wave Federation Protocol|Wave Federation Protocol]]. Being an open protocol, anyone can use it to build a custom Wave system and become a wave provider.&lt;ref&gt;{{cite web|url=http://www.waveprotocol.org/|title=Google Wave Federation Protocol|publisher=Google}}&lt;/ref&gt;  The use of an open protocol is intended to parallel the openness and ease of adoption of the [[e-mail]] protocol and, like e-mail, allow communication regardless of provider. Google hoped that waves would replace e-mail as the dominant form of Internet communication.&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture"&gt;[http://www.waveprotocol.org/whitepapers/google-wave-architecture Google Wave Federation Architecture – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref name="wpcsmodel"&gt;[http://www.waveprotocol.org/whitepapers/internal-client-server-protocol Google Wave Client-Server Protocol – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt;  In this way, Google intended to be only one of many wave providers&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;  and to also be used as a supplement to [[e-mail]], [[instant messaging]], [[FTP]], etc.

A key feature of the protocol is that waves are stored on the service provider's servers instead of being sent between users. Waves are federated; copies of waves and wavelets are distributed by the wave provider of the originating user to the providers of all other participants in a particular wave or wavelet so all participants have immediate access to up-to-date content. The originating wave server is responsible for hosting, processing, and concurrency control of waves.&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;  The protocol allows private reply wavelets within parent waves, where other participants have no access or knowledge of them.&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;

Security for the communications is provided via [[Transport Layer Security]] authentication, and encrypted connections and waves/wavelets are identified uniquely by a service provider's [[domain name]] and ID strings. User-data is not federated, that is, not shared with other wave providers.

===Adoption of Wave Protocol and Wave Federation Protocol===
Besides Apache Wave itself, there are other open-source variants of servers and clients with different percentage of Wave Federation and Wave Protocol support. Wave was re-engineered into a backend-as-a-service solution by the [[SwellRT]] project. Wave has been adopted in different forms for corporate applications by Novell for [[Novell Pulse]],&lt;ref&gt;[http://www.novell.com/products/pulse/ Novell Vibe cloud service]. Novell.com. Retrieved on 2010-12-14.&lt;/ref&gt; or by [[SAP AG|SAP]] for Cloudave,&lt;ref&gt;Elliott, Timo. (2009-10-19) [http://www.cloudave.com/link/sap-gravity-prototype-business-collaboration-using-google-wave SAP's Gravity Prototype: Business Collaboration Using Google Wave]. Cloudave.com. Retrieved on 2010-12-14.&lt;/ref&gt; and community projects such as PyOfWave or [[Kune (software)|Kune]].

====Compatible third-party servers====
The following servers are compatible with the Google Wave protocol:
* '''[[Kune (software)|Kune]]'''&lt;ref&gt;{{cite web|title=Kune Homepage|url=http://kune.ourproject.org|accessdate=22 April 2012}}&lt;/ref&gt; is a free/open source platform for social networking, collaborative work and web publishing, focusing on work groups and organizations rather than in individuals. It provides lists, tasks, documents, galleries, etc., while using waves underneath. It focuses on [[free culture movement|free culture]] and [[social movements]] needs.
* '''[[Novell Vibe]]''', formerly known as Novell Pulse&lt;ref&gt;[http://www.novell.com/promo/vibe.html Novell Vibe]. Novell.com (2009-12-31). Retrieved on 2010-12-14.&lt;/ref&gt;
* '''Rizzoma'''&lt;ref&gt;{{cite web|title=Rizzoma Homepage|url=http://rizzoma.com|accessdate=9 May 2012}}&lt;/ref&gt; is a platform for collaborative work in real time. It allows communication within a certain context permitting a chat to instantly become a document where topics of a discussion organized into branches of mind-map diagram and minor details are collapsed to avoid distraction. The user is able to sign in using a Google or Facebook account and choose whether your topics are private or public.
* '''[[SAP StreamWork]]''' is a collaboration decision making service.&lt;ref&gt;Williams, Alex. (2010-05-17) [http://www.readwriteweb.com/cloud/2010/05/sap-streamworks-integrates-wit.php SAP StreamWork Integrates With Google Wave – ReadWriteCloud]. Readwriteweb.com. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;[http://www.sapstreamwork.com/how-it-works/ How It Works | SAP® StreamWork™]. Sapstreamwork.com. Retrieved on 2010-12-14.&lt;/ref&gt;
* '''[[SwellRT]]'''&lt;ref&gt;{{Cite news|url=http://swellrt.org/|title=SwellRT homepage|last=|first=|date=|work=SwellRT|access-date=2017-12-17|archive-url=|archive-date=|dead-url=|language=en-US}}&lt;/ref&gt; is a backend-as-a-service for building collaborative and federated apps. It is a fork which re-engineered Apache Wave, and was adopted within the Apache Wave project.

==See also==
{{Portal|Software}}
* [[Microsoft Sharepoint Workspace]]
* [[Real-time text]]
* [[SwellRT]]

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Commons category|Apache Wave}}
* [http://incubator.apache.org/wave/ Apache Wave]
* [http://incubator.apache.org/wave/demo-servers.html Wave in a Box]
* [http://wave.google.com/ Google Wave]
* [https://code.google.com/apis/wave/ Google Wave API]
* [http://googlewavedev.blogspot.com/ Google Wave Developer Blog]
* [https://www.youtube.com/watch?v=v_UyVmITiYQ Full Video of the Developer Preview at Google IO on ]
* [https://www.youtube.com/watch?v=p6pgxLaDdQw Google Wave overview video]
* [http://www.waveprotocol.org/ Google Wave Federation Protocol]

{{Google Inc.}}
{{Apache}}

[[Category:Discontinued Google services|Wave]]
[[Category:Web applications]]
[[Category:Computing platforms]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Internet Protocol based network software]]
[[Category:Self-organization]]
[[Category:Blogging]]
[[Category:Collaborative real-time editors]]
[[Category:2009 software]]
[[Category:Products and services discontinued in 2010]]
[[Category:Discontinued software]]
[[Category:Discontinued Google software]]
[[Category:Software using the Apache license]]
[[Category:Social networking services]]
[[Category:Apache Software Foundation|Wave]]</text>
      <sha1>obvsuc1zlgznzej4xf1c97qwd8t129l</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Axis2</title>
    <ns>0</ns>
    <id>9442659</id>
    <revision>
      <id>838336239</id>
      <parentid>791905774</parentid>
      <timestamp>2018-04-26T11:09:17Z</timestamp>
      <contributor>
        <ip>195.65.154.67</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7909">{{Infobox Software 
| name                   = Apache Axis2
| logo                   = &lt;!-- Deleted image removed: [[Image:Apache Axis2 Logo.jpg|150px|Apache Axis2 Logo]] --&gt;
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 1.7.7
| latest release date    = {{release date|2017|11|21}}
| operating system       = [[Cross-platform]] 
| programming language   = [[Java (programming language)|Java]] and [[C (programming language)|C]]
| genre                  = [[Web service]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://axis.apache.org}}
}}
{{Update||24 January 2016|date=January 2016}}
'''Apache Axis2''' is a core engine for [[Web services]]. It is a complete re-design and re-write of the widely used [[Apache Axis]] [[SOAP]] stack. Implementations of Axis2 are available in [[Java (programming language)|Java]] and [[C (programming language)|C]].

Axis2 provides the capability to add [[Web service|Web services]] interfaces to [[Web application|Web applications]]. It can also function as a standalone [[application server]].

==Why Apache Axis2==
A new architecture for Axis2 was introduced during the August 2004 Axis2 Summit in Colombo, Sri Lanka. The new architecture on which Axis2 is based is more flexible, efficient and configurable in comparison to Axis1.x architecture. Some well-established concepts from Axis 1.x, like handlers etc., have been preserved in the new architecture.

Apache Axis2 supports SOAP 1.1 and SOAP 1.2, and it has integrated support for the widely popular [[Representational state transfer|REST]] style of Web services. The same business-logic implementation can offer both a WS-* style interface as well as a [[REST]]/[[Plain Old XML|POX]] style interface simultaneously.

Axis2/Java has support for [[Spring Framework]].

Axis2/C is a high-performance Web services implementation. It has been implemented with portability and ability to be embedded or hosted in Apache Httpd, Microsoft IIS or Axis Http Server. [http://wso2.com/library/3532/ See article about Apache Axis2/C Performance (2008)].
Latest release occurred on 2009 [http://axis.apache.org/axis2/c/core/download.cgi].

Axis2 came with many new features, enhancements and industry specification implementations. Key features offered include:

==Axis2 Features==
Apache Axis2 includes support for following standards:
*[[WS-ReliableMessaging|WS - ReliableMessaging]] - Via [[Apache Sandesha2]]
*[[WS-Coordination|WS - Coordination]] - Via [[Apache Kandula2]]
*[[WS-AtomicTransaction|WS - AtomicTransaction]] - Via Apache Kandula2
*[[WS-SecurityPolicy|WS - SecurityPolicy]] - Via [[Apache Rampart]]
*[[WS-Security|WS - Security]] - Via Apache Rampart
*[[WS-Trust|WS - Trust]] - Via Apache Rampart
*[[WS-SecureConversation|WS - SecureConversation]] - Via Apache Rampart
*[[SAML 1.1]] - Via Apache Rampart
*[[SAML 2.0]] - Via Apache Rampart
*[[WS-Addressing|WS - Addressing]] - Module included as part of Axis2 core

Below a list of features and selling points cited from the Apache axis site:
*'''Speed''' - Axis2 uses its own object model and [[StAX|StAX (Streaming API for XML)]] parsing to achieve significantly greater speed than earlier versions of Apache Axis.
*'''Low memory foot print''' - Axis2 was designed ground-up keeping low memory foot print in mind.
*'''AXIOM''' {{Anchor|Axiom}} - Axis2 comes with its own light-weight object model, [https://ws.apache.org/axiom/ AXIOM], for message processing which is extensible, optimized for performance, and simplified for developers.
*'''Hot Deployment''' - Axis2 is equipped with the capability of deploying Web services and handlers while the system is up and running. In other words, new services can be added to the system without having to shut down the server. Simply drop the required Web service archive into the services directory in the repository, and the deployment model will automatically deploy the service and make it available for use.
*'''Asynchronous Web services''' - Axis2 now supports asynchronous Web services and asynchronous Web services invocation using non-blocking clients and transports.
*'''MEP Support''' - Axis2 now comes handy with the flexibility to support Message Exchange Patterns (MEPs) with in-built support for basic MEPs defined in [[Web Services Description Language|WSDL]] 2.0.
*'''Flexibility''' - The Axis2 architecture gives the developer complete freedom to insert extensions into the engine for custom header processing, system management, and anything else you can imagine.
*'''Stability''' - Axis2 defines a set of published interfaces which change relatively slowly compared to the rest of Axis.
*'''Component-oriented Deployment''' - You can easily define reusable networks of Handlers to implement common patterns of processing for your applications, or to distribute to partners.
*'''Transport Framework''' - We have a clean and simple abstraction for integrating and using Transports (i.e., senders and listeners for SOAP over various protocols such as SMTP, FTP, [[message-oriented middleware]], etc.), and the core of the engine is completely transport-independent.
*'''WSDL support''' - Axis2 supports the [[Web Services Description Language]], version 1.1 and 2.0, which allows you to easily build stubs to access remote services, and also to automatically export machine-readable descriptions of your deployed services from Axis2.
*'''Add-ons''' - Several Web services specifications have been incorporated including [http://ws.apache.org/wss4j/ WSS4J] for security (Apache Rampart), Sandesha for reliable messaging, Kandula which is an encapsulation of [[WS-Coordination]], [[WS-AtomicTransaction]] and WS-BusinessActivity.
*'''Composition and Extensibility''' - Modules and phases improve support for composability and extensibility. Modules support composability and can also support new WS-* specifications in a simple and clean manner. They are however not hot deployable as they change the overall behavior of the system.

==Axis2 Modules==
Axis2 modules provides [[Quality of service|QoS]] features like security, reliable messaging, etc.

*[[Apache Rampart module]] - Apache Rampart modules adds [[WS-Security]] features to Axis2 engine
*Apache Sandesha module - An implementation of [[WS-ReliableMessaging]] specification

==Related technologies==
*[[Apache Axis]]
*[[Apache CXF]], other Apache web services framework (old [[Codehaus XFire|XFire]] &amp; [[Celtix]])
*[[Java Web Services Development Pack]], web services framework
*[[XML Interface for Network Services]], RPC/web services framework
*[[Web Services Invocation Framework]], Java API for invoking Web services
*AlchemySOAP, [[C++]] open source SOAP-based web services framework

==Axis2 Books==
*Quickstart Apache Axis2.

==External links==
{{Portal|Java}} 
*[https://axis.apache.org/axis/ Apache Axis Homepage] at the Apache Software Foundation
*[https://axis.apache.org/axis2/java/core/ Apache Axis2/Java] at the Apache Software Foundation
*[https://axis.apache.org/axis2/c/core/ Apache Axis2/C] at the Apache Software Foundation
*[https://ws.apache.org/axis2/modules/index.html Apache Axis2 Module Page]
*[http://www.ibm.com/developerworks/webservices/library/ws-apacheaxis/index.html Web services using Apache Axis2]
*[http://robaustin.wikidot.com/axis How to run an Axis2 client running against a Windows Web Server] - Rob Austin
*{{cite conference | title = Axis2, Middleware for Next Generation Web Services | citeseerx = 10.1.1.62.1740 | booktitle = Proceedings of the IEEE International Conference on Web Services | conference = ICWS '06 }}
*[http://www.journaldev.com/255/axis2-web-services-tutorial Axis2 Tutorial]

{{Apache}}

[[Category:Apache Software Foundation|Axis2]]
[[Category:Web services]]
[[Category:Web service specifications]]
[[Category:Java enterprise platform]]</text>
      <sha1>md6vdv7vh6l1ys0a0kn6taltb2979h8</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Rampart module</title>
    <ns>0</ns>
    <id>11378675</id>
    <revision>
      <id>789525562</id>
      <parentid>744597362</parentid>
      <timestamp>2017-07-07T21:54:29Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1984">{{Infobox Software
| name                   = Apache Rampart Module
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| released               = 2006
| latest release version = 1.5
| latest release date    = {{release date|mf=yes|2010|02|01}}
| programming language   = [[Java (programming language)]]
| operating system       = [[Cross-platform]]
| language               = English
| genre                  = [[Axis2 module]]
| license                = [[Apache License]] 2.0
| website                = http://ws.apache.org/rampart/
}}

'''Apache Rampart''' is an implementation of the [[WS-Security]] standard for the [[Axis2]] [[Web services]] engine by the [[Apache Software Foundation]]. It supplies security features to web services by implementing the following specifications: 

* [[WS-Security]]
* [[WS-SecurityPolicy]]
* [[WS-Trust]]
* [[WS-SecureConversation]]
* [[SAML 1.1]]
* [[SAML 2.0]]

== See also ==
* [[Free Software Foundation]]
* [[Open Software Foundation]]

==External links==
* [http://ws.apache.org/rampart/ Apache Rampart] at the Apache Software Foundation
* [http://ws.apache.org/rampart/c/ Apache Rampart C-language implementation] at the Apache Software Foundation
* [http://ws.apache.org/axis2/ Apache AXIS2] at the Apache Software Foundation
* [http://download.boulder.ibm.com/ibmdl/pub/software/dw/specs/ws-secpol/ws-secpol.pdf Web Services Security Policy Language V1.1 specification]
* [https://web.archive.org/web/20090608174959/http://www.ibm.com/developerworks/webservices/library/specification/ws-secmap/ Security in a Web Services World: A Proposed Architecture and Roadmap]
* [http://docs.oasis-open.org/ws-sx/ws-securitypolicy/200702/ws-securitypolicy-1.2-spec-os.html WS-SecurityPolicy 1.2 OASIS Standard]

[[Category:Computer security software]]
[[Category:Apache Software Foundation|Rampart module]]
[[Category:Web services]]

{{web-software-stub}}</text>
      <sha1>h6ipii2ug9rf7vt8qwrwno3wocf6nxj</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Struts 2</title>
    <ns>0</ns>
    <id>41721636</id>
    <revision>
      <id>844677338</id>
      <parentid>828103198</parentid>
      <timestamp>2018-06-06T11:40:17Z</timestamp>
      <contributor>
        <username>Flyingfischer</username>
        <id>20140660</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4459">{{for|the predecessor of Apache Struts 2|Apache Struts 1}}
{{Infobox software
| name                   = Apache Struts 2
| logo                   = [[File:Struts logo.gif|frameless|Apache Struts Logo]]
| developer              = [[Apache Software Foundation]]
| released               = {{Start date|2006|10|10}}
| latest release version = 2.5.16
| latest release date    = {{release date|2018|03|16}}
&lt;!-- we may reuse this again
| latest preview version = '''2.5-BETA3'''&lt;ref&gt;[https://struts.apache.org/announce.html#a20160126 ''26 January 2016 - Struts 2.5-BETA3 (BETA)'']&lt;/ref&gt;
--&gt;
| latest preview date    = {{start date and age|2016|01|26}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| platform               = [[Cross-platform]] ([[Java Virtual Machine|JVM]])
| license                = [[Apache License]] 2.0
| website                = {{URL|http://struts.apache.org/}}
}}
'''Apache Struts 2''' is an [[open-source]] [[web application framework]] for developing [[Java EE]] [[web application]]s. It uses and extends the [[Java Servlet]] [[application programming interface|API]] to encourage developers to adopt a [[model–view–controller]] (MVC) architecture. The [[WebWork]] framework spun off from [[Apache Struts]] aiming to offer enhancements and refinements while retaining the same general architecture of the original Struts framework. In December 2005, it was announced that WebWork 2.2 was adopted as Apache Struts 2, which reached its first full release in February 2007.&lt;ref&gt;[http://struts.apache.org/release/2.2.x/ About Apache Struts 2] {{webarchive |url=https://web.archive.org/web/20140114170139/http://struts.apache.org/release/2.2.x/ |date=January 14, 2014 }}&lt;/ref&gt;

Struts 2 has a history of critical security bugs,&lt;ref&gt;{{cite web |url=https://www.cvedetails.com/vulnerability-list/vendor_id-45/product_id-6117/Apache-Struts.html |title=Apache Struts : List of security vulnerabilities |website=cvedetails.com |accessdate=October 2, 2017}}&lt;/ref&gt; many tied to its use of [[OGNL]] technology;&lt;ref&gt;{{cite web |url=https://community.saas.hpe.com/t5/Security-Research/Struts-2-OGNL-Expression-Injections/ba-p/288881#.WdL6ca2ZNxw |title=Struts 2: OGNL Expression Injections |first=Alvaro |last=Munoz |website=HPE.com |date=January 14, 2014 |accessdate=October 2, 2017}}&lt;/ref&gt; some vulnerabilities can lead to [[arbitrary code execution]]. In October 2017, it was reported that failure by [[Equifax]] to address a Struts 2 vulnerability advised in March 2017 was later exploited in the [[data breach]] that was disclosed by Equifax in September 2017.&lt;ref&gt;{{cite web |url=https://www.theregister.co.uk/2017/10/02/equifax_ceo_richard_smith_congressional_testimony/?mt=1506988904204 |title=Equifax couldn't find or patch vulnerable Struts implementations |first=Richard |last=Chirgwin |website=[[The Register]] |date=October 2, 2017 |accessdate=October 2, 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://arstechnica.com/information-technology/2017/10/a-series-of-delays-and-major-errors-led-to-massive-equifax-breach/ |title=A series of delays and major errors led to massive Equifax breach |first=Dan |last=Goodin |website=[[Ars Technica]] |date=October 2, 2017 |accessdate=October 2, 2017}}&lt;/ref&gt;

== Features ==
* Simple [[Plain Old Java Object|POJO]]-based actions
* Simplified testability
* Thread safe
* [[Ajax (programming)|AJAX]] support
** [[jQuery]] plugin
** [[Dojo Toolkit]] plugin (deprecated)
** Ajax client-side validation
* Template support
* Support for different result types
* Easy to extend with plugins
** [[Representational state transfer|REST]] plugin (REST-based actions, extension-less URLs)
** Convention plugin (action configuration via Conventions and Annotations)
** Spring plugin ([[dependency injection]])
** [[Hibernate (Java)|Hibernate]] plugin
** Support in design
** JFreechart plugin (charts)
** [[jQuery]] plugin (Ajax support, UI widgets, dynamic table, charts)
** Rome plugin
**plugin

== See also ==
*[[Comparison of web frameworks]]

== References ==
{{Reflist}}

== External links ==
* {{Official website|http://struts.apache.org/}}

{{Web frameworks}}
{{apache}}

[[Category:Apache Software Foundation|Struts 2]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java enterprise platform]]
[[Category:Web frameworks]]
[[Category:Software using the Apache license]]</text>
      <sha1>8x4cvqf95dth3e1kvz4qyc7p638u4zp</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Subversion</title>
    <ns>0</ns>
    <id>144868</id>
    <revision>
      <id>845115383</id>
      <parentid>844833396</parentid>
      <timestamp>2018-06-09T13:55:36Z</timestamp>
      <contributor>
        <ip>108.45.150.80</ip>
      </contributor>
      <comment>/* Subversion tags and branches */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33582">{{Infobox software
| name                   = Subversion
| title                  = 
| logo                   = Subversion Logo.svg
| logo size              =
| logo caption           = 
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = [[CollabNet]]
| developer              = [[Apache Software Foundation]]
| released               = {{Start date and age|2000|10|20|df=yes/no}}
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| status                 = Active
| programming language   = [[C (programming language)|C]]
| operating system       = [[Cross-platform]]
| platform               = 
| size                   = 
| language               = 
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| genre                  = [[Revision control]]
| license                = [[Apache License]] version 2.0
| alexa                  = 
| website                = {{URL|//subversion.apache.org/}}
| standard               = 
| AsOf                   = 
}}
'''Apache Subversion''' (often abbreviated '''SVN''', after its command name ''svn'') is a [[software versioning]] and [[revision control]] system distributed as [[open source]] under the [[Apache License]].&lt;ref&gt;{{cite web |url=http://directory.fsf.org/wiki/Subversion#tab=Details |title=Subversion |year=2013 |website=directory.fsf.org |publisher=[[Free Software Directory]] |accessdate=11 September 2013}}&lt;/ref&gt; Software developers use Subversion to maintain current and historical versions of files such as [[source code]], web pages, and documentation.  Its goal is to be a mostly compatible successor to the widely used [[Concurrent Versions System]] (CVS).

The [[open source]] community has used Subversion widely: for example in projects such as [[Apache Software Foundation]], [[Free Pascal]], [[FreeBSD]], [[GNU Compiler Collection|GCC]] and [[SourceForge]]. [[CodePlex]] used to offer access to Subversion as well as to other types of clients.

Subversion was created by [[CollabNet]] Inc. in 2000, and is now a top-level Apache project being built and used by a global community of contributors.

==History==
[[CollabNet]] founded the Subversion project in 2000 as an effort to write an open-source version-control system which operated much like [[Concurrent Versions System|CVS]] but which fixed the bugs and supplied some features missing in CVS.&lt;ref name=bookhistory /&gt; By 2001, Subversion had advanced sufficiently to [[Self-hosting|host its own source code]],&lt;ref name=bookhistory&gt;{{cite web |url=http://svnbook.red-bean.com/en/1.7/svn.intro.whatis.html#svn.intro.history |work=Version Control with Subversion (for Subversion 1.7) |title=What is Subversion? &gt; Subversion's History |author1=Collins-Sussman, Ben |author2=Brian W. Fitzpatrick |author3=C. Michael Pilato |year=2011 |accessdate=15 March 2012}}&lt;/ref&gt; and in February 2004, version 1.0 was released.&lt;ref&gt;{{cite web |url=https://lwn.net/Articles/72498/ |work=Linux Weekly News |title=subversion 1.0 is released |author=Benjamin Zeiss |year=2004 |accessdate=30 March 2014}}&lt;/ref&gt; In November 2009, Subversion was accepted into [[Apache Incubator]]: this marked the beginning of the process to become a standard top-level Apache project.&lt;ref name=SDT2011&gt;{{cite web |url=http://www.sdtimes.com/link/33886 |title=Subversion joins forces with Apache |author=Rubinstein, David |publisher=SD Times |date=4 November 2009 |accessdate=15 March 2012 |archiveurl=https://web.archive.org/web/20091111000653/http://www.sdtimes.com/SUBVERSION_JOINS_FORCES_WITH_APACHE/By_David_Rubinstein/About_APACHE_and_SUBVERSION/33886 |archivedate=2009-11-11}}&lt;/ref&gt; It became a top-level Apache project on February 17, 2010.&lt;ref&gt;{{cite web |url=http://subversion.wandisco.com/component/content/article/1/43.html |archiveurl=https://web.archive.org/web/20110512171259/http://subversion.wandisco.com/component/content/article/1/43.html |archivedate=12 May 2011 |title=Subversion is now Apache Subversion |date=18 February 2010 |accessdate=15 March 2012}}&lt;/ref&gt;

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date
! Status
|-
| {{Version|o|1.0}}
| 2004-02-23
| 1.0.9
| 2004-10-13
| No longer supported
|-
| {{Version|o|1.1}}
| 2004-09-29&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.1.html |title=Subversion 1.1 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.1.4
| 2005-04-01
| No longer supported
|-
| {{Version|o|1.2}}
| 2005-05-21&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.2.html |title=Subversion 1.2 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.2.3
| 2005-08-19
| No longer supported
|-
| {{Version|o|1.3}}
| 2005-12-30&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.3.html |title=Subversion 1.3 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.3.2
| 2006-05-23
| No longer supported
|-
| {{Version|o|1.4}}
| 2006-09-10&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.4.html |title=Subversion 1.4 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.4.6
| 2007-12-21
| No longer supported
|-
| {{Version|o|1.5}}
| 2008-06-19&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.5.html |title=Subversion 1.5 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.5.9
| 2010-12-06
| No longer supported
|-
| {{Version|o|1.6}}
| 2009-03-20&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.6.html |title=Apache Subversion 1.6 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.6.23
| 2013-05-30
| No longer supported
|-
| {{Version|o|1.7}}
| 2011-10-11&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.7.html |title=Apache Subversion 1.7 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.7.22
| 2015-08-12
| No longer supported
|-
| {{Version|o|1.8}}
| 2013-06-18&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.8.html |title=Apache Subversion 1.8 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.8.19
| 2017-08-10
| No longer supported
|-
| {{Version|co|1.9}}
| 2015-08-05&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.9.html |title=Apache Subversion 1.9 Release Notes |access-date=2015-09-21}}&lt;/ref&gt;
| 1.9.7
| 2017-08-10
| Partially supported
|-
| {{Version|c|1.10}}
| 2018-04-13&lt;ref&gt;{{cite web |url=https://subversion.apache.org/docs/release-notes/1.10.html |title=Apache Subversion 1.10 Release Notes |access-date=2018-04-17}}&lt;/ref&gt;
| 1.10.0
| 2018-04-13
| Fully supported
|-
| colspan="5" | &lt;small&gt;{{Version |l |show=111110}}&lt;/small&gt;
|}
&lt;!-- o=Old-Not-Supported; co=Old-Still-Supported; c=Latest-Stable; cp=Preview; p=Planned-Future --&gt;

Release dates are extracted from Apache Subversion's &lt;code&gt;CHANGES&lt;/code&gt; file, which records all release history.

==Features==
* [[Commit (data management)|Commits]] as true [[Atomicity (database systems)|atomic operations]] (interrupted commit operations in CVS would cause repository inconsistency or corruption).
* Renamed/copied/moved/removed files retain full revision history.
* The system maintains [[Software versioning|versioning]] for directories, renames, and file [[metadata]] (but not for timestamps). Users can move and/or copy entire directory-trees very quickly, while retaining full revision history.
* Versioning of [[symbolic link]]s.
* Native support for binary files, with space-efficient binary-diff storage.
* [[Apache HTTP Server]] as network server, [[WebDAV]]/[[WebDAV#Extensions and derivatives|Delta-V]] for [[Protocol (computing)|protocol]]. There is also an independent server [[process (computing)|process]] called svnserve that uses a custom protocol over [[Internet Protocol Suite|TCP/IP]].
* [[Branching (software)|Branching]] is a cheap operation, independent of file size (though Subversion itself does not distinguish between a branch and a directory)
* Natively [[client–server model|client–server]], [[Abstraction layer|layered]] [[Library (computing)|library]] design.
* Client/server protocol sends [[diff]]s in both directions.
* Costs proportional to change size, not to data size.
* [[Parsing|Parsable]] output, including [[XML]] log output.
* [[open source]] licensed – [[Apache License]] since the 1.7 release; prior versions use a derivative of the Apache Software License 1.1.
* [[Internationalization and localization|Internationalized]] program messages.
* [[File locking]] for unmergeable files ("reserved checkouts").
* Path-based authorization.
* [[Language binding]]s for [[C Sharp (programming language)|C#]], [[PHP]], [[Python (programming language)|Python]], [[Perl]], [[Ruby (programming language)|Ruby]], and [[Java (programming language)|Java]].
* Full [[MIME]] support – users can view or change the MIME type of each file, with the software knowing which MIME types can have their differences from previous versions shown.
* Merge tracking – Merges between branches will be tracked, this allows automatic merging between branches without telling Subversion what does and does not need to be merged.
* Changelists to organize commits into commit groups.

===Repository types===
Subversion offers two types of repository storage.

====Berkeley DB (deprecated&lt;ref&gt;http://subversion.apache.org/docs/release-notes/1.8.html#bdb-deprecated&lt;/ref&gt;)====
The original development of Subversion used the [[Berkeley DB]] package.
Subversion has some limitations with Berkeley DB usage when a program that accesses the database crashes or terminates forcibly. No data loss or corruption occurs, but the repository remains offline while Berkeley DB replays the journal and cleans up any outstanding locks. The safest way to use Subversion with a Berkeley DB repository involves a single server-process running as one user (instead of through a shared filesystem).&lt;ref name="backend"&gt;
{{cite book |author1=Ben Collins-Sussman |author2=Brian W. Fitzpatrick |author3=C. Michael Pilato |title= Version Control with Subversion: For Subversion 1.7 |year= 2011  |chapter= Chapter 5: Strategies for Repository Deployment |url= http://svnbook.red-bean.com/en/1.7/svn.reposadmin.planning.html#svn.reposadmin.basics.backends | publisher = O'Reilly}}
&lt;/ref&gt;

====FSFS====
In 2004, a new storage subsystem was developed and named FSFS.
It works faster than the Berkeley DB backend on directories with a large number of files and takes less disk space,
due to less logging.&lt;ref name="backend"/&gt;

Beginning with Subversion 1.2, FSFS became the default data store for new repositories.

The etymology of "FSFS" is based on Subversion's use of the term "filesystem" for its repository storage system.
FSFS stores its contents directly within the operating system's filesystem, rather than a structured system like Berkeley DB.
Thus, it is a "[Subversion] FileSystem atop the FileSystem".

====FSX====
A new file system, called FSX, is under development to remove some limitations of FSFS. As of Version 1.9, it was not considered production-ready.&lt;ref&gt;https://subversion.apache.org/docs/release-notes/1.9.html#fsx&lt;/ref&gt;

===Repository access===
{{Main article| Comparison of Subversion clients}}

Access to Subversion repositories can take place by:

# Local filesystem or network filesystem,&lt;ref&gt;[[Berkeley DB]] relies on file locking and thus should not be used on (network) filesystems which do not implement them&lt;/ref&gt; accessed by client directly. This mode uses the &lt;tt&gt;file:///path&lt;/tt&gt; access scheme.
# [[WebDAV]]/Delta-V (over http or https) using the &lt;tt&gt;mod_dav_svn&lt;/tt&gt; module for [[Apache HTTP Server|Apache 2]]. This mode uses the &lt;tt&gt;&lt;nowiki&gt;http://host/path&lt;/nowiki&gt;&lt;/tt&gt; access scheme or &lt;tt&gt;&lt;nowiki&gt;https://host/path&lt;/nowiki&gt;&lt;/tt&gt; for secure connections using ssl.
# Custom "svn" protocol (default [[List of TCP and UDP port numbers|port]] 3690), using plain text or over [[TCP/IP]]. This mode uses either the &lt;tt&gt;&lt;nowiki&gt;svn://host/path&lt;/nowiki&gt;&lt;/tt&gt; access scheme for unencrypted transport or &lt;tt&gt;&lt;nowiki&gt;svn+ssh://host/path&lt;/nowiki&gt;&lt;/tt&gt; scheme for tunneling over ssh.

All three means can access both FSFS and Berkeley DB repositories.

Any 1.x version of a client can work with any 1.x server. Newer clients and servers have additional features and performance capabilities, but have fallback support for older clients/servers.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.5.html SVN 1.5 release notes]&lt;/ref&gt;

==Layers==
Internally, a Subversion system comprises several libraries arranged as layers. Each performs a specific task and allows developers to create their own tools at the desired level of complexity and specificity.

; Fs : The lowest level; it implements the versioned filesystem which stores the user data.
; Repos : Concerned with the repository built up around the filesystem. It has many helper functions and handles the various "hooks" that a repository may have, e.g., scripts that run when an action is performed. Together, Fs and Repos constitute the "filesystem interface".
; mod_dav_svn : Provides [[WebDAV]]/Delta-V access through Apache 2.
; Ra : Handles "repository access", both local and remote. From this point on, repositories are referred to using URLs, e.g.
;* &lt;tt&gt;file:///path/&lt;/tt&gt; for local access,
;* &lt;tt&gt;&lt;nowiki&gt;http://host/path/&lt;/nowiki&gt;&lt;/tt&gt; or &lt;tt&gt;&lt;nowiki&gt;https://host/path/&lt;/nowiki&gt;&lt;/tt&gt; for WebDAV access, or
;* &lt;tt&gt;&lt;nowiki&gt;svn://host/path/&lt;/nowiki&gt;&lt;/tt&gt; or &lt;tt&gt;&lt;nowiki&gt;svn+ssh://host/path/&lt;/nowiki&gt;&lt;/tt&gt; for the SVN protocol.
; Client, Wc : The highest level. It abstracts repository access and provides common client tasks, such as authenticating users or comparing versions. Subversion clients use the Wc library to manage the local working copy.

==Filesystem==
[[File:Svn 3D-tree.svg|right|thumb|250px]]

One can view the Subversion filesystem as "two-dimensional".&lt;ref&gt;[http://svnbook.red-bean.com/nightly/en/svn.branchmerge.basicmerging.html#svn.branchmerge.basicmerging.resurrect Basic Merging&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Two coordinates are used to unambiguously address filesystem items:
* '''Path''' (regular [[Path (computing)|path]] of [[Unix-like]] OS filesystem)
* '''Revision'''
Each revision in a Subversion filesystem has its own ''[[root directory|root]]'', which is used to access contents at that revision. Files are stored as links to the most recent change; thus a Subversion repository is quite compact. The system consumes storage space proportional to the number of changes made, not to the number of revisions.

The Subversion filesystem uses transactions to keep changes [[Atomicity (database systems)|atomic]]. A transaction operates on a specified revision of the filesystem, not necessarily the latest. The transaction has its own ''root'', on which changes are made. It is then either committed and becomes the latest revision, or is aborted. The transaction is actually a long-lived filesystem object; a client does not need to commit or abort a transaction itself, rather it can also begin a transaction, exit, and then can re-open the transaction and continue using it. Potentially, multiple clients can access the same transaction and work together on an atomic change, though no existing clients expose this capability.

==Properties==
One important feature of the Subversion filesystem is properties: simple ''name''=''value'' pairs of text. Properties occur on filesystem entries (i.e., files and directories). These are versioned just like other changes to the filesystem. Users can add any property they wish, and the Subversion client uses a set of properties, which it prefixes with 'svn:'.

; &lt;tt&gt;svn:executable&lt;/tt&gt; : Makes files on [[Unix]]-hosted working copies executable.
; &lt;tt&gt;svn:mime-type&lt;/tt&gt; : Stores the [[Internet media type]] ("MIME type") of a file. Affects the handling of diffs and merging.
; &lt;tt&gt;svn:ignore&lt;/tt&gt; : A list of filename patterns to ignore in a directory. Similar to [[Concurrent Versions System|CVS]]'s &lt;tt&gt;.cvsignore&lt;/tt&gt; file.
; &lt;tt&gt;svn:keywords&lt;/tt&gt; : A list of ''keywords'' to substitute into a file when changes are made. The file itself must also reference the keywords as &lt;tt&gt;$keyword$&lt;/tt&gt; or &lt;tt&gt;$keyword:...$&lt;/tt&gt;. This is used to maintain certain information (e.g., author, date of last change, revision number) in a file without human intervention.&lt;br /&gt;The keyword substitution mechanism originates from [[Revision Control System|RCS]] and from CVS.&lt;ref&gt;http://man.openbsd.org/OpenBSD-current/man1/rcs.1&lt;/ref&gt;
; &lt;tt&gt;svn:eol-style&lt;/tt&gt; : Makes the client convert [[Newline|end-of-line]] characters in text files. Used when the working copy is needed with a specific EOL style. "native" is commonly used, so that EOLs match the user's OS EOL style. Repositories may require this property on all files to prevent inconsistent line endings, which can cause a problem in itself.
; &lt;tt&gt;svn:externals&lt;/tt&gt; : Allows parts of other repositories to be automatically checked-out into a sub-directory.
; &lt;tt&gt;svn:needs-lock&lt;/tt&gt; : Specifies that a file is to be checked out with file permissions set to read-only. This is designed for use with the locking mechanism. The read-only permission reminds one to obtain a lock before modifying the file: obtaining a lock makes the file writable, and releasing the lock makes it read-only again. Locks are only enforced during a commit operation. Locks can be used without setting this property. However, that is not recommended, because it introduces the risk of someone modifying a locked file; they will only discover it has been locked when their commit fails.
; &lt;tt&gt;svn:special&lt;/tt&gt; : This property is not meant to be set or modified directly by users. {{As of | 2010}} it is only used for having [[symbolic link]]s in the repository. When a symbolic link is added to the repository, a file containing the link target is created with this property set. When a Unix-like system checks out this file, the client converts it to a symbolic link.
; &lt;tt&gt;svn:mergeinfo&lt;/tt&gt; : Used to track merge data (revision numbers) in Subversion 1.5 (or later). This property is automatically maintained by the &lt;tt&gt;merge&lt;/tt&gt; command, and it is not recommended to change its value manually.&lt;ref&gt;[http://svnbook.red-bean.com/en/1.5/svn.ref.properties.html Subversion Properties&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

Subversion also uses properties on revisions themselves. Like the above properties on filesystem entries the names are completely arbitrary, with the Subversion client using certain properties prefixed with 'svn:'. However, these properties are not versioned and can be changed later.

; &lt;tt&gt;svn:date&lt;/tt&gt; : the date and time stamp of a revision
; &lt;tt&gt;svn:author&lt;/tt&gt; : the name of the user that submitted the change(s)
; &lt;tt&gt;svn:log&lt;/tt&gt; : the user-supplied description of the change(s);

==Branching and tagging==
Subversion uses the inter-file branching model from [[Perforce]]&lt;ref&gt;[http://www.perforce.com/perforce/branch.html Inter-File Branching: A Practical Method for Representing Variants] {{webarchive|url=https://web.archive.org/web/20070714160428/http://www.perforce.com/perforce/branch.html |date=2007-07-14 }}&lt;/ref&gt; to implement [[Branching (software)|branches]] and [[Revision tag|tagging]]. A branch is a separate line of development.&lt;ref&gt;[http://tortoisesvn.net/docs/release/TortoiseSVN_en/tsvn-dug-branchtag.html Branching / Tagging — TortoiseSVN]&lt;/ref&gt; Tagging refers to labeling the repository at a certain point in time so that it can be easily found in the future. In Subversion, the only difference between branches and tags is how they are used.

A new branch or tag is set up by using the "&lt;tt&gt;svn copy&lt;/tt&gt;" command, which should be used in place of the native operating system mechanism. The copied directory is linked to the original in the repository to preserve its history, and the copy takes very little extra space in the repository.

All the versions in each branch maintain the history of the file up to the point of the copy, plus any changes made since. One can "merge" changes back into the [[Trunk (software)|trunk]] or between branches.

[[File:Subversion project visualization.svg|650px|thumb|center|Visualization of a simple Subversion project]]

==Limitations and problems==
A known problem in Subversion affects the implementation of the file and directory rename operation. {{As of|2014}}, Subversion implements the renaming of files and directories as a "copy" to the new name followed by a "delete" of the old name. Only the names change, all data relating to the edit history remains the same, and Subversion will still use the old name in older revisions of the "tree". However, Subversion may become confused when a move conflicts with edits made elsewhere,&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=898 Implement true renames]&lt;/ref&gt; both for regular commits and when merging branches.&lt;ref&gt;[http://svnbook.red-bean.com/en/1.7/svn.branchmerge.advanced.html#svn.branchmerge.advanced.moves Advanced Merging]&lt;/ref&gt; The Subversion 1.5 release addressed some of these scenarios while others remained problematic.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.5.html#copy-move-improvements Copy/move-related improvements in Subversion 1.5]&lt;/ref&gt; The Subversion 1.8 release addressed some of these problems by making moves a first-class operation on the client, but it is still treated as copy+delete in the repository.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.8.html#moves Working copy records moves as first-class operation in Subversion 1.8]&lt;/ref&gt;

{{As of | 2013}}, Subversion lacks some repository-administration and management features. For instance, someone may wish to edit the repository to permanently remove all historical records of certain data. Subversion does not have built-in support to achieve this simply.&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=516 svn obliterate]&lt;/ref&gt;

Subversion stores additional copies of data on the local machine, which can become an issue with very large projects or files, or if developers work on multiple branches simultaneously. In versions prior to 1.7 these &lt;tt&gt;.svn&lt;/tt&gt; directories on the client side could become corrupted by ill-advised user activity like global search/replace operations.&lt;ref&gt;[https://stackoverflow.com/a/579442]&lt;/ref&gt; Starting with version 1.7 Subversion uses a single centralized &lt;tt&gt;.svn&lt;/tt&gt; folder per working area.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.7.html#wc-ng Working Copy Metadata Storage Improvements (client)]&lt;/ref&gt;

Subversion does not store the modification times of files. As such, a file checked out of a Subversion repository will have the 'current' date (instead of the modification time in the repository), and a file checked into the repository will have the date of the check-in (instead of the modification time of the file being checked in). This might not always be what is wanted.&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=1256 Issue 1256] at Tigris.org
&lt;/ref&gt;
To mitigate this, third-party tools exist that allow for preserving modification time and other filesystem meta-data.&lt;ref&gt;[http://sourceforge.net/projects/freezeattrib/files/ FreezeAttrib (saves/restores file attributes using properties)]&lt;/ref&gt;&lt;ref&gt;[http://fsvs.tigris.org/ FSVS (Fast System VerSioning)]&lt;/ref&gt;
However, giving checked out files a current date is important as well — this is how tools like [[make (software)|make(1)]] will take notice of a changed file for rebuilding it.

Subversion uses a centralized [[revision control]] model. Ben Collins-Sussman, one of the designers of Subversion, believes a centralised model would help prevent "insecure programmers" from hiding their work from other team members.&lt;ref&gt;[http://blog.red-bean.com/sussman/?p=96 Programmer Insecurity @ iBanjo&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;  Some users of version control systems see the centralised model as detrimental; famously, [[Linus Torvalds]] attacked Subversion's model and its developers.&lt;ref&gt;[https://www.youtube.com/watch?v=4XpnKHJAok8 Google Tech Talk video] and its [https://web.archive.org/web/20110725034129/https://git.wiki.kernel.org/index.php/LinusTalk200705Transcript transcript]&lt;/ref&gt;

Subversion often does not deal well with the [[Unicode equivalence#Normalization|filename normalization]] performed by the [[HFS+]] filesystem. This can cause problems when files with accented characters in their names are added to the repository on a non-HFS+ filesystem and the repository is then used with HFS+.&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=2464 subversion: Issue 2464&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

===Subversion tags and branches===
Revision numbers are difficult to remember in any version-control system. For this reason, most systems offer symbolic ''tags'' as user-friendly references to them. Subversion does not have such a feature and what its documentation recommends to use instead is very different in nature. Instead of implementing tags as ''references'' to points in history, Subversion recommends making snapshot ''copies'' into a well-known subdirectory ("&lt;code&gt;tags/&lt;/code&gt;") in the space of the repository tree. Only a few predefined ''references'' are available: HEAD, BASE, PREV and COMMITTED.

This history-to-space projection has multiple issues:

1. When a snapshot is taken, the system does not assign any special meaning to the name of the tag/snapshot. This is the difference between a ''copy'' and a ''reference''. The revision is recorded and the snapshot can be accessed by URL. This makes some operations less convenient and others impossible. For instance, a naïve &lt;code&gt;svn diff -r tag1:tag2 myfile&lt;/code&gt; does not work; it is slightly more complicated than that to achieve, requiring the user to know and input URL/paths to the snapshots instead of just the names: &lt;code&gt;svn diff &lt;URL-TO-TAG1&gt;/myfile &lt;URL-TO-TAG2&gt;/myfile&lt;/code&gt;. Other operations like for instance &lt;code&gt;svn log -r tag1:tag2 myfile&lt;/code&gt; are just impossible.

2. When two (ideally independent) object types live in the repository tree, a "fight to the top" can ensue. In other words, it is often difficult to decide at which level to create the "&lt;code&gt;tags/&lt;/code&gt;" subdirectory:

{|
|
 '''trunk'''/
      /component'''foo'''/
      /component'''bar'''/
 '''tags'''/
     /1.1/
         /component'''foo'''/
         /component'''bar'''/
| &amp;nbsp;or&amp;nbsp;
|
 component'''foo'''/
             /'''trunk'''/
             /'''tags'''/
                  /1.1/
 component'''bar'''/
             /'''trunk'''/
             /'''tags'''/
                  /1.1/
|}

3. Tags, by their conventional definition are both read-only and light-weight, on the repository and client.  Subversion copies are not read-only, and while they are light-weight on the repository, they are incredibly heavy-weight on the client.

To address such issues, posters on the Subversion mailing lists have suggested a new feature called "labels" or "aliases".&lt;ref&gt;[https://www.google.com/search?q=tag+%28labels+OR+aliases%29+site%3Asvn.haxx.se Subversion mailing lists]&lt;/ref&gt;{{Full citation needed}}
SVN labels would more closely resemble the "tags" of other systems such as [[Concurrent Versions System|CVS]] or [[Git (software)|Git]]. The fact that Subversion has global revision numbers opens the way to a very simple label-&gt;revision implementation. Yet as of 2013, no progress has been made and symbolic tags are not in the list of the most wanted features.&lt;ref&gt;[http://subversion.apache.org/roadmap.html Subversion Roadmap]&lt;/ref&gt;

==Development and implementation==
[[CollabNet]] has continued its involvement with Subversion, but the project runs as an independent open source community. In November 2009, the project was accepted into the [[Apache Incubator]], aiming to become part of the [[Apache Software Foundation]]'s efforts.&lt;ref&gt;http://www.open.collab.net/news/press/2009/svn-asf.html Collabnet Press Release&lt;/ref&gt; Since March 2010, the project is formally known as Apache Subversion, being a part of the Apache Top-Level Projects.&lt;ref name=CollabNetPR2010&gt;{{cite web|title=CollabNet Supports Subversion’s Graduation to Apache Top-Level Project|url=http://www.open.collab.net/news/press/2010/apache.html|publisher=CollabNet|archiveurl=https://web.archive.org/web/20100311192650/http://www.open.collab.net/news/press/2010/apache.html|archivedate=2010-03-11|date=1 March 2010}}&lt;/ref&gt;

In October 2009, [[WANdisco]] announced the hiring of core Subversion committers as the company moved to become a major corporate sponsor of the project. This included Hyrum Wright, president of the Subversion Corporation and release manager for the Subversion project since early 2008, who joined the company to lead its open source team.&lt;ref&gt;{{Cite news |title= WANdisco Names Hyrum Wright to Lead Subversion Open Source Efforts |date= January 7, 2010 |work= News release |author= WANdisco |publisher= Open Source magazine |url= http://opensource.sys-con.com/node/1239202 |accessdate= October 29, 2011 |archiveurl=https://web.archive.org/web/20120118161238/http://opensource.sys-con.com/node/1239202 |archivedate=2012-01-18 |deadurl=no}}&lt;/ref&gt;

The Subversion open-source community does not provide binaries, but potential users can download binaries from volunteers.&lt;ref&gt;{{Cite web |title= Apache Subversion Binary Packages |work= Official project website  |url= http://subversion.apache.org/packages.html  |accessdate= October 29, 2011 }}&lt;/ref&gt; While the Subversion project does not include an official [[graphical user interface]] (GUI) for use with Subversion, third parties have developed a number of different GUIs, along with a wide variety of additional ancillary software.

Work announced in 2009 included SubversionJ (a [[Java API]]) and implementation of the Obliterate command, similar to that provided by [[Perforce]]. Both of these enhancements were sponsored by WANdisco.&lt;ref&gt;{{Cite news|title=WANdisco Presents New Initiatives for the Subversion Open Source Project |author=WANdisco |publisher=CM Crossroads |work=News release |date=October 28, 2009 |url=http://www.cmcrossroads.com/index.php?Itemid=100152&amp;catid=101:news-and-announcements&amp;id=13065:wandisco-presents-new-initiatives-for-the-subversion-open-source-project-&amp;option=com_content&amp;view=article |accessdate=October 29, 2011 |archiveurl=https://web.archive.org/web/20111118040455/http://www.cmcrossroads.com/index.php?Itemid=100152&amp;catid=101%3Anews-and-announcements&amp;id=13065%3Awandisco-presents-new-initiatives-for-the-subversion-open-source-project-&amp;option=com_content&amp;view=article |archivedate=2011-11-18 |deadurl=yes }}&lt;/ref&gt;

The Subversion committers normally have at least one or two new features under active development at any one time. The 1.7 release of Subversion in October 2011 included a streamlined HTTP transport to improve performance and a rewritten working-copy library.&lt;ref&gt;{{Cite web |title= Apache Subversion Roadmap |work= Official project website |url= http://subversion.apache.org/roadmap.html  |accessdate= October 29, 2011 }}&lt;/ref&gt;

==See also==
{{Portal|Free software}}

* [[List of revision control software]]
* [[Comparison of revision control software]]
* [[Comparison of Subversion clients]]
* [[List of software that uses Subversion]]
* [[TortoiseSVN]]

==References==
===Citations===
{{Reflist |colwidth = 30em}}

===Sources===
{{refbegin}}
* C. Michael Pilato, Ben Collins-Sussman, Brian W. Fitzpatrick; &lt;cite&gt;Version Control with Subversion&lt;/cite&gt;; O'Reilly; {{ISBN|0-596-00448-6}} (1st edition, paperback, 2004, [http://svnbook.red-bean.com/ full book online], [https://web.archive.org/web/20080107072118/http://www.mentalpointer.com/Subversion/svn-book.html mirror])
* Garrett Rooney; &lt;cite&gt;Practical Subversion&lt;/cite&gt;; Apress; {{ISBN|1-59059-290-5}} (1st edition, paperback, 2005)
* Mike Mason; &lt;cite&gt;Pragmatic Version Control Using Subversion&lt;/cite&gt;; Pragmatic Bookshelf; {{ISBN|0-9745140-6-3}} (1st edition, paperback, 2005)
* William Nagel; &lt;cite&gt;Subversion Version Control: Using the Subversion Version Control System in Development Projects&lt;/cite&gt;; Prentice Hall; {{ISBN|0-13-185518-2}} (1st edition, paperback, 2005)
{{refend}}

==Further reading==
* [http://www.red-bean.com/sussman/svn-anti-fud.html Dispelling Subversion FUD] by Ben Collins-Sussman (Subversion developer), link broken as of 2013-03-07 (Internet Archive.org [[Wayback machine|Wayback Machine]] [https://web.archive.org/web/20110718233416/http://www.red-bean.com/sussman/svn-anti-fud.html 2011-07-18 captured version], "last updated" 2004-12-21)

==External links==
* {{Official website}}
** [http://subversion.tigris.org/ Previous official site] &lt;small&gt;Not all content has yet been migrated to the new official site.&lt;/small&gt;
* [http://svnbook.red-bean.com/ Version Control with Subversion], an [[O'Reilly Media|O'Reilly]] book available for free online
* {{dmoz|Computers/Software/Configuration_Management/Tools/Subversion/|Subversion}}

{{Version control software}}
{{Apache}}

{{DEFAULTSORT:Apache Subversion}}
[[Category:Apache Software Foundation|Subversion]]
[[Category:Collaborative software]]
[[Category:Concurrent Versions System]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in C]]
[[Category:Free version control software]]
[[Category:Software using the Apache license]]
[[Category:Subversion| ]]
[[Category:Unix archivers and compression-related utilities]]</text>
      <sha1>h4uxi4q2jliozlt2olciapi0noup6t9</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Aries</title>
    <ns>0</ns>
    <id>27761008</id>
    <revision>
      <id>813705332</id>
      <parentid>789524027</parentid>
      <timestamp>2017-12-04T20:44:39Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v477)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2365">{{multiple issues|
{{Notability|Products|date=August 2011}}
{{primary sources|date=August 2011}}
}}

{{Infobox software
| name                   = Apache Aries
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Incubation
| latest release version = 0.3 
| latest release date    = {{release date|2011|01|22}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[OSGi]] Blueprint Container implementations
| license                = [[Apache License]]
| website                = {{url|http://aries.apache.org/}}
}}
'''Apache Aries''', a Blueprint Container implementations and extensions of application-focused specifications defined by [[OSGi]] Enterprise Expert Group. The project aims to deliver a set of pluggable Java components enabling an enterprise OSGi application programming model.&lt;ref name=Bharti_2010-05-14_dzone /&gt; The Aries project Content includes the following:
* [[WAR (Sun file format)|WAR]] to Web Application Bundle Converter
* Blueprint Container
* [[Java Persistence API]] integration
* [[Java Transaction API]] integration
* [[Java Management Extensions]]
* [[Java Naming and Directory Interface]] integration
* Application Assembly and Deployment
* [[Apache Maven]] Plugin
* META-INF/services handler
* Samples, tutorials, documentation, and integrator’s guide

==See also==
*[[Virgo (software)]]

== References ==
{{reflist|refs=

&lt;ref name=Bharti_2010-05-14_dzone&gt;[https://www.webcitation.org/6g9bXuKon?url=https://dzone.com/articles/apache-aries Apache Aries: Marrying OSGi with Java EE] in [[dzone]] by Nitin Bharti, 14 May 2010&lt;/ref&gt;

}}

==External links==
*{{official website|http://aries.apache.org/}}
*{{cite web | last = Gawor | first = Jarek | title = Building OSGi applications with the Blueprint Container specification | work = IBM | publisher = IBM | date = 2009-10-27 | url = http://www.ibm.com/developerworks/opensource/library/os-osgiblueprint/index.html }}

{{Apache}}

[[Category:Free software programmed in Java (programming language)]]
[[Category:Apache Software Foundation]]


{{Free-software-stub}}</text>
      <sha1>h9yj3u7dnrdvaaf3qpnbg6wfqc28279</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Cassandra</title>
    <ns>0</ns>
    <id>18462573</id>
    <revision>
      <id>846346516</id>
      <parentid>845387007</parentid>
      <timestamp>2018-06-18T04:44:35Z</timestamp>
      <contributor>
        <username>Bschoeni</username>
        <id>888791</id>
      </contributor>
      <comment>Reverted helenos which is not directly relevant and out of date</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="31267">{{Infobox software
| name                   = Apache Cassandra
| logo                   = [[File:Cassandra logo.svg|frameless|Cassandra logo]]
| author                 = Avinash Lakshman, Prashant Malik
| developer              = [[Apache Software Foundation]]
| released               = 07/2008
| status                 = Active
| latest release version = 3.11.2
| latest release date    = {{release date|2018|02|19}}
| programming language   = [[Java (programming language)|Java]]
| operating system       = [[Cross-platform]]
| language               = English
| genre                  = [[NoSQL]] [[Database]], [[data store]] 
| license                = [[Apache License 2.0]]
| website                = {{URL|//cassandra.apache.org/}}
}}
'''Apache Cassandra'''  is a [[free and open-source software|free and open-source]] [[distributed database|distributed]] [[wide column store]] [[NoSQL]] [[database]] management system designed to handle large amounts of data across many [[commodity computing|commodity servers]], providing high availability with no [[single point of failure]]. Cassandra offers robust support for [[computer cluster|clusters]] spanning multiple datacenters,&lt;ref&gt;{{cite web |accessdate=2013-07-25 |first=Joaquin |last=Casares |date=2012-11-05 |publisher=DataStax |title=Multi-datacenter Replication in Cassandra |quote=Cassandra’s innate datacenter concepts are important as they allow multiple workloads to be run across multiple datacenters… |url=http://www.datastax.com/dev/blog/multi-datacenter-replication}}&lt;/ref&gt; with asynchronous masterless replication allowing low latency operations for all clients.

== History ==
[[Avinash Lakshman]], one of the authors of [[Dynamo (storage system)|Amazon's Dynamo]], and Prashant Malik initially developed  Cassandra at [[Facebook]] to power the Facebook inbox search feature.  Facebook released Cassandra as an open-source [[project]] on [[Google code]] in July 2008.&lt;ref name=JH2008&gt;{{cite web |accessdate= 2009-06-04 |date= July 12, 2008 |url= http://perspectives.mvdirona.com/2008/07/12/FacebookReleasesCassandraAsOpenSource.aspx |title= Facebook Releases Cassandra as Open Source |first= James |last= Hamilton}}&lt;/ref&gt;  In March 2009 it became an [[Apache Incubator]] project.&lt;ref&gt;{{cite web|url=http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00004.html |title=Is this the new hotness now? |publisher=Mail-archive.com |date=2009-03-02 |accessdate=2010-03-29 |archiveurl=https://web.archive.org/web/20100425071855/http://www.mail-archive.com/cassandra-dev%40incubator.apache.org/msg00004.html |archivedate=25 April 2010 |deadurl=no }}&lt;/ref&gt; On February 17, 2010 it graduated to a top-level project.&lt;ref name=GRAD&gt;{{cite web|url=http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg01518.html |title=Cassandra is an Apache top level project |publisher=Mail-archive.com |date=2010-02-18 |accessdate=2010-03-29 |archiveurl=https://web.archive.org/web/20100328090322/http://www.mail-archive.com/cassandra-dev%40incubator.apache.org/msg01518.html |archivedate=28 March 2010 |deadurl=no }}&lt;/ref&gt;

Facebook developers named their database after the Trojan mythological prophet [[Cassandra]] - with classical allusions to a curse on an [[oracle]].&lt;ref&gt;{{cite web|url= http://kellabyte.com/2013/01/04/the-meaning-behind-the-name-of-apache-cassandra/|title= The meaning behind the name of Apache Cassandra|publisher= |accessdate= 2016-07-19 | quote = Apache Cassandra is named after the Greek mythological prophet Cassandra. [...] Because of her beauty Apollo granted her the ability of prophecy. [...] When Cassandra of Troy refused Apollo, he put a curse on her so that all of her and her descendants' predictions would not be believed. [...] Cassandra is the cursed Oracle[.]}}&lt;/ref&gt;

=== Releases ===
Releases after graduation include
* 0.6, released Apr 12 2010, added support for integrated caching, and [[Apache Hadoop]] [[MapReduce]]&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces3|title=The Apache Software Foundation Announces Apache Cassandra Release 0.6  : The Apache Software Foundation Blog|publisher=|accessdate=5 January 2016}}&lt;/ref&gt;
* 0.7, released Jan 08 2011, added secondary indexes and online schema changes&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces9|title=The Apache Software Foundation Announces Apache Cassandra 0.7 : The Apache Software Foundation Blog|publisher=|accessdate=5 January 2016}}&lt;/ref&gt;
* 0.8, released Jun 2 2011, added the Cassandra Query Language (CQL), self-tuning memtables, and support for zero-downtime upgrades&lt;ref&gt;{{cite web|url=http://grokbase.com/t/cassandra/user/1162fkpwx2/release-0-8-0|title=[Cassandra-user]  [RELEASE] 0.8.0|author=Eric Evans|publisher=|accessdate=5 January 2016}}&lt;/ref&gt;
* 1.0, released Oct 17 2011, added integrated compression, leveled compaction, and improved read-performance&lt;ref&gt;{{cite web|url=http://www.infoq.com/news/2011/10/Cassandra-1|title=Cassandra 1.0.0. Is Ready for the Enterprise|work=InfoQ|accessdate=5 January 2016}}&lt;/ref&gt;
* 1.1, released Apr 23 2012, added self-tuning caches, row-level isolation, and support for mixed ssd/spinning disk deployments&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces26|title=The Apache Software Foundation Announces Apache Cassandra™ v1.1 : The Apache Software Foundation Blog|publisher=|accessdate=5 January 2016}}&lt;/ref&gt;
* 1.2, released Jan 2 2013, added clustering across virtual nodes, inter-node communication, atomic batches, and request tracing&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces38|title=The Apache Software Foundation Announces Apache Cassandra™ v1.2 : The Apache Software Foundation Blog|work=apache.org|accessdate=11 December 2014}}&lt;/ref&gt;
* 2.0, released {{Release date and age|2013|09|04}}, added lightweight transactions (based on the [[Paxos (computer science)|Paxos]] consensus protocol), triggers, improved compactions
* 2.1 released Sep 10 2014 &lt;ref&gt;{{cite web|url=http://www.mail-archive.com/dev@cassandra.apache.org/msg07512.html|title=[VOTE SUCCESS] Release Apache Cassandra 2.1.0|author=Sylvain Lebresne|date=10 September 2014|work=mail-archive.com|accessdate=11 December 2014}}&lt;/ref&gt;
* 2.2 released July 20, 2015
* 3.0 released November 11, 2015
* 3.1 through 3.10 releases were monthly releases using a [[Tick-Tock model | tick-tock]]-like release model, with even-numbered releases providing both new features and bug fixes while odd-numbered releases will include bug fixes only.&lt;ref&gt;{{cite web|url= http://www.planetcassandra.org/blog/cassandra-2-2-3-0-and-beyond/|title= Cassandra 2.2, 3.0, and beyond|date= 16 June 2015|accessdate= 22 April 2016}}&lt;/ref&gt;
* 3.11 released June 23, 2017 as a stable 3.11 release series and bug fix from the last tick-tock feature release.

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date
! Status&lt;ref&gt;
{{cite web
|url= http://cassandra.apache.org/download/
|title= Cassandra Server Releases
|work= cassandra.apache.org
|accessdate= 15 December 2015
}}
&lt;/ref&gt;
|-
| {{Version|o|0.6}}
| 2010-04-12
| 0.6.13
| 2011-04-18
| No longer supported
|-
| {{Version|o|0.7}} 
| 2011-01-10
| 0.7.10
| 2011-10-31
| No longer supported
|-
| {{Version|o|0.8}}
| 2011-06-03
| 0.8.10
| 2012-02-13
| No longer supported
|-
| {{Version|o|1.0}}
| 2011-10-18
| 1.0.12
| 2012-10-04
| No longer supported
|-
| {{Version|o|1.1}}
| 2012-04-24
| 1.1.12
| 2013-05-27
| No longer supported
|-
| {{Version|o|1.2}}
| 2013-01-02
| 1.2.19
| 2014-09-18
| No longer supported
|-
| {{Version|o|2.0}}
| {{Release date and age|2013|09|03}}
| 2.0.17
| 2015-09-21
| No longer supported
|-
| {{Version|co|2.1}}
| 2014-09-16
| 2.1.20
| 2018-02-16
| Still supported, critical fixes only
|-
| {{Version|co|2.2}}
| 2015-07-20
| 2.2.12
| 2018-02-16
| Still supported
|-
| {{Version|co|3.0}}
| 2015-11-09
| 3.0.16
| 2018-02-19
| Still supported
|-
| {{Version|c|3.11}}
| 2017-06-23
| 3.11.2
| 2018-02-10
| Latest release
|-

| colspan="5" | &lt;small&gt;{{Version |l |show=111110}}&lt;/small&gt;
|}
&lt;!-- o=Old-Not-Supported; co=Old-Still-Supported; c=Latest-Stable; cp=Preview; p=Planned-Future --&gt;

== Main features ==

; Distributed
: Every node in the cluster has the same role. There is no single point of failure.  Data is distributed across the cluster (so each node contains different data), but there is no master as every node can service any request.

; Supports replication and multi data center replication
: Replication strategies are configurable.&lt;ref&gt;{{cite web|url=http://www.datastax.com/dev/blog/deploying-cassandra-across-multiple-data-centers|title=Deploying Cassandra across Multiple Data Centers|work=DataStax|accessdate=11 December 2014}}&lt;/ref&gt; Cassandra is designed as a distributed system, for deployment of large numbers of nodes across multiple data centers. Key features of Cassandra’s distributed architecture are specifically tailored for multiple-data center deployment, for redundancy,  for failover and disaster recovery.

; Scalability
: Designed to have read and write throughput both increase linearly as new machines are added, with the aim of no downtime or interruption to applications.

; Fault-tolerant
: Data is automatically replicated to multiple nodes for [[fault-tolerance]]. [[Replication (computer science)|Replication]] across multiple data centers is supported. Failed nodes can be replaced with no downtime.

; Tunable consistency
: Cassandra is typically classified as an [[CAP theorem|AP system]], meaning that availability and partition tolerance are generally considered to be more important than consistency in Cassandra&lt;ref&gt;https://teddyma.gitbooks.io/learncassandra/content/about/the_cap_theorem.html&lt;/ref&gt;, Writes and reads offer a tunable level of [[Consistency (database systems)|consistency]], all the way from "writes never fail" to "block for all replicas to be readable", with the [[Quorum (distributed computing)|quorum level]] in the middle.&lt;ref name="tunable_consistency" /&gt;

; MapReduce support
: Cassandra has [[Hadoop]] integration, with [[MapReduce]] support. There is support also  for [[Pig (programming tool)|Apache Pig]] and [[Apache Hive]].&lt;ref name="hadoopsupport"&gt;[http://wiki.apache.org/cassandra/HadoopSupport "Hadoop Support"] article on Cassandra's wiki&lt;/ref&gt;

; Query language
: Cassandra introduced the [[Cassandra Query Language]] (CQL). CQL is a simple interface for accessing Cassandra, as an alternative to the traditional [[SQL|Structured Query Language]] (SQL). 

=== Cassandra Query Language ===
Cassandra introduced the [[Cassandra Query Language]] (CQL). CQL is a simple interface for accessing Cassandra, as an alternative to the traditional [[SQL|Structured Query Language]] (SQL). CQL adds an abstraction layer that hides implementation details of this structure and provides native syntaxes for collections and other common encodings. Language drivers are available for Java (JDBC), Python (DBAPI2), Node.JS (Helenus), Go (gocql) and C++.&lt;ref&gt;{{cite web|url=https://github.com/datastax/cpp-driver|title=DataStax C/C++ Driver for Apache Cassandra|work=DataStax|accessdate=15 December 2014}}&lt;/ref&gt;

Below an example of keyspace creation, including a column family in CQL  3.0:&lt;ref&gt;{{cite web|url=https://cassandra.apache.org/doc/cql3/CQL.html|title=CQL|publisher=|accessdate=5 January 2016}}&lt;/ref&gt;
&lt;source lang=sql&gt;
CREATE KEYSPACE MyKeySpace
  WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

USE MyKeySpace;

CREATE COLUMNFAMILY MyColumns (id text, Last text, First text, PRIMARY KEY(id));

INSERT INTO MyColumns (id, Last, First) VALUES ('1', 'Doe', 'John');

SELECT * FROM MyColumns;
&lt;/source&gt;
Which gives:
&lt;source lang=dos&gt;
 id | first | last
----+-------+------
  1 |  John |  Doe

(1 rows)
&lt;/source&gt;

=== Known issues ===
Cassandra is not row level consistent,&lt;ref&gt;{{cite web|url=http://datanerds.io/post/cassandra-no-row-consistency/|title=WAT - Cassandra: Row level consistency #$@&amp;%*! - datanerds.io|work=datanerds.io|accessdate=28 November 2016}}&lt;/ref&gt; meaning that inserts and updates into the table that affect the same row that are processed at approximately the same time may affect the non-key columns in inconsistent ways. One update may affect one column while another affects the other, resulting in sets of values within the row that were never specified or intended.

=== Data model ===
Cassandra is [[wide column store]], and, as such, essentially a hybrid between a key-value and a tabular database management system. Its data model is a partitioned row store with tunable consistency.&lt;ref name="tunable_consistency"&gt;{{cite web  |accessdate=2013-07-25 |author=DataStax |authorlink=DataStax |date=2013-01-15 |title=About data consistency |url=http://www.datastax.com/docs/1.2/dml/data_consistency}}&lt;/ref&gt; Rows are organized into [[Table (database)|tables]]; the first component of a table's primary key is the partition key; within a partition, rows are [[Clustered index|clustered]] by the remaining columns of the key.&lt;ref&gt;{{cite web  |accessdate=2013-07-25 |first=Jonathan |last=Ellis |date=2012-02-15 |title=Schema in Cassandra 1.1 |publisher=DataStax |url=http://www.datastax.com/dev/blog/schema-in-cassandra-1-1}}&lt;/ref&gt; Other columns may be indexed separately from the primary key.&lt;ref&gt;{{cite web  |accessdate=2013-07-25 |first=Jonathan |last=Ellis |date=2010-12-03 |title=What’s new in Cassandra 0.7: Secondary indexes |publisher=DataStax |url=http://www.datastax.com/dev/blog/whats-new-cassandra-07-secondary-indexes}}&lt;/ref&gt;

Tables may be created, dropped, and altered at run-time without blocking updates and queries.&lt;ref&gt;{{cite web  |accessdate=2013-07-25 |first=Jonathan |last=Ellis |date=2012-03-02 |title=The Schema Management Renaissance in Cassandra 1.1 |publisher=DataStax |url=http://www.datastax.com/dev/blog/the-schema-management-renaissance}}&lt;/ref&gt;

Cassandra cannot do [[Join (SQL)|joins]] or [[Correlated subquery|subqueries]].  Rather, Cassandra emphasizes [[denormalization]] through features like collections.&lt;ref&gt;{{cite web  |accessdate=2013-07-25 |first=Sylvain |last=Lebresne |date=2012-08-05 |title=Coming in 1.2: Collections support in CQL3 |publisher=DataStax |url=http://www.datastax.com/dev/blog/cql3_collections}}&lt;/ref&gt;

A [[column family]] (called "table" since CQL 3) resembles a table in an RDBMS. Column families contain rows and columns. Each row is uniquely identified by a row key. Each row has multiple columns, each of which has a name, value, and a timestamp. Unlike a table in an RDBMS, different rows in the same column family do not have to share the same set of columns, and a column may be added to one or multiple rows at any time.&lt;ref&gt;{{cite web|last=DataStax|title=Apache Cassandra 0.7 Documentation - Column Families|url=http://www.datastax.com/docs/0.7/data_model/column_families#column-families|work=Apache Cassandra 0.7 Documentation|accessdate=29 October 2012}}&lt;/ref&gt;

Each key in Cassandra corresponds to a value which is an object. Each key has values as columns, and columns are grouped together into sets called column families. Thus, each key identifies a row of a variable number of elements. These column families could be considered then as tables.  A table in Cassandra is a distributed multi dimensional map indexed by a key. Furthermore, applications can specify the sort order of columns within a Super Column or Simple Column family.

== Management and monitoring==
Cassandra is a Java-based system that can be managed and monitored via [[Java Management Extensions]] (JMX). The JMX-compliant [[nodetool]] utility, for instance, can be used to manage a Cassandra cluster (adding nodes to a ring, draining nodes, decommissioning nodes, and so on).&lt;ref&gt;{{cite web|title=NodeTool|url=https://wiki.apache.org/cassandra/NodeTool|website=Cassandra Wiki|accessdate=5 January 2016}}&lt;/ref&gt; Nodetool also offers a number of commands to return Cassandra metrics pertaining to disk usage, latency, compaction, garbage collection, and more.&lt;ref&gt;{{cite web|title=How to monitor Cassandra performance metrics|url=https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/|publisher=Datadog|accessdate=5 January 2016}}&lt;/ref&gt; 

Since Cassandra 2.0.2 in 2013, measures of several metrics are produced via the Dropwizard metrics framework&lt;ref&gt;{{cite web|title=Metrics|url=https://wiki.apache.org/cassandra/Metrics|website=Cassandra Wiki|accessdate=5 January 2016}}&lt;/ref&gt;, and may be queried via JMX using tools such as [[JConsole]] or passed to external monitoring systems via Dropwizard-compatible reporter plugins&lt;ref&gt;{{cite web|title=Monitoring|url=http://cassandra.apache.org/doc/latest/operating/metrics.html|website=Cassandra Documentation|accessdate=1 February 2018}}&lt;/ref&gt;.

== Notable applications ==
{{Example farm|section|date=October 2016}}

According to [[DB-Engines ranking]], Cassandra is the most popular [[wide column store]],&lt;ref name=DB-Engines&gt;{{cite web|url=http://db-engines.com/en/ranking/wide+column+store |title=DB-Engines Ranking of Wide Column Stores |author=[[DB-Engines]]}}&lt;/ref&gt; and in September 2014 become the 9th most popular database.&lt;ref name=DB-Engines-Ranking&gt;{{cite web|url=http://db-engines.com/en/ranking |title=DB-Engines Ranking |author=DB-Engines}}&lt;/ref&gt;

*[[Apple Inc.|Apple]] uses 100,000 Cassandra nodes, as revealed at Cassandra Summit San Francisco 2015,&lt;ref&gt;{{Twitter|id = /lucamartinetti/status/646789308116938752|name = Luca Martinetti: &lt;nowiki&gt;Apple runs more than 100k [production] Cassandra nodes.&lt;/nowiki&gt;}}&lt;/ref&gt; although it has not elaborated for which products, services or features.
*[[AppScale]] uses Cassandra as a back-end for Google App Engine applications&lt;ref&gt;{{cite web|title=Datastores on Appscale  |url=http://appscale.cs.ucsb.edu/datastores.html#cassandra}}&lt;/ref&gt;
*[[BlackRock]] uses Cassandra in their Aladdin investment management platform&lt;ref&gt;{{cite web|title=Top Cassandra Summit Sessions For Advanced Cassandra Users|url=http://www.datastax.com/2015/09/top-cassandra-sessions-for-advanced-cassandra-users}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Multi-Tenancy in Cassandra at BlackRock|url=https://vimeopro.com/user35188327/cassandra-summit-2015/video/143826965}}&lt;/ref&gt;
*[[CERN]] used Cassandra-based prototype for its [[ATLAS experiment]] to archive the online DAQ system's monitoring information&lt;ref name=CERN-ATLAS&gt;{{cite web|url=https://cdsweb.cern.ch/record/1432912 |title=A Persistent Back-End for the ATLAS Online Information Service (P-BEAST)}}&lt;/ref&gt;
*[[Cisco]]'s [[WebEx]] uses Cassandra to store user feed and activity in near real time.&lt;ref name=CISCO&gt;{{cite web|url=http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg01163.html |title=Re: Cassandra users survey |publisher=Mail-archive.com |date=2009-11-21 |accessdate=2010-03-29 |archiveurl=https://web.archive.org/web/20100417083733/http://www.mail-archive.com/cassandra-dev%40incubator.apache.org/msg01163.html |archivedate=17 April 2010 |deadurl=no }}&lt;/ref&gt;
*[[Constant Contact]] uses Cassandra in their email and social media marketing applications.&lt;ref name="ConstantContact"&gt;{{cite web |url=http://www.readwriteweb.com/enterprise/2011/02/this-week-in-consolidation-hp.php |title=This Week in Consolidation: HP Buys Vertica, Constant Contact Buys Bantam Live and More |first=Klint |last=Finley |publisher=Read Write Enterprise |date=2011-02-18}}&lt;/ref&gt; Over 200 nodes are deployed.
*[[Digg]], a social news website, announced on Sep 9th, 2009 that it is rolling out its use of Cassandra&lt;ref name="NM2009"&gt;{{cite web |url=http://blog.digg.com/?p=966 |title=Looking to the future with Cassandra |first=Ian |last=Eure}}&lt;/ref&gt; and confirmed this on March 8, 2010.&lt;ref name="DG2010"&gt;{{cite web|url=http://about.digg.com/node/564 |title=Saying Yes to NoSQL; Going Steady with Cassandra |first=John |last=Quinn |deadurl=yes |archiveurl=https://web.archive.org/web/20120307033743/http://about.digg.com/node/564 |archivedate=2012-03-07 |df= }}&lt;/ref&gt; [[TechCrunch]] has since linked Cassandra to Digg v4 reliability criticisms and recent company struggles.&lt;ref name="ES2010"&gt;{{cite web|url=https://techcrunch.com/2010/09/07/digg-struggles-vp-engineering-door/|title=As Digg Struggles, VP Of Engineering Is Shown The Door|first=Erick |last=Schonfeld}}&lt;/ref&gt; Lead engineers at Digg later rebuked these criticisms as red herring and blamed a lack of load testing.&lt;ref name="QU2010"&gt;{{cite web|url=http://www.quora.com/Is-Cassandra-to-blame-for-Digg-v4s-technical-failures/|title=Is Cassandra to Blame for Digg v4's Failures?}}&lt;/ref&gt;
*[[Formspring]] uses Cassandra to count responses, as well as store Social Graph data (followers, following, blockers, blocking) for 26 Million accounts with 10 million responses a day&lt;ref&gt;{{cite web|url=http://www.slideshare.net/martincozzi/cassandra-formspring |date=2011-08-31 |first=Martin |last=Cozzi |title=Cassandra at Formspring}}&lt;/ref&gt;
*[[Globo.com]] uses Cassandra as a back-end database for their streaming services&lt;ref&gt;{{cite web|url=https://www.javacodegeeks.com/2016/06/cassandra-heart-globos-live-streaming-platform.html |date=2016-06-22 |first=Alexandre |last=Nunes |title=Cassandra At The Heart Of Globo’s Live Streaming Platform}}&lt;/ref&gt;
*[[Mahalo.com]] uses Cassandra to record user activity logs and topics for their Q&amp;A website&lt;ref name="Mahalo"&gt;{{cite web|title=Mahalo.com powered by Apache Cassandra™ |date=2012-04-10 |accessdate=2014-06-13 |website=DataStax.com |publisher=[[DataStax]] |location=Santa Clara, CA, USA|url=http://www.datastax.com/wp-content/uploads/2011/06/DataStax-CaseStudy-Mahalo.pdf}}&lt;/ref&gt;&lt;ref name=Mahalo2&gt;[http://blip.tv/datastax/cassandra-at-mahalo-com-4030941 Watch Cassandra at Mahalo.com |DataStax Episodes |Blip&lt;!-- Bot generated title --&gt;] {{webarchive|url=https://web.archive.org/web/20111210143921/http://blip.tv/datastax/cassandra-at-mahalo-com-4030941 |date=2011-12-10 }}&lt;/ref&gt;
*[[Netflix]] uses Cassandra as their back-end database for their streaming services&lt;ref name="Netflix1"&gt;{{cite web|url=http://www.slideshare.net/adrianco/migrating-netflix-from-oracle-to-global-cassandra |title=Migrating Netflix from Datacenter Oracle to Global Cassandra |first=Adrian |last=Cockcroft |date=2011-07-11 |accessdate=2014-06-13 |website=slideshare.net}}&lt;/ref&gt;&lt;ref name=Netflix2&gt;{{cite web|url=http://techblog.netflix.com/2011/01/nosql-at-netflix.html |date=2011-01-28 |first=Yury |last=Izrailevsky |title=NoSQL at Netflix}}&lt;/ref&gt;
*[[Nutanix]] appliances use Cassandra to store metadata and stats.&lt;ref&gt;{{cite web|url=http://stevenpoitras.com/the-nutanix-bible/#components |title=Nutanix Bible}}&lt;/ref&gt;
*[[Ooyala]] built a scalable, flexible, real-time analytics engine using Cassandra&lt;ref name="Ooyala"&gt;{{cite web |title=Designing a Scalable Database for Online Video Analytics |url=http://www.datastax.com/wp-content/uploads/2011/04/WP-Ooyala.pdf |website=DataStax.com |location=Mountain View CA, USA |author=Ooyala |authorlink=Ooyala |date=2010-05-18 |accessdate=2014-06-14}}&lt;/ref&gt;
*[[Openwave]] uses Cassandra as a distributed database and as a distributed storage mechanism for their next generation messaging platform&lt;ref name="Openwave"&gt;{{cite web |title=DataStax Case Study of Openwave Messaging |url=http://www.datastax.com/wp-content/uploads/2011/05/DataStax-CaseStudy-Openwave.pdf |author=Mainstay LLC |date=2013-11-11 |accessdate=2014-06-15 |website=DataStax.com |publisher=[[DataStax]] |location=Santa Clara, CA, USA}}&lt;/ref&gt;
*[[OpenX (software)|OpenX]] is running over 130 nodes on Cassandra for their OpenX Enterprise product to store and replicate advertisements and targeting data for ad delivery&lt;ref name=OpenX&gt;[http://openx.com/publisher/technology Ad Serving Technology - Advanced Optimization, Forecasting, &amp; Targeting |OpenX&lt;!-- Bot generated title --&gt;] {{webarchive|url=https://web.archive.org/web/20111007063645/http://openx.com/publisher/technology |date=2011-10-07 }}&lt;/ref&gt;
*[[Rackspace]] uses Cassandra internally.&lt;ref name="Rackspace"&gt;{{cite web|url=http://www.slideshare.net/stuhood/hadoop-and-cassandra-at-rackspace |title=Hadoop and Cassandra (at Rackspace) |publisher=Stu Hood |date=2010-04-23 |accessdate=2011-09-01}}&lt;/ref&gt;
*[[Reddit]] switched to Cassandra from [[memcacheDB]] on March 12, 2010&lt;ref name=REDDIT&gt;{{cite web|author=david [ketralnis] |url=https://redditblog.com/2010/03/she-who-entangles-men.html |title=what's new on reddit: She who entangles men |publisher=blog.reddit |date=2010-03-12 |accessdate=2010-03-29|archiveurl= https://web.archive.org/web/20100325115755/http://blog.reddit.com/2010/03/she-who-entangles-men.html|archivedate= 25 March 2010 &lt;!--DASHBot--&gt;|deadurl= no}}&lt;/ref&gt; and experienced some problems in May of that year due to insufficient nodes in their cluster.&lt;ref name=REDDIT2&gt;{{cite web|author= Posted by the reddit admins at |url=https://redditblog.com/2010/05/reddits-may-2010-state-of-servers.html |title=blog.reddit -- what's new on reddit: reddit's May 2010 "State of the Servers" report |publisher=blog.reddit |date=2010-05-11 |accessdate=2010-05-16|archiveurl= https://web.archive.org/web/20100514085008/http://blog.reddit.com/2010/05/reddits-may-2010-state-of-servers.html|archivedate= 14 May 2010 &lt;!--DASHBot--&gt;|deadurl= no}}&lt;/ref&gt;
*[[RockYou]] uses Cassandra to record every single click for 50 million Monthly Active Users in real-time for their online games&lt;ref name=RockYou&gt;{{cite web |url=http://mysqldba.blogspot.com/2010/03/cassandra-is-my-nosql-solution-but.html |date=2011-03-23 |first=Dathan Vance |last=Pattishall |title=Cassandra is my NoSQL Solution but}}&lt;/ref&gt;
*[[SoundCloud]] uses Cassandra to store the dashboard of their users&lt;ref name=SoundCloud&gt;{{cite web|url=http://backstage.soundcloud.com/2011/04/failing-with-mongodb/|title=Cassandra at SoundCloud}}&lt;/ref&gt;
*[[Uber]] uses Cassandra to store around 10,000 features in their daily updated company-wide Feature Store for low-latency access during live model predictions&lt;ref name="FeaturesStore"&gt;{{cite web|url=https://eng.uber.com/michelangelo/ |title=Meet Michelangelo: Uber’s Machine Learning Platform |first=Jeremy|last=Hermann}}&lt;/ref&gt;
*[[Urban Airship]] uses Cassandra with the mobile service hosting for over 160 million application installs across 80 million unique devices&lt;ref name="UrbanAirship"&gt;{{cite web|url=http://www.slideshare.net/eonnen/from-100s-to-100s-of-millions |title=From 100s to 100s of Millions |first=Erik |last=Onnen}}&lt;/ref&gt;

== See also ==
{{Portal|Free software}}
* [[Bigtable]] - Original distributed database by Google
* [[Distributed database]]
* [[Distributed hash table]] (DHT)
* [[Dynamo (storage system)]] - Cassandra borrows many elements from Dynamo

== References ==
{{Reflist|30em}}

==Bibliography==
{{refbegin}}
*{{cite book
| first1      = Jeff
| last1       = Carpenter
| first2      = Eben
| last2       = Hewitt
| date        = July 24, 2016
| title       = Cassandra: The Definitive Guide
| publisher   = [[O'Reilly Media]]
| edition     = 2nd
| page       = 370
| isbn        = 978-1-4919-3366-4
| url         = http://shop.oreilly.com/product/0636920043041.do
}}
*{{cite book
| first1      = Edward
| last1       = Capriolo
| date        = July 15, 2011
| title       = Cassandra High Performance Cookbook
| publisher   = [[Packt Publishing]]
| edition     = 1st
| page       = 324
| isbn        = 1-84951-512-3
| url         = http://www.packtpub.com/cassandra-apache-high-performance-cookbook/book
}}
*{{cite book
| first1      = Eben
| last1       = Hewitt
| date        = December 15, 2010
| title       = Cassandra: The Definitive Guide
| publisher   = [[O'Reilly Media]]
| edition     = 1st
| page       = 300
| isbn        = 978-1-4493-9041-9
| url         = http://shop.oreilly.com/product/0636920010852.do
}}
{{refend}}

==External links==
{{Commons category}}
{{Wikiversity|Big Data/Cassandra}}

*{{cite web |title=Cassandra - A structured storage system on a P2P Network |url=https://www.facebook.com/note.php?note_id=24413138919&amp;id=9445547199&amp;index=9 |first=Avinash |last=Lakshman
|date=2008-08-25 |accessdate=2014-06-17 |publisher=Engineering @ Facebook's Notes}}
*{{cite web |url=https://cassandra.apache.org/ |title=The Apache Cassandra Project |accessdate=2014-06-17 |publisher=[[Apache Software Foundation|The Apache Software Foundation]] |location=Forest Hill, MD, USA}}
*{{cite web |url=https://wiki.apache.org/cassandra/ |title=Project Wiki|accessdate=2014-06-17 |publisher=[[Apache Software Foundation|The Apache Software Foundation]] |location=Forest Hill, MD, USA}}
*{{cite web |url=http://www.infoq.com/presentations/Adopting-Apache-Cassandra |title=Adopting Apache Cassandra |first=Eben |last=Hewitt |date=2010-12-01 |accessdate=2014-06-17 |website=infoq.com |publisher=InfoQ, C4Media Inc}}
*{{cite web |first1=Avinash |last1=Lakshman |first2=Prashant |last2=Malik |url=https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf |title=Cassandra - A Decentralized Structured Storage System |website=cs.cornell.edu |date=2009-08-15 |accessdate=2014-06-17 |others=The authors are from [[Facebook]]}}
*{{cite web |url=http://www.slideshare.net/jbellis/what-every-developer-should-know-about-database-scalability |title=What Every Developer Should Know About Database Scalability |first=Jonathan |last=Ellis |date=2009-07-29 |accessdate=2014-06-17 |website=slideshare.net}} From the [[O'Reilly Open Source Convention|OSCON]] 2009 talk on RDBMS vs. Dynamo, Bigtable, and Cassandra.
*{{cite web |url=https://code.google.com/p/cassandra-rpm/ |title=Cassandra-RPM - Red Hat Package Manager (RPM) build for the Apache Cassandra project  |website=code.google.com |accessdate=2014-06-17 |publisher=[[Google Code#Project hosting|Google Project Hosting]] |location=Menlo Park, CA, USA}}
*{{cite web |url=http://de.slideshare.net/grro/cassandra-by-example-the-path-of-read-and-write-requests |title=Cassandra by example - the path of read and write requests |first=Gregor |last=Roth |date=2012-10-14|accessdate=2014-06-17 |website=slideshare.net}}
*{{cite web |url=http://10kloc.wordpress.com/category/cassandra-2/ |title=A collection of Cassandra tutorials |first=Umer |last=Mansoor |date=2012-11-04 |accessdate=2015-02-08}}
*{{cite web|url=http://www.networkworld.com/news/tech/2012/102212-nosql-263595.html |title=A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak |first=Sergey |last=Bushik |date=2012-10-22 |work=[[Network World|NetworkWorld]] |publisher=[[International Data Group|IDG]] |location=Framingham, MA, USA and Staines, Middlesex, UK |accessdate=2014-06-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20140528110238/http://www.networkworld.com/news/tech/2012/102212-nosql-263595.html |archivedate=2014-05-28 |df= }}

{{apache}}
{{Facebook navbox}}

{{DEFAULTSORT:Cassandra (Database)}}
[[Category:2008 software]]
[[Category:Apache Software Foundation projects]]
[[Category:Apache Software Foundation]]
[[Category:Big data products]]
[[Category:Bigtable implementations]]
[[Category:Column-oriented DBMS software for Linux]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:NoSQL]]
[[Category:Structured storage]]</text>
      <sha1>iu1wtye0veathnj23raikfe7h5py3tx</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Apex</title>
    <ns>0</ns>
    <id>50498645</id>
    <revision>
      <id>847348489</id>
      <parentid>847348407</parentid>
      <timestamp>2018-06-24T17:56:54Z</timestamp>
      <contributor>
        <username>Trbrusli</username>
        <id>28326570</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4360">{{Infobox software
| title                  = Apache Apex
| name                   = Apache Apex
| logo                   = Apache Apex Logo Small.png
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| latest release version = {{URL| http://apex.apache.org/downloads.html | 3.7.0}}
| latest release date    = 28 April 2018
| status                 = Active
| programming language   = [[Java (programming language)|Java]] and [[Scala (programming language)|Scala]]
| operating system       = [[Cross-platform]]
| genre                  =
| license                = [[Apache License]] 2.0
| website                = {{url|//apex.apache.org/}}
}}

'''Apache Apex''' is a [[YARN]]-native platform that unifies [[stream processing|stream]] and [[batch processing]]. It processes [[big data]]-in-motion in a way that is [[scalable]], performant, [[fault-tolerant]], [[stateful]], secure, distributed, and easily operable.&lt;ref&gt;{{cite web|url=https://apex.apache.org|title=Apache Apex Web Page}}&lt;/ref&gt;

Apache Apex was named a top-level project by The Apache Software Foundation on April 25, 2016.&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2016/04/26/spark-rival-apache-apex-hits-top-level-status/|title=Spark rival Apache Apex hits top-level status|work=siliconangle.com}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces90|title=The Apache Software Foundation Announces Apache® Apex™ as a Top-Level Project|work=The Apache Software Foundation}}&lt;/ref&gt;

==Overview==
Apache Apex is developed under the [[Apache License]] 2.0.&lt;ref&gt;{{cite web|url=https://github.com/apache/apex-core|title=Apache Apex Github Repos}}&lt;/ref&gt; The project was driven by the San Jose, California-based start-up company DataTorrent.

There are two parts of Apache Apex: Apex Core and Apex Malhar.   Apex Core is the platform or framework for building distributed applications on [[Apache Hadoop|Hadoop]]. The core Apex platform is supplemented by Malhar, a library of connector and logic functions, enabling rapid application development. These input and output operators provide templates to sources and sinks such as [[Apache Hadoop|HDFS]], [[Amazon S3|S3]], [[NTFS|NFS]], [[File Transfer Protocol|FTP]], [[Apache Kafka|Kafka]], [[Apache ActiveMQ|ActiveMQ]], [[RabbitMQ]], [[Java Message Service|JMS]], [[Apache Cassandra|Cassandra]], [[MongoDB]], [[Redis]], [[Apache HBase|HBase]], [[CouchDB]], generic [[Java Database Connectivity|JDBC]], and other database connectors.

==History==
DataTorrent has developed the platform since 2012 and then decided to open source the core that became Apache Apex.&lt;ref&gt;{{cite web|url=http://www.informationweek.com/big-data/software-platforms/apache-apex-is-promoted-to-top-level-project/d/d-id/1325275|title=Apache Apex Is Promoted To Top-Level Project|work=informationweek.com}}&lt;/ref&gt;  It entered incubation in August 2015 and became Apache Software Foundation top level project within 8 months. DataTorrent itself shuts down in May 2018.&lt;ref&gt;{{cite web|url=https://www.datanami.com/2018/05/08/datatorrent-stream-processing-startup-folds/|title=DataTorrent, Stream Processing Startup, Folds}}&lt;/ref&gt; 

{| class="wikitable"
|-
! Module
! Version
! Release date
|-
| Apex Core
| 3.7.0
| 28 April 2018
|-
| Apex Malhar
| 3.8.0
| 12 November 2017
|-
| Apex Core
| 3.6.0
| 5 May 2017
|-
| Apex Malhar
| 3.7.0
| 31 March 2017
|-
| Apex Core
| 3.5.0
| 12 December 2016
|-
| Apex Malhar
| 3.6.0
| 9 December 2016
|-
| Apex Malhar
| 3.5.0
| 3 September 2016
|-
| Apex Core and Malhar
| 3.4.0
| 5 May 2016
|}

==Apex Big Data World==
Apex Big Data World &lt;ref&gt;{{cite web|url=http://www.apexbigdata.com/|title=Apex Big Data World}}&lt;/ref&gt;
is a conference about Apache Apex. The first conference of Apex Big Data World took place in 2017.  They were held in Pune, India and Mountain View, California, USA. 

==References==
{{reflist}}

{{Apache}}

==External links==
*{{cite web|title=Apache Apex Twitter Handle|url=https://twitter.com/ApacheApex}}

[[Category:Apache Software Foundation projects|Apex]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Apache Software Foundation]]
[[Category:Software using the Apache license]]
[[Category:Free system software]]
[[Category:Distributed stream processing]]</text>
      <sha1>j4ljffftab716xc9x8g1wznlu7369vp</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Accumulo</title>
    <ns>0</ns>
    <id>34571412</id>
    <revision>
      <id>845270226</id>
      <parentid>837773959</parentid>
      <timestamp>2018-06-10T17:07:59Z</timestamp>
      <contributor>
        <username>Yeoh Joer</username>
        <id>33249648</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5737">{{Infobox software
| name                   = Apache Accumulo
| logo                   = Accumulo logo.png
| logo size              = 220px
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 1.9.1
| latest release date    = {{release date|2018|05|14}}
| status                 = Active
| programming language   = [[Java (programming language)|Java]]
| operating system       = [[Cross-platform]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//accumulo.apache.org}}
}}
'''Apache Accumulo''' is a highly scalable sorted, distributed key-value store based on [[Google]]'s [[Bigtable]].&lt;ref&gt;[http://accumulo.apache.org/ Apache Accumulo]. Accumulo.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;  It is a system built on top of [[Apache Hadoop]], [[Apache ZooKeeper]], and [[Apache Thrift]]. Written in [[Java (programming language)|Java]], Accumulo has cell-level [[Apache Accumulo#Cell-Level Security|access label]]s and [[server-side programming]] mechanisms. According to  [[DB-Engines ranking]], Accumulo is the third most popular [[NoSQL]] [[wide column store]] behind [[Apache Cassandra]] and [[Hbase]] and the 61st most popular database engine of any type as of 2018.&lt;ref&gt;[http://db-engines.com/en/ranking/wide+column+store DB-Engines Ranking - popularity ranking of wide column stores]. Db-engines.com. Retrieved on 2018-04-10. [https://web.archive.org/web/20180410051147/http://db-engines.com/en/ranking/wide+column+store Archived 2018-04-10].&lt;/ref&gt;

==History==
Accumulo was created in 2008 by the US [[National Security Agency]] and contributed to the [[Apache Foundation]] as an incubator project in September 2011.&lt;ref name="informationweek1"&gt;[http://www.informationweek.com/news/government/enterprise-apps/231600835 NSA Submits Open Source, Secure Database To Apache - Government]. Informationweek.com (2011-09-06). Retrieved on 2013-09-18.&lt;/ref&gt;

On March 21, 2012, Accumulo graduated from incubation at Apache, making it a top-level project.&lt;ref&gt;[http://incubator.apache.org/projects/accumulo.html Accumulo Incubation Status - Apache Incubator]. Incubator.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;

===Controversy===
In June 2012 the US [[Senate Armed Services Committee]] (SASC) released the Draft 2012 Department of Defense (DoD) Authorization Bill, which included references to Apache Accumulo.  In the draft bill SASC required DoD to evaluate whether Apache Accumulo could achieve commercial viability before implementing it throughout DoD.&lt;ref&gt;Metz, Cade. (2012-12-19) [https://www.wired.com/wiredenterprise/2012/07/nsa-accumulo-google-bigtable/ NSA Mimics Google, Pisses Off Senate | Wired Enterprise]. Wired.com. Retrieved on 2013-09-18.&lt;/ref&gt; Specific criteria were not included in the draft language, but the establishment of commercial entities supporting Apache Accumulo could be considered a success factor.&lt;ref&gt;[http://www.fiercegovernmentit.com/story/sasc-accumulo-language-pro-open-source-say-proponents/2012-06-14 SASC Accumulo language pro-open source, say proponents]. FierceGovernmentIT (2012-06-14). Retrieved on 2013-09-18.&lt;/ref&gt;

==Main features==

===Cell-level security===
Apache Accumulo extends the [[Bigtable#Design|Bigtable data model]], adding a new element to the key called [http://accumulo.apache.org/1.4/user_manual/Security.html Column Visibility]. This element stores a logical combination of security labels that must be satisfied at query time in order for the key and value to be returned as part of a user request. This allows data of varying security requirements to be stored in the same table, and allows users to see only those keys and values for which they are authorized.&lt;ref name="informationweek1"/&gt;

===Server-side programming===
In addition to Cell-Level Security, Apache Accumulo provides a server-side programming mechanism called Iterators that allows users to perform additional processing at the Tablet Server. The range of operations that can be applied is equivalent to those that can be implemented within a [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/mapreduce-osdi04.pdf MapReduce Combiner function], which produces an aggregate value for several key-value pairs.

==Papers==
* 2011 [http://www.pdl.cmu.edu/PDL-FTP/Storage/socc2011.pdf YCSB++: Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores] by Carnegie Mellon University and the National Security Agency.
* 2012 [http://www.mit.edu/~kepner/pubs/ByunKepner_2012_BigData_Paper.pdf Driving Big Data With Big Compute] by MIT Lincoln Laboratory.
* 2013 [http://www.mit.edu/~kepner/pubs/D4Mschema_HPEC2013_Paper.pdf  D4M 2.0 Schema:A General Purpose High Performance Schema for the Accumulo Database] by MIT Lincoln Laboratory.
* 2013 [https://geomesa.github.io/assets/outreach/SpatioTemporalIndexing_IEEEcopyright.pdf Spatio-temporal Indexing in Non-relational Distributed Databases] by CCRi

==See also==
{{Portal|Java|Free software}}
* [[Bigtable]]
* [[Apache Cassandra]]
* [[Column-oriented DBMS]]
* [[Hypertable]]
* [[HBase]]
* [[Hadoop]]
* [[sqrrl]]

==References==
{{Reflist}}

==External links==
* {{Official website|//accumulo.apache.org}}
* [https://www.reddit.com/r/accumulo Accumulo topic on reddit]
* [http://qnalist.com/g/accumulo Accumulo mailing list archives]

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Apache Software Foundation projects|Accumulo]]
[[Category:Bigtable implementations]]
[[Category:Distributed computing architecture]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:Hadoop]]
[[Category:NoSQL products]]</text>
      <sha1>hac15h9zntwmm29id4pex4drscf8qbk</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Flink</title>
    <ns>0</ns>
    <id>46228433</id>
    <revision>
      <id>843235296</id>
      <parentid>835170744</parentid>
      <timestamp>2018-05-27T20:19:40Z</timestamp>
      <contributor>
        <ip>111.185.87.234</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23561">{{Infobox software
| title                  = Apache Flink
| name                   = Apache Flink
| logo                   = [[File:Apache_Flink_Logo.png]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| latest release version = {{URL|https://flink.apache.org/news/2018/05/25/release-1.5.0.html|1.5.0}}
| latest release date    = 25 May 2018
| status                 = Active
| programming language   = [[Java (programming language)|Java]] and [[Scala (programming language)|Scala]]
| operating system       = [[Cross-platform]]
| genre                  = Data analytics, [[machine learning]] algorithms
| license                = [[Apache License]] 2.0
| website                = {{url|//flink.apache.org/}}
}}
'''Apache Flink''' is an [[open source]] [[stream processing]] [[software framework|framework]] developed by the [[Apache Software Foundation]]. The core of Apache Flink is a distributed streaming dataflow engine written in [[Java (programming language)|Java]] and [[Scala (programming language)|Scala]].&lt;ref&gt;{{cite web|url=https://flink.apache.org/|title=Apache Flink: Scalable Batch and Stream Data Processing|work=apache.org}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://github.com/apache/flink|title=apache/flink|work=GitHub}}&lt;/ref&gt; Flink executes arbitrary [[Dataflow programming|dataflow]] programs in a [[Data parallelism|data-parallel]] and [[Pipeline (computing)|pipelined]] manner.&lt;ref&gt;Alexander Alexandrov, Rico Bergmann, Stephan Ewen, Johann-Christoph Freytag, Fabian Hueske, Arvid Heise, Odej Kao, Marcus Leich, Ulf Leser, Volker Markl, Felix Naumann, Mathias Peters, Astrid Rheinländer, Matthias J. Sax, Sebastian Schelter, Mareike Höger, Kostas Tzoumas, and Daniel Warneke. 2014. ''The Stratosphere platform for big data analytics''. The VLDB Journal 23, 6 (December 2014), 939-964. [https://dx.doi.org/10.1007/s00778-014-0357-y DOI]&lt;/ref&gt; Flink's pipelined runtime system enables the execution of [[Batch processing|bulk/batch]] and stream processing programs.&lt;ref&gt;{{cite web|url=http://www.infoworld.com/article/2919602/hadoop/flink-hadoops-new-contender-for-mapreduce-spark.html|title=Apache Flink: New Hadoop contender squares off against Spark|author=Ian Pointer|date=7 May 2015|work=InfoWorld}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.odbms.org/blog/2015/06/on-apache-flink-interview-with-volker-markl/|title=On Apache Flink. Interview with Volker Markl.|work=odbms.org}}&lt;/ref&gt; Furthermore, Flink's runtime supports the execution of [[Iterative method|iterative algorithms]] natively.&lt;ref&gt;Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl. 2012. ''Spinning fast iterative data flows''. Proc. VLDB Endow. 5, 11 (July 2012), 1268-1279. [https://dx.doi.org/10.14778/2350229.2350245 DOI]&lt;/ref&gt;

Flink provides a high-throughput, low-latency streaming engine&lt;ref&gt;{{Cite news|url=https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at|title=Benchmarking Streaming Computation Engines at Yahoo!|newspaper=Yahoo Engineering|access-date=2017-02-23}}&lt;/ref&gt; as well as support for event-time processing and state management. Flink applications are fault-tolerant in the event of machine failure and support exactly-once semantics.&lt;ref name=":2"&gt;{{Cite arxiv|last=Carbone|first=Paris|last2=Fóra|first2=Gyula|last3=Ewen|first3=Stephan|last4=Haridi|first4=Seif|last5=Tzoumas|first5=Kostas|date=2015-06-29|title=Lightweight Asynchronous Snapshots for Distributed Dataflows|eprint=1506.08603|class=cs.DC}}&lt;/ref&gt; Programs can be written in [[Java (programming language)|Java]], [[Scala (programming language)|Scala]],&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/datastream_api.html|title=Apache Flink 1.2.0 Documentation: Flink DataStream API Programming Guide|website=ci.apache.org|language=en|access-date=2017-02-23}}&lt;/ref&gt; [[Python (programming language)|Python]],&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/batch/python.html|title=Apache Flink 1.2.0 Documentation: Python Programming Guide|website=ci.apache.org|language=en|access-date=2017-02-23}}&lt;/ref&gt; and [[SQL]]&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/table_api.html|title=Apache Flink 1.2.0 Documentation: Table and SQL|website=ci.apache.org|language=en|access-date=2017-02-23}}&lt;/ref&gt; and are automatically compiled and optimized&lt;ref&gt;Fabian Hueske, Mathias Peters, Matthias J. Sax, Astrid Rheinländer, Rico Bergmann, Aljoscha Krettek, and Kostas Tzoumas. 2012. ''Opening the black boxes in data flow optimization''. Proc. VLDB Endow. 5, 11 (July 2012), 1256-1267. [https://dx.doi.org/10.14778/2350229.2350244 DOI]&lt;/ref&gt; into dataflow programs that are executed in a cluster or cloud environment.&lt;ref&gt;Daniel Warneke and Odej Kao. 2009. ''Nephele: efficient parallel data processing in the cloud''. In Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers (MTAGS '09). ACM, New York, NY, USA, Article 8, 10 pages. [http://doi.acm.org/10.1145/1646468.1646476 DOI]&lt;/ref&gt;

Flink does not provide its own data storage system and provides data source and sink connectors to systems such as [[Amazon_Web_Services#Analytics|Amazon Kinesis]], [[Apache Kafka]], [[HDFS]], [[Apache Cassandra]], and [[Elasticsearch|ElasticSearch]].&lt;ref name=":0"&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/connectors/index.html|title=Apache Flink 1.2.0 Documentation: Streaming Connectors|website=ci.apache.org|language=en|access-date=2017-02-23}}&lt;/ref&gt;

==Development==
Apache Flink is developed under the [[Apache License]] 2.0&lt;ref&gt;{{cite web|url=https://git1-us-west.apache.org/repos/asf?p=flink.git;a=blob;f=LICENSE;hb=HEAD|title=ASF Git Repos - flink.git/blob - LICENSE|work=apache.org}}&lt;/ref&gt; by the Apache Flink Community within the [[Apache Software Foundation]]. The project is driven by over 25 committers and over [https://github.com/apache/flink 340 contributors.]

[http://www.data-artisans.com data Artisans] is a company that was founded by the original creators of Apache Flink.&lt;ref&gt;{{cite web|url=http://data-artisans.com/about/team/|title=Team – data Artisans|website=data-artisans.com|language=en-US|access-date=2017-02-23}}&lt;/ref&gt; 12 Apache Flink committers are currently employed by data Artisans.&lt;ref&gt;{{cite web|url=http://flink.apache.org/community.html#people|title=Apache Flink: Community &amp; Project Info|website=flink.apache.org|language=en|access-date=2017-02-23}}&lt;/ref&gt;

== Overview ==
Apache Flink’s [[Dataflow programming|dataflow programming model]] provides event-at-a-time processing on both finite and infinite datasets. At a basic level, Flink programs consist of streams and transformations. “Conceptually, a stream is a (potentially never-ending) flow of data records, and a transformation is an operation that takes one or more streams as input, and produces one or more output streams as a result.”&lt;ref name=":1"&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/concepts/programming-model.html#programs-and-dataflows|title=Apache Flink 1.2.0 Documentation: Dataflow Programming Model|website=ci.apache.org|language=en|access-date=2017-02-23}}&lt;/ref&gt;

Apache Flink includes two core APIs: a DataStream API for bounded or unbounded streams of data and a DataSet API for bounded data sets. Flink also offers a Table API, which is a SQL-like expression language for relational stream and batch processing that can be easily embedded in Flink’s DataStream and DataSet APIs. The highest-level language supported by Flink is SQL, which is semantically similar to the Table API and represents programs as SQL query expressions.

=== Programming Model and Distributed Runtime ===
Upon execution, Flink programs are mapped to streaming [[dataflow]]s.&lt;ref name=":1" /&gt; Every Flink dataflow starts with one or more sources (a data input, e.g. a message queue or a file system) and ends with one or more sinks (a data output, e.g. a message queue, file system, or database). An arbitrary number of transformations can be performed on the stream. These streams can be arranged as a directed, acyclic dataflow graph, allowing an application to branch and merge dataflows.

Flink offers ready-built source and sink connectors with [[Apache Kafka]], Amazon Kinesis, [[HDFS]], [[Apache Cassandra]], and more.&lt;ref name=":0" /&gt;

Flink programs run as a distributed system within a cluster and can be deployed in a standalone mode as well as on YARN, Mesos, Docker-based setups along with other resource management frameworks.&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/concepts/runtime.html|title=Apache Flink 1.2.0 Documentation: Distributed Runtime Environment|website=ci.apache.org|language=en|access-date=2017-02-24}}&lt;/ref&gt;

=== State: Checkpoints, Savepoints, and Fault-tolerance ===
Apache Flink includes a lightweight fault tolerance mechanism based on distributed checkpoints.&lt;ref name=":2" /&gt; A checkpoint is an automatic, asynchronous snapshot of the state of an application and the position in a source stream. In the case of a failure, a Flink program with checkpointing enabled will, upon recovery, resume processing from the last completed checkpoint, ensuring that Flink maintains exactly-once state semantics within an application. The checkpointing mechanism exposes hooks for application code to include external systems into the checkpointing mechanism as well (like opening and committing transactions with a database system).

Flink also includes a mechanism called savepoints, which are manually-triggered checkpoints.&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/concepts/runtime.html#savepoints|title=Apache Flink 1.2.0 Documentation: Distributed Runtime Environment - Savepoints|last=|first=|date=|website=ci.apache.org|language=en|archive-url=|archive-date=|dead-url=|access-date=2017-02-24}}&lt;/ref&gt; A user can generate a savepoint, stop a running Flink program, then resume the program from the same application state and position in the stream. Savepoints enable updates to a Flink program or a Flink cluster without losing the application's state . As of Flink 1.2, savepoints also allow to restart an application with a different parallelism—allowing users to adapt to changing workloads.

=== DataStream API ===
[https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/datastream_api.html Flink’s DataStream API] enables transformations (e.g. filters, aggregations, window functions) on bounded or unbounded streams of data. The DataStream API includes more than 20 different types of transformations and is available in Java and Scala.&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/datastream_api.html|title=Apache Flink 1.2.0 Documentation: Flink DataStream API Programming Guide|website=ci.apache.org|language=en|access-date=2017-02-24}}&lt;/ref&gt;

A simple example of a stateful stream processing program is an application that emits a word count from a continuous input stream and groups the data in 5-second windows:&lt;syntaxhighlight lang="scala"&gt;
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time

case class WordCount(word: String, count: Int)

object WindowWordCount {
  def main(args: Array[String]) {

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val text = env.socketTextStream("localhost", 9999)

    val counts = text.flatMap { _.toLowerCase.split("\\W+") filter { _.nonEmpty } }
      .map { WordCount(_, 1) }
      .keyBy("word")
      .timeWindow(Time.seconds(5))
      .sum("count")

    counts.print

    env.execute("Window Stream WordCount")
  }
}
&lt;/syntaxhighlight&gt;

==== Apache Beam - Flink Runner ====
[[Apache Beam]] “provides an advanced unified programming model, allowing (a developer) to implement batch and streaming data processing jobs that can run on any execution engine.”&lt;ref&gt;{{cite web|url=https://beam.apache.org/|title=Apache Beam|website=beam.apache.org|language=en|access-date=2017-02-24}}&lt;/ref&gt; The Apache Flink-on-Beam runner is the most feature-rich according to a capability matrix maintained by the Beam community.&lt;ref&gt;{{cite web|url=https://beam.apache.org/documentation/runners/capability-matrix/|title=Apache Beam Capability Matrix|website=beam.apache.org|language=en|access-date=2017-02-24}}&lt;/ref&gt;

[http://www.data-artisans.com data Artisans], in conjunction with the Apache Flink community, worked closely with the Beam community to develop a robust Flink runner.&lt;ref&gt;{{cite web|url=https://cloud.google.com/blog/big-data/2016/05/why-apache-beam-a-google-perspective|title=Why Apache Beam? A Google Perspective {{!}} Google Cloud Big Data and Machine Learning Blog  {{!}}  Google Cloud Platform|website=Google Cloud Platform|language=en|access-date=2017-02-24}}&lt;/ref&gt;

=== DataSet API ===
[https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/batch/index.html Flink’s DataSet API] enables transformations (e.g. filters, mapping, joining, grouping) on bounded datasets. The DataSet API includes more than 20 different types of transformations.&lt;ref&gt;{{cite web|url=https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/batch/index.html|title=Apache Flink 1.2.0 Documentation: Flink DataSet API Programming Guide|website=ci.apache.org|language=en|access-date=2017-02-24}}&lt;/ref&gt; The API is available in Java, Scala and an experimental Python API. Flink’s DataSet API is conceptually similar to the DataStream API.

=== Table API and SQL ===
[https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/table_api.html Flink's Table API] is a SQL-like expression language for relational stream and batch processing that can be embedded in Flink’s Java and Scala DataSet and DataStream APIs. The Table API and SQL interface operate on a relational Table abstraction. Tables can be created from external data sources or from existing DataStreams and DataSets. The Table API supports relational operators such as selection, aggregation, and joins on Tables.

Tables can also be queried with regular SQL. The Table API and SQL offer equivalent functionality and can be mixed in the same program. When a Table is converted back into a DataSet or DataStream, the logical plan, which was defined by relational operators and SQL queries, is optimized using [[Apache Calcite]] and is transformed into a DataSet or DataStream program.

==Flink Forward==
[http://flink-forward.org Flink Forward] is an annual conference about Apache Flink. The first edition of Flink Forward took place in 2015 in Berlin. The two-day conference had over 250 attendees from 16 countries. Sessions were organized in two tracks with over 30 technical presentations from Flink developers and one additional track with hands-on Flink training.

In 2016, 350 participants joined the conference and over 40 speakers presented technical talks in 3 parallel tracks. On the third day, attendees were invited to participate in hands-on training sessions.

In 2017, the event expands to San Francisco, as well. The conference day is dedicated to technical talks on how Flink is used in the enterprise, Flink system internals, ecosystem integrations with Flink, and the future of the platform. It features keynotes, talks from Flink users in industry and academia, and hands-on training sessions on Apache Flink.

Speakers from the following organizations have presented at Flink Forward conferences: [[Alibaba Group|Alibaba]], [[Amadeus IT Group|Amadeus]], [[Bouygues Telecom]], [[Capital One]], [[Cloudera]], data Artisans, [[Dell EMC|EMC]], [[Ericsson]], [[Hortonworks]], [[Huawei]], [[IBM]], [[Google]], [[MapR]], [[MongoDB]], [[Netflix]], [[New Relic]], [[Otto GmbH|Otto Group]], [[Red Hat]], [[ResearchGate]], [[Uber]], and [[Zalando]].&lt;ref&gt;{{cite web|url=http://2016.flink-forward.org/program/sessions/|title=Sessions {{!}}             FlinkForward {{!}}                             12-14 Sep 2016                         {{!}} Berlin|website=2016.flink-forward.org|language=en-US|access-date=2017-02-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://2015.flink-forward.org/|title=Flink Forward » Flink Forward 2015|website=2015.flink-forward.org|language=en-US|access-date=2017-02-24}}&lt;/ref&gt;

==History==
In 2010, the research project "Stratosphere: Information Management on the Cloud"&lt;ref name="stratosphere"&gt;{{cite web|url=http://stratosphere.eu/|title=Stratosphere|work=stratosphere.eu}}&lt;/ref&gt; (funded by the [[Deutsche Forschungsgemeinschaft|German Research Foundation (DFG)]]&lt;ref&gt;{{cite web|url=http://dfg.de/foerderung/programme/listen/projektdetails/index.jsp?id=132320961|title=DFG - Deutsche Forschungsgemeinschaft -|work=dfg.de}}&lt;/ref&gt;) was started as a collaboration of [[Technical University of Berlin|Technical University Berlin]], [[Humboldt University of Berlin|Humboldt-Universität zu Berlin]], and [[Hasso Plattner Institute|Hasso-Plattner-Institut]] Potsdam. Flink started from a fork of Stratosphere’s distributed execution engine and it became an [[Apache Incubator]] project in March 2014.&lt;ref&gt;{{cite web|url=https://wiki.apache.org/incubator/StratosphereProposal|title=Stratosphere|work=apache.org}}&lt;/ref&gt; In December 2014, Flink was accepted as an Apache top-level project.&lt;ref&gt;{{cite web|url=https://projects.apache.org/project.html?flink|title=Project Details for Apache Flink|work=apache.org}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces69|title=The Apache Software Foundation Announces Apache™ Flink™ as a Top-Level Project : The Apache Software Foundation Blog|work=apache.org}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2015/02/09/will-the-mysterious-apache-flink-find-its-sweet-spot-in-the-enterprise|title=Will the mysterious Apache Flink find a sweet spot in the enterprise?|work=siliconangle.com}}&lt;/ref&gt;&lt;ref&gt;[http://www.heise.de/developer/meldung/Big-Data-Apache-Flink-wird-Top-Level-Projekt-2516177.html (in German)]&lt;/ref&gt;

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date
|-
| {{Version|o|0.9}}
| 2015-06-24
| 0.9.1
| 2015-09-01
|-
| {{Version|o|0.10}}
| 2015-11-16
| 0.10.2
| 2016-02-11
|-
| {{Version|o|1.0}}
| 2016-03-08
| 1.0.3
| 2016-05-11
|-
| {{Version|o|1.1}}
| 2016-08-08
| 1.1.4
| 2016-12-21
|-
| {{Version|o|1.2}}
| 2017-02-06
| 1.2.1
| 2017-04-26
|-
| {{Version|o|1.3}}
| 2017-06-01
| 1.3.3
| 2018-03-15
|-
| {{Version|co|1.4}}
| 2017-12-12
| 1.4.2
| 2018-03-08
|-
| {{Version|c|1.5}}
| 2018-05-25
| 1.5.0
| 2018-05-25
|-
| colspan="5" | &lt;small&gt;{{Version |l |show=111110}}&lt;/small&gt;
|}

''Release Dates''
* 05/2018: [https://flink.apache.org/news/2018/05/25/release-1.5.0.html Apache Flink 1.5]
* 12/2017: [https://flink.apache.org/news/2017/12/12/release-1.4.0.html Apache Flink 1.4](02/2018: [https://flink.apache.org/news/2018/02/15/release-1.4.1.html v1.4.1]; 03/2018: [https://flink.apache.org/news/2018/03/08/release-1.4.2.html v1.4.2])
* 06/2017: [https://flink.apache.org/news/2017/06/01/release-1.3.0.html Apache Flink 1.3](06/2017: [https://flink.apache.org/news/2017/06/23/release-1.3.1.html v1.3.1]; 08/2017: [https://flink.apache.org/news/2017/08/05/release-1.3.2.html v1.3.2]; 03/2018: [https://flink.apache.org/news/2018/03/15/release-1.3.3.html v1.3.3])
* 02/2017: [https://flink.apache.org/news/2017/02/06/release-1.2.0.html Apache Flink 1.2](04/2017: [https://flink.apache.org/news/2017/04/26/release-1.2.1.html v1.2.1])
* 08/2016: [http://flink.apache.org/news/2016/08/08/release-1.1.0.html Apache Flink 1.1] (08/2016: [https://flink.apache.org/news/2016/08/11/release-1.1.1.html v1.1.1]; 09/2016 [https://flink.apache.org/news/2016/09/05/release-1.1.2.html v1.1.2]; 10/2016 [https://flink.apache.org/news/2016/10/12/release-1.1.3.html v1.1.3];; 12/2016 [https://flink.apache.org/news/2016/12/21/release-1.1.4.html v1.1.4]))
* 03/2016: [https://flink.apache.org/news/2016/03/08/release-1.0.0.html Apache Flink 1.0] (04/2016: [https://flink.apache.org/news/2016/04/06/release-1.0.1.html v1.0.1]; 04/2016: [https://flink.apache.org/news/2016/04/22/release-1.0.2.html v1.0.2]; 05/2016 [http://flink.apache.org/news/2016/05/11/release-1.0.3.html v1.0.3])
* 11/2015: [https://flink.apache.org/news/2015/11/16/release-0.10.0.html Apache Flink 0.10] (11/2015: [https://flink.apache.org/news/2015/11/27/release-0.10.1.html v0.10.1]; 02/2016: [https://flink.apache.org/news/2016/02/11/release-0.10.2.html v0.10.2])
* 06/2015: [https://flink.apache.org/news/2015/06/24/announcing-apache-flink-0.9.0-release.html Apache Flink 0.9] (09/2015: [https://flink.apache.org/news/2015/09/01/release-0.9.1.html v0.9.1])
** 04/2015: [https://flink.apache.org/news/2015/04/13/release-0.9.0-milestone1.html Apache Flink 0.9-milestone-1]
''Apache Incubator Release Dates''
* 01/2015: [https://flink.apache.org/news/2015/01/21/release-0.8.html Apache Flink 0.8-incubating]
* 11/2014: [https://flink.apache.org/news/2014/11/04/release-0.7.0.html Apache Flink 0.7-incubating]
* 08/2014: [https://flink.apache.org/news/2014/08/26/release-0.6.html Apache Flink 0.6-incubating] (09/2014: [https://flink.apache.org/news/2014/09/26/release-0.6.1.html v0.6.1-incubating])
* 05/2014: Stratosphere 0.5 (06/2014: v0.5.1; 07/2014: v0.5.2)
''Pre-Apache Stratosphere Release Dates''
* 01/2014: Stratosphere 0.4 (version 0.3 was skipped)
* 08/2012: Stratosphere 0.2
* 05/2011: Stratosphere 0.1 (08/2011: v0.1.1)

==See also==
*[[List of Apache Software Foundation projects]]
* Other comparable data processing engines such as Storm and Spark.&lt;ref&gt;{{cite web |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7530084 |format=PDF |title= Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming |date=May 2016 |publisher=IEEE }}&lt;/ref&gt;
* [[Apache Beam]], a shared programming model for which Flink is a founding backend.

==References==
{{reflist|colwidth=30em}}

==External links==
*{{cite web|title=Data Artisans Blog|url=http://data-artisans.com/blog/}}
*{{cite web|title=Announcing Google Cloud Dataflow runner for Apache Flink|url=http://googlecloudplatform.blogspot.de/2015/03/announcing-Google-Cloud-Dataflow-runner-for-Apache-Flink.html}}
*{{cite web|title=Apache Flink is now running on Google Cloud Dataflow|url=http://www.cloudhub.uk.com/2754/apache-flink-now-running-google-cloud-dataflow}}
*{{cite web|title=Google Cloud Dataflow bekommt Runner für Apache Flink|url=http://www.heise.de/developer/meldung/Big-Data-Google-Cloud-Dataflow-bekommt-Runner-fuer-Apache-Flink-2583392.html}} (in German)
*{{cite web|title=Flink@Twitter|url=https://twitter.com/ApacheFlink}}
*[http://www.zdnet.com/article/going-with-the-stream-unbounded-data-processing-with-apache-flink/ "Going with the stream: Unbounded data processing with Apache Flink"]
*[https://www.infoq.com/news/2017/02/apache-flink-1.2 "Apache Flink 1.2 Released with Dynamic Rescaling, Security and Queryable State"]
*[https://research.google.com/pubs/pub43864.html "The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing"]

{{Apache}}

[[Category:Apache Software Foundation projects|Flink]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Apache Software Foundation|Flink]]
[[Category:Software using the Apache license]]
[[Category:Free system software]]
[[Category:Distributed stream processing]]
[[Category:Articles with example Scala code]]</text>
      <sha1>f6qlb2jtpq7ai6wdw1swnt2icq8izfe</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Parquet</title>
    <ns>0</ns>
    <id>51579024</id>
    <revision>
      <id>845999894</id>
      <parentid>832730836</parentid>
      <timestamp>2018-06-15T16:04:09Z</timestamp>
      <contributor>
        <username>LordOfPens</username>
        <id>3914242</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6607">{{Primary sources|date=October 2016}}

{{Infobox Software
| name = Apache Parquet
| logo = 
| screenshot =
| caption = Apache Parquet
| developer = 
| status = Active
| latest release version = 1.9.0&lt;ref&gt;{{cite web|title=Github releases|url=https://github.com/apache/parquet-mr/releases}}&lt;/ref&gt;
| latest release date = {{start date and age|2016|10|19|df=yes}}
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language =
| genre = [[Database management system]]
| license = [[Apache License]] 2.0
| website = https://parquet.apache.org
}}

'''Apache Parquet''' is a [[free and open-source]] [[Column-oriented DBMS|column-oriented]] data store of the [[Apache Hadoop]] ecosystem. It is similar to the other columnar-storage file formats available in [[Apache Hadoop|Hadoop]] namely [[RCFile]] and Optimized RCFile. It is compatible with most of the data processing frameworks in the [[Hadoop]] environment. It provides efficient [[data compression]] and [[encoding]] schemes with enhanced performance to handle complex data in bulk.

The [[open-source]] project to build Apache Parquet began as a joint effort between [[Twitter]]&lt;ref&gt;{{cite web|url=https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop|title=Release Date}}&lt;/ref&gt; and [[Cloudera]].&lt;ref&gt;{{Cite web|url=http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/|title=Introducing Parquet: Efficient Columnar Storage for Apache Hadoop - Cloudera Engineering Blog|date=2013-03-13|language=en-US|access-date=2016-09-14}}&lt;/ref&gt; The first version&amp;mdash;Apache Parquet 1.0&amp;mdash;was released in July 2013. Since April 27, 2015, Apache Parquet is a top-level Apache Software Foundation (ASF)-sponsored project.&lt;ref&gt;http://www.infoworld.com/article/2915565/big-data/apache-parquet-paves-the-way-towards-better-hadoop-data-storage.html&lt;/ref&gt;&lt;ref&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&lt;/ref&gt;

==Features==

Apache Parquet is implemented using the record-shredding and assembly algorithm,&lt;ref&gt;{{cite web|title=The striping and assembly algorithms from the Google-inspired Dremel paper|url=https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper|website=github|accessdate=13 November 2017}}&lt;/ref&gt; which accommodates the complex [[data structures]] that can be used to store the data.&lt;ref name=":1"&gt;{{cite web|url=https://parquet.apache.org/documentation/latest/|title=Apache Parquet Documentation|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:&lt;ref&gt;{{cite web|title=Apache Parquet Cloudera|url=http://www.cloudera.com/documentation/enterprise/5-5-x/topics/cdh_ig_parquet.html}}&lt;/ref&gt;

* Column-wise compression is efficient and saves storage space
* Compression techniques specific to a type can be applied as the column values tend to be of the same type
* Queries that fetch specific column values need not read the entire row data thus improving performance
* Different encoding techniques can be applied to different columns

Moreover, Apache Parquet is implemented using the [[Apache Thrift]] framework which increases its flexibility; it can work with a number of programming languages like [[C++]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[PHP]], etc.&lt;ref&gt;{{cite web|title=Apache Thrift|url=http://thrift.apache.org/}}&lt;/ref&gt;

As of August 2015,&lt;ref&gt;{{cite web|title=Supported Frameworks|url=https://cwiki.apache.org/confluence/display/Hive/Parquet}}&lt;/ref&gt; Parquet supports the big-data-processing frameworks including [[Apache Hive]], [[Apache Drill]], [[Apache Impala]], [[Apache Crunch]], [[Apache Pig]], [[Cascading (software)|Cascading]] and [[Apache Spark]].

== Compression and encoding ==
In Parquet, compression is performed column by column, which enables different encoding schemes to be used for text and integer data. This strategy also keeps the door open for newer and better encoding schemes to be implemented as they are invented.

=== Dictionary encoding ===

Parquet has an automatic dictionary encoding enabled dynamically for data with a small number of unique values ( &lt; 10^5 ) that enables significant compression and boosts processing speed.&lt;ref name=":0"&gt;{{Cite web|url=https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop|title=Announcing Parquet 1.0: Columnar Storage for Hadoop {{!}} Twitter Blogs|website=blog.twitter.com|access-date=2016-09-14}}&lt;/ref&gt;

=== Bit packing ===
Storage of integers is usually done with dedicated 32 or 64 bits per integer. For small integers, packing multiple integers into the same space makes storage more efficient.&lt;ref name=":0" /&gt;

=== Run-length encoding (RLE) ===
To optimize storage of multiple occurrences of the same value, a single value is stored once along with the number of occurrences.&lt;ref name=":0" /&gt;

Parquet implements a hybrid of bit packing and RLE, in which the encoding switches based on which produces the best compression results. This strategy works well for certain types of integer data and combines well with dictionary encoding.&lt;ref name=":0" /&gt;

==Comparison==
{{Unreferenced section|date=October 2016}}

Apache Parquet is comparable to [[RCFile]] and Optimized RCFile (ORC) file formats---all three fall under the category of columnar data storage within the Hadoop ecosystem. They all have better compression and encoding with improved read performance at the cost of slower writes. In addition to these features, Apache Parquet supports limited [[schema evolution]], i.e., the schema can be modified according to the changes in the data. It also provides the ability to add new columns and merge schemas that don't conflict.

==See also==
{{Portal|Free software}}
* [[Pig (programming tool)]]
* [[Apache Hive]]
* [[Apache Impala]]
* [[Apache Drill]]
* [[Apache Kudu]]
* [[Apache Spark]]
* [[Apache Thrift]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}
* [https://research.google.com/pubs/pub36632.html Dremel paper]
* [https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04 How to Be a Hero with Powerful Apache Parquet, Google and Amazon]

{{Apache}}

[[Category:2015 software]]
[[Category:Apache Software Foundation|Parquet]]
[[Category:Cloud computing]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>leiz2h6go9twsquiew43uwxdeok40oq</sha1>
    </revision>
  </page>
  <page>
    <title>Apache LDAP API</title>
    <ns>0</ns>
    <id>52748263</id>
    <revision>
      <id>790240371</id>
      <parentid>790239418</parentid>
      <timestamp>2017-07-12T13:47:36Z</timestamp>
      <contributor>
        <username>Smckin</username>
        <id>28129064</id>
      </contributor>
      <minor/>
      <comment>update latest version of release</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3014">{{unreferenced|date=June 2017}}

{{Infobox software
| name = Apache LDAP API
| developer = [[Apache Software Foundation]]
| latest preview version = 1.0.0
| latest preview date = {{release date|2017|06|07}}
| programming language = [[Java (programming language)|Java]]
| genre = [[Application programming interface]] ([[LDAP]])
| license = [[Apache License]] 2.0
| website = {{URL|directory.apache.org/api/}}
}}
'''Apache LDAP API''' is an [[open source]] project of the [[Apache Software Foundation]] and a subproject of the [[Apache Directory]].  It's a replacement for outdated Java/LDAP libraries like (jLdap, Mozilla LDAP SDK and JNDI) and works with any [[Lightweight Directory Access Protocol|LDAP]] server.

== History ==
The [[Apache Directory]] project was started using the '''[[Java Naming and Directory Interface|JNDI]]''' library, but many of its '''LDAP''' structures had to be developed in-house because the JNDI library was ineffective for interacting with an LDAP server.  It wasn't convenient for the project team to use JNDI which indicated to them it wouldn't be easy for typical users either.  Eventually, all of the necessary LDAP data structures (_Attribute_, _Entry_, _DN_, ...) were re-implemented by the project team.

At some point it became necessary to communicate with other LDAP servers without using the JNDI library, so a new _LdapConnection_ class was developed. This was the first step toward a full Java API specifically designed for LDAP usage on the Java platform.

After starting this effort (back in 2007), some people from '''[[Sun Microsystems|Sun]]''' (Microsystems), who was working on the '''[[OpenDJ|OpenDS]]''' project, contacted the Apache Directory project team to gauge interest in helping create a new version of JNDI. ([Resurrecting The Java LDAP Centric API](https://blogs.oracle.com/treydrake/entry/resurrecting_the_java_ldap_centric). Unfortunately, the effort stalled, as the need for '''JNDI2''' was no longer a priority for Sun. Nevertheless the Apache Directory team continued with the work but the pace was slow.

Collaboration with Sun renewed after the '''[[OpenDJ|OpenDS]]''' project team's presentation at '''LdapCon''' in 2009 ([Towards a common LDAP API for the Java Platform](http://www.symas.com/ldapcon2009/papers/poitou1.shtml)). The story repeated itself once again after '''Oracle''' bought Sun in 2010, and its project team disbanded.

Despite these fits and starts, a consensus was reached about the need for a new LDAP API and what it should be capable of doing. An agreement that these key features should be included:

* A complete coverage of the LDAP protocol
* A schema aware API
* An easy to use API
* An API taking advantage of the new Java construction (generics, ellipsis, NIO)


== References ==
{{reflist}}

== External links ==
* [https://directory.apache.org/api/ Apache Directory LDAP API Project Page]
{{apache}}

[[Category:Apache Software Foundation|Directory]]
[[Category:Directory services]]

{{security-software-stub}}</text>
      <sha1>a3bovxnol2g2tex7igw3hrn6xcgmhg1</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Beam</title>
    <ns>0</ns>
    <id>51237252</id>
    <revision>
      <id>831806398</id>
      <parentid>830767367</parentid>
      <timestamp>2018-03-22T08:27:12Z</timestamp>
      <contributor>
        <ip>2601:600:8780:7501:6801:C7F4:A843:3725</ip>
      </contributor>
      <comment>New release.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6630">{{Infobox Software
| name                   = Apache Beam
| logo                   = Beam-logo-full-color-name-right-200-autocrop.png
| caption                = Beam logo
| developer              = [[Apache Software Foundation]]
| status                 = Active
| released               = {{Start date and age|2016|06|15}}
| latest release version = {{URL|https://beam.apache.org/get-started/downloads/|2.4.0}}
| latest release date    = {{Start date and age|2018|03|20}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]], [[Python (programming language)|Python]]
| genre                  =
| license                = [[Apache License]] 2.0 
| website                = {{URL|https://beam.apache.org}}
}}
'''Apache Beam''' is an [[Open-source software|open source]] unified programming model to define and execute data processing [[Pipeline (computing)|pipelines]], including [[Extract, transform, load|ETL]], [[Batch processing|batch]] and [[Stream processing|stream]] (continuous) processing.&lt;ref name="Woodie2016"&gt;{{cite web|last1=Woodie|first1=Alex|title=Apache Beam's Ambitious Goal: Unify Big Data Development|url=https://www.datanami.com/2016/04/22/apache-beam-emerges-ambitious-goal-unify-big-data-development/|website=Datanami|accessdate=4 August 2016|date=22 April 2016}}&lt;/ref&gt; Beam Pipelines are defined using one of the provided [[Software development kit|SDKs]] and executed in one of the Beam’s supported ''runners'' (distributed processing back-ends) including [[Apache Apex]], [[Apache Flink]], [[Apache Spark]], and [[Google Cloud Dataflow]]&lt;ref name="google.com"&gt;{{cite web|url=https://cloud.google.com/dataflow/|title=Cloud Dataflow - Batch &amp; Stream Data Processing|publisher=}}&lt;/ref&gt;

It has been termed an "uber-API for [[big data]]".&lt;ref name=uber&gt;{{cite web|url=http://www.infoworld.com/article/3056172/application-development/apache-beam-wants-to-be-uber-api-for-big-data.html |title=Apache Beam wants to be uber-API for big data |author=Ian Pointer |publisher=[[InfoWorld]] |date=April 14, 2016}}&lt;/ref&gt;

==History==
Apache Beam&lt;ref name="google.com"/&gt; is one implementation of the Dataflow model paper.&lt;ref name="Akidau2015"&gt;{{cite journal|last1=Akidau|first1=Tyler|last2=Schmidt|first2=Eric|last3=Whittle|first3=Sam|last4=Bradshaw|first4=Robert|last5=Chambers|first5=Craig|last6=Chernyak|first6=Slava|last7=Fernández-Moctezuma|first7=Rafael J.|last8=Lax|first8=Reuven|last9=McVeety|first9=Sam|last10=Mills|first10=Daniel|last11=Perry|first11=Frances|title=The dataflow model|journal=Proceedings of the VLDB Endowment|date=1 August 2015|volume=8|issue=12|pages=1792–1803|doi=10.14778/2824032.2824076|url=http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf|accessdate=4 August 2016}}&lt;/ref&gt; The Dataflow model is based on previous work on distributed processing abstractions at Google, in particular on FlumeJava&lt;ref name="Chambers2010"&gt;{{cite journal|last1=Chambers |first1=Craig |last2=Raniwala |first2=Ashish |last3=Perry |first3=Frances |last4=Adams |first4=Stephen |last5=Henry |first5=Robert R. |last6=Bradshaw |first6=Robert |last7=Weizenbaum |first7=Nathan |title=FlumeJava: Easy, Efficient Data-parallel Pipelines |journal=Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation |date=1 January 2010 |pages=363–375 |doi=10.1145/1806596.1806638 |url=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf |archive-url=https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf |dead-url=yes |archive-date=23 September 2016 |accessdate=4 August 2016 |publisher=ACM }}&lt;/ref&gt; and Millwheel.&lt;ref name="Akidau2013"&gt;{{cite journal|last1=Akidau |first1=Tyler |last2=Whittle |first2=Sam |last3=Balikov |first3=Alex |last4=Bekiroğlu |first4=Kaya |last5=Chernyak |first5=Slava |last6=Haberman |first6=Josh |last7=Lax |first7=Reuven |last8=McVeety |first8=Sam |last9=Mills |first9=Daniel |last10=Nordstrom |first10=Paul |title=MillWheel |journal=Proceedings of the VLDB Endowment |date=27 August 2013 |volume=6 |issue=11 |pages=1033–1044 |doi=10.14778/2536222.2536229 |url=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf |archive-url=https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf |dead-url=yes |archive-date=1 February 2016 |accessdate=4 August 2016 }}&lt;/ref&gt;&lt;ref name="Pointer2016"&gt;{{cite web|last1=Pointer|first1=Ian|title=Apache Beam wants to be uber-API for big data|url=http://www.infoworld.com/article/3056172/application-development/apache-beam-wants-to-be-uber-api-for-big-data.html|publisher=InfoWorld|accessdate=4 August 2016}}&lt;/ref&gt;

Google released an open SDK implementation of the Dataflow model in 2014 and an environment to execute Dataflows locally (non-distributed) as well as in the [[Google Cloud Platform]] service.

In 2016 Google donated the core SDK as well as the implementation of a local runner, and a set of IOs (data connectors) to access [[Google Cloud Platform]] data services to the [[Apache Software Foundation]]. Other companies and members of the community have contributed runners for existing distributed execution platforms, as well as new IOs to integrate the Beam Runners with existing Databases, Key-Value stores and Message systems. Additionally new [[Domain-specific language|DSLs]] have been proposed to support specific domain needs on top of the Beam Model.

===Timeline===

{| class="wikitable"
|-
! Version
! Release date
|-
| {{Version|c|2.4.0}}
| 2018-03-20
|-
| {{Version|co|2.3.0}}
| 2018-01-30
|-
| {{Version|co|2.2.0}}
| 2017-12-02
|-
| {{Version|o|2.1.0}}
| 2017-08-23
|-
| {{Version|o|2.0.0}}
| 2017-05-17
|-
| {{Version|o|0.6.0}}
| 2017-03-11
|-
| {{Version|o|0.5.0}}
| 2017-02-02
|-
| {{Version|o|0.4.0}}
| 2016-12-29
|-
| {{Version|o|0.3.0}}
| 2016-10-31
|-
| {{Version|o|0.2.0}}
| 2016-08-08
|-
| {{Version|o|0.1.0}}
| 2016-06-15
|-
| colspan="2" | &lt;small&gt;{{Version |l |show=111100}}&lt;/small&gt;
|}

==See also==
*[[List of Apache Software Foundation projects]]

==References==
{{reflist|30em}}

{{Apache}}

[[Category:Apache Software Foundation|Beam]]
[[Category:Apache Software Foundation projects|Beam]]
[[Category:Big data products]]
[[Category:Cluster computing]]
[[Category:Distributed stream processing]]
[[Category:Hadoop]]
[[Category:Java platform]]
[[Category:Free software programmed in Java (programming language)]]</text>
      <sha1>axmly7lg7cvfrzlo7y7cwulcvamlt5u</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Lucene</title>
    <ns>0</ns>
    <id>522923</id>
    <revision>
      <id>847713521</id>
      <parentid>846767425</parentid>
      <timestamp>2018-06-27T08:11:02Z</timestamp>
      <contributor>
        <username>Jpountz</username>
        <id>28586963</id>
      </contributor>
      <comment>Update latest release.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13446">{{refimprove|date=February 2012}}
{{Infobox software
| name = Lucene
| logo = Lucene logo green 300.png
| screenshot =
| caption =
| developer = [[Apache Software Foundation]]
| released = {{release date|1999}}
| latest release version = 7.4.0
| latest release date = {{release date|2018|06|27}}&lt;ref&gt;{{Cite web|url=https://lucene.apache.org/|title=Welcome to Apache Lucene|accessdate=21 December 2017|at=Lucene™ News section|deadurl=no|archiveurl=https://web.archive.org/web/20171221194344/http://lucene.apache.org/|archivedate=21 December 2017|df=}}&lt;/ref&gt;
| status = Active
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]
| license = [[Apache License]] 2.0
| website = {{URL|//lucene.apache.org}}
}}

'''Apache Lucene''' is a [[free and open-source software|free and open-source]] [[information retrieval]] [[Library (computing)|software library]], originally written completely in [[Java (programming language)|Java]] by [[Doug Cutting]]. It is supported by the [[Apache Software Foundation]] and is released under the [[Apache Software License]].

Lucene has been ported to other programming languages including [[Object Pascal]], [[Perl]], [[C Sharp (programming language)|C#]], [[C++]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]] and [[PHP]].&lt;ref name="port"&gt;{{cite web|url=http://wiki.apache.org/lucene-java/LuceneImplementations|title=LuceneImplementations|work=apache.org|accessdate=23 September 2015|deadurl=no|archiveurl=https://web.archive.org/web/20151006021755/http://wiki.apache.org/lucene-java/LuceneImplementations|archivedate=6 October 2015|df=}}&lt;/ref&gt;

==History==
[[Doug Cutting]] originally wrote Lucene in 1999.&lt;ref&gt;KeywordAnalyzer
{{cite web |url=http://trijug.org/downloads/TriJug-11-07.pdf |title=Better Search with Apache Lucene and Solr |date=19 November 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20120131154001/http://trijug.org/downloads/TriJug-11-07.pdf |archivedate=31 January 2012 |df= }}&lt;/ref&gt; It was initially available for download from its home at the [[SourceForge]] web site. It joined the Apache Software Foundation's [[Jakarta Project|Jakarta]] family of open-source Java products in September 2001 and became its own top-level Apache project in February 2005.  The name Lucene is Doug Cutting's wife's middle name and her maternal grandmother's first name.&lt;ref&gt;{{cite book |title=Web Content Management |last1= Barker |first1=Deane |authorlink= |coauthors= |year=2016 |publisher=O'Reilly  |location= |isbn=1491908106 |page=233 |pages= |url= |accessdate=}}&lt;/ref&gt;

Lucene formerly included a number of sub-projects, such as Lucene.NET, [[Apache Mahout|Mahout]], [[Apache Tika|Tika]] and [[Nutch]]. These three are now independent top-level projects.

In March 2010, the [[Apache Solr]] search server joined as a Lucene sub-project, merging the developer communities.

Version 4.0 was released on October 12, 2012.&lt;ref name="apache.org"&gt;{{cite web|url = https://lucene.apache.org/|title = Apache Lucene - Welcome to Apache Lucene|work = apache.org|accessdate = 4 February 2016|deadurl = no|archiveurl = https://web.archive.org/web/20160204002101/https://lucene.apache.org/|archivedate = 4 February 2016|df = }}&lt;/ref&gt;

==Features and common use==
While suitable for any application that requires full text [[Index (search engine)|indexing]] and searching capability, Lucene has been widely recognized&lt;ref&gt;{{cite book |title=Lucene in Action, Second Edition |last1= McCandless |first1=Michael |last2=Hatcher |first2=Erik |last3=Gospodnetić |first3=Otis |authorlink= |coauthors= |year=2010 |publisher=Manning  |location= |isbn=1933988177 |page=8 |pages= |url= |accessdate=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.glscube.org/downloads/glscube_design.pdf|title=GNU/Linux Semantic Storage System|author=|date=|website=glscube.org|deadurl=yes|archiveurl=https://web.archive.org/web/20100601210729/http://www.glscube.org/downloads/glscube_design.pdf|archivedate=2010-06-01|df=}}&lt;/ref&gt; for its utility in the implementation of [[Internet search engine]]s and local, single-site searching.

Lucene includes a feature to perform a fuzzy search based on [[Levenshtein distance|edit distance]].&lt;ref&gt;{{cite web|url=https://lucene.apache.org/core/2_9_4/queryparsersyntax.html#Fuzzy+Searches|title=Apache Lucene - Query Parser Syntax|author=|date=|website=lucene.apache.org|deadurl=no|archiveurl=https://web.archive.org/web/20170502011748/http://lucene.apache.org/core/2_9_4/queryparsersyntax.html#Fuzzy+Searches|archivedate=2017-05-02|df=}}&lt;/ref&gt;

Lucene has also been used to implement recommendation systems.&lt;ref&gt;J. Beel, S. Langer, and B. Gipp, “The Architecture and Datasets of Docear’s Research Paper Recommender System,” in Proceedings of the 3rd International Workshop on Mining Scientific Publications (WOSP 2014) at the ACM/IEEE Joint Conference on Digital Libraries (JCDL 2014), London, UK, 2014&lt;/ref&gt; For example, Lucene's 'MoreLikeThis' Class can generate recommendations for similar documents. In a comparison of the term vector-based similarity approach of 'MoreLikeThis' with citation-based document similarity measures, such as [[Co-citation]] and [[co-citation proximity|Co-citation Proximity Analysis]] Lucene's approach excelled at recommending documents with very similar structural characteristics and more narrow relatedness.&lt;ref name="Schwarzer16"&gt;M. Schwarzer, M. Schubotz, N. Meuschke, C. Breitinger, V. Markl, and B. Gipp, https://www.gipp.com/wp-content/papercite-data/pdf/schwarzer2016.pdf "Evaluating Link-based Recommendations for Wikipedia" in Proceedings of the 16th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL), New York, NY, USA, 2016, pp. 191-200.&lt;/ref&gt; In contrast, citation-based document similarity measures, tended to be more suitable for recommending more broadly related documents,&lt;ref name="Schwarzer16" /&gt; meaning citation-based approaches may be more suitable for generating [[Recommender system#Beyond accuracy|serendipitous]] recommendations, as long as documents to be recommended contain in-text citations.

At the core of Lucene's logical architecture is the idea of a document containing fields of text. This flexibility allows Lucene's {{clarify|text=API|date=October 2017}} to be independent of the [[file format]]. Text from [[Portable Document Format|PDFs]], [[HTML]], [[Microsoft Word]], [[Mind Map]]s, and [[OpenDocument]] documents, as well as many others (except images), can all be indexed as long as their textual information can be extracted.&lt;ref&gt;{{cite book |title=Machine Learning and Data Mining in Pattern Recognition: 5th International Conference |last=Perner |first=Petra |authorlink= |year=2007 |publisher=Springer |location= |isbn=978-3-540-73498-7 |page=387 |pages= |url= |accessdate=}}&lt;/ref&gt;

==Lucene-based projects==
Lucene itself is just an indexing and search library and does not contain [[web spider|crawling]] and HTML [[parsers|parsing]] functionality. However, several projects extend Lucene's capability:
* Apache [[Nutch]] &amp;mdash; provides [[web crawling]] and HTML parsing{{citation needed|date=June 2015}}
* [[Apache Solr]] &amp;mdash; an enterprise search server&lt;ref name="quora"&gt;{{cite web|url=https://www.quora.com/What-are-the-main-differences-between-ElasticSearch-Apache-Solr-and-SolrCloud|title=What are the main differences between ElasticSearch, Apache Solr and SolrCloud? - Quora|work=quora.com|accessdate=23 September 2015}}&lt;/ref&gt;
* [[Compass Project|Compass]] &amp;mdash; the predecessor to Elasticsearch&lt;ref&gt;{{Cite web|url=http://thedudeabides.com/articles/the_future_of_compass/|title=The Future of Compass &amp; Elasticsearch|website=the dude abides|language=en|accessdate=2015-10-14|deadurl=no|archiveurl=https://web.archive.org/web/20151015021211/http://thedudeabides.com/articles/the_future_of_compass/|archivedate=2015-10-15|df=}}&lt;/ref&gt;
* [[CrateDB]] &amp;mdash; open source, distributed SQL database built on Lucene &lt;ref&gt;{{cite news|url=http://www.infoworld.com/article/2984469/database/11-cutting-edge-databases-worth-exploring-now.html|title=11 cutting-edge databases worth exploring now|last=Wayner|first=Peter|accessdate=21 September 2015|publisher=InfoWorld|deadurl=no|archiveurl=https://web.archive.org/web/20150921214828/http://www.infoworld.com/article/2984469/database/11-cutting-edge-databases-worth-exploring-now.html|archivedate=21 September 2015|df=}}&lt;/ref&gt;
* [[DocFetcher]] &amp;mdash; a [[multiplatform]] desktop search application{{citation needed|date=June 2015}}
* [[Elasticsearch]] &amp;mdash; an enterprise search server&lt;ref name="quora" /&gt;&lt;ref&gt;{{cite web|url=https://www.elastic.co/products/elasticsearch|title=Elasticsearch: RESTful, Distributed Search &amp; Analytics  - Elastic|work=elastic.co|accessdate=23 September 2015|deadurl=no|archiveurl=http://archive.wikiwix.com/cache/20150923130337/https://www.elastic.co/products/elasticsearch|archivedate=23 September 2015|df=}}&lt;/ref&gt;
* Kinosearch &amp;mdash; a search engine written in [[Perl]] and [[C (programming language)|C]]&lt;ref name="cmswire"&gt;{{cite news|url=http://www.cmswire.com/cms/enterprise-20/socialtext-updates-search-goes-kino-001037.php|title=Socialtext Updates Search, Goes Kino|last=Natividad|first=Angela|accessdate=2011-05-31|publisher=CMS Wire|deadurl=no|archiveurl=https://web.archive.org/web/20120929122221/http://www.cmswire.com/cms/enterprise-20/socialtext-updates-search-goes-kino-001037.php|archivedate=2012-09-29|df=}}&lt;/ref&gt; and a loose [[Porting|port]] of Lucene.&lt;ref name="test"&gt;{{cite web|url=http://p3rl.org/KinoSearch#DESCRIPTION|title=KinoSearch - Search engine library. - metacpan.org|author=Marvin Humphrey|work=p3rl.org|accessdate=23 September 2015}}&lt;/ref&gt; The [[Socialtext]] wiki software uses this search engine,&lt;ref name="cmswire" /&gt; and so does the [[MojoMojo]] wiki.&lt;ref name="catbook"&gt;{{cite book|title=The Definitive Guide to Catalyst|last=Diment|first=Kieren|author2=Trout, Matt S|publisher=[[Apress]]|year=2009|isbn=978-1-4302-2365-8|page=280|chapter=Catalyst Cookbook}}&lt;/ref&gt; It is also used by the [[Human Metabolome Database]] (HMDB)&lt;ref&gt;{{cite journal|date=January 2009|title=HMDB: a knowledgebase for the human metabolome|journal=[[Nucleic Acids Res.]]|volume=37|issue=Database issue|pages=D603–10|doi=10.1093/nar/gkn810|pmc=2686599|pmid=18953024}}&lt;/ref&gt; and the [[Toxin and Toxin-Target Database]] (T3DB).&lt;ref&gt;{{cite journal|date=January 2010|title=T3DB: a comprehensively annotated database of common toxins and their targets|journal=Nucleic Acids Res.|volume=38|issue=Database issue|pages=D781–6|doi=10.1093/nar/gkp934|pmc=2808899|pmid=19897546}}&lt;/ref&gt;
* [[Swiftype]] &amp;mdash; an enterprise search startup based on Lucene&lt;ref&gt;{{Cite web|url = https://www.quora.com/What-is-the-technology-stack-behind-Swiftype|title = What is the technology stack behind Swiftype? - Quora|date = May 9, 2012|accessdate = 3 October 2014|website = Quora|publisher = |last = Riley|first = Matt}}&lt;/ref&gt;

==Users==

For a list of companies that use Lucene (rather than extend), see Lucene's "Powered By" page.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/lucene-java/PoweredBy|title=PoweredBy|work=apache.org|accessdate=23 September 2015|deadurl=no|archiveurl=https://web.archive.org/web/20150921145513/http://wiki.apache.org/lucene-java/PoweredBy|archivedate=21 September 2015|df=}}&lt;/ref&gt; As an example, [[Twitter]] is using Lucene for its real time search.&lt;ref name="twitter"&gt;{{cite web|url=https://techcrunch.com/2010/10/06/new-twitter-search/|title=Twitter Quietly Launched A New Search Backend Weeks Ago|author=MG Siegler|publisher=AOL|work=TechCrunch|accessdate=23 September 2015|deadurl=no|archiveurl=https://web.archive.org/web/20150925102734/http://techcrunch.com/2010/10/06/new-twitter-search/|archivedate=25 September 2015|df=}}&lt;/ref&gt;

== See also ==
{{Portal|Free software}}
* [[Enterprise search]]
* [[Information extraction]]
* [[List of information retrieval libraries]]
* [[Text mining]]

==References==
{{Reflist}}

==Bibliography==
* {{cite book |last=Gospodnetic |first=Otis |author2=Erik Hatcher |author3=Michael McCandless 
  |title=Lucene in Action |edition=2nd |date=28 June 2009 |publisher=[[Manning Publications]] |location=
  |isbn=1-9339-8817-7 |url=
}}
* {{cite book |last=Gospodnetic |first=Otis |author2=Erik Hatcher 
  |title=Lucene in Action |edition=1st |date=1 December 2004 |publisher=[[Manning Publications]] |location=
  |isbn=978-1-9323-9428-3 |url=
}}

==External links==
* {{Official website|//lucene.apache.org/}}
* [https://wiki.apache.org/lucene-java/LuceneImplementations List of Lucene Ports (or Implementations)] in Other Languages on the Apache wiki
* {{cite web|archive-date=2006-07-15 |archive-url=https://web.archive.org/web/20060715234923/http://schmidt.devlib.org/software/lucene-wikipedia.html |url=http://schmidt.devlib.org/software/lucene-wikipedia.html |quote=Introductory article with Java code for search |title=Lucene Wikipedia indexer |first=Marco |last=Schmidt |date=2005 |deadurl=yes }}
* [http://apiwave.com/java/api/org.apache.lucene Apache Lucene API tracker]

{{Apache}}
{{Authority control}}

[[Category:Apache Software Foundation|Lucene]]
[[Category:Free search engine software]]
[[Category:Java (programming language) libraries]]
[[Category:C Sharp libraries]]
[[Category:Cross-platform software]]
[[Category:Software using the Apache license]]
[[Category:Search engine software]]</text>
      <sha1>qlr8rqq842wtsaid2k6zv8aiebc7tqy</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Fortress</title>
    <ns>0</ns>
    <id>50237338</id>
    <revision>
      <id>801376189</id>
      <parentid>791632736</parentid>
      <timestamp>2017-09-19T09:02:31Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v456)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4096">{{Infobox Software
| name                   = Apache Fortress
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.0.0
| latest release date    = {{release date|2017|06|26}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Access Control]]
| license                = [[Apache License]] 2.0
| website                = http://directory.apache.org/fortress/
}}
'''Apache Fortress''' is an [[open source]] project of the [[Apache Software Foundation]] and a subproject of the [[Apache Directory]].  It's a standards-based access management system, written in Java, that provides role-based access control, delegated administration and password policy services with [[Lightweight Directory Access Protocol|LDAP]].

Standards implemented:
* [[Role-based access control|Role-Based Access Control]] (ANSI INCITS 359)
* Administrative Role-Based Access Control (ARBAC02)
* IETF [[Password policy|Password Policy]] (draft)
  
Fortress has four separate components:
* Core - Java Access Management SDK
* Realm - [[Web container|Web Container]] security for [[Apache Tomcat]]
* Rest - HTTP protocol wrappers for the APIs
* Web - HTML pages for the APIs

==History==
Fortress was first contributed in 2011 to the OpenLDAP Foundation&lt;ref&gt;{{Cite web |title=Request for Contribution of Identity Access Management Software to OpenLDAP Project |url=http://www.openldap.org/its/index.cgi/Contrib?id=6995;expression=fortress;statetype=-1#themesg
 |publisher=OpenLDAP Foundation |date=15 July 2011 |accessdate=20 April 2016 }}&lt;/ref&gt; and moved to the Apache Directory project in 2014.&lt;ref&gt;{{Cite web |title=Notice to donate Fortress to the Apache Directory Project |url=http://mail-archives.apache.org/mod_mbox/directory-dev/201409.mbox/%3C54181FD6.1060200%40att.net%3E
 |publisher=Apache Software Foundation |date=16 September 2014 |accessdate=20 April 2016 }}&lt;/ref&gt;

== Releases ==
{| class="wikitable"
!Version
!Date
|-
|2.0.0
|2017-06-26 (current release)
|-
|2.0.0-RC2
|2017-05-04
|-
|2.0.0-RC1
|2016-11-07
|-
|1.0.1
|2016-07-22
|-
|1.0.0
|2016-04-12
|-
|1.0-RC42
|2016-03-19
|-
|1.0-RC40
|2015-04-10
|}

== References ==
{{reflist}}

== External links ==
*[https://directory.apache.org/fortress/ Apache Fortress Project Page]
*[https://shawnmckinney.github.io/apache-fortress-demo/apidocs/overview-summary.html Apache Fortress Demo]
*[https://www.openhub.net/p/apache-fortress Apache Fortress on Open HUB]
*[http://directory.apache.org/ Apache Directory Server Project Page]
*[http://sdtimes.com/apache-eases-access-management-fortress-1-0/ Apache eases access management in Fortress 1.0]
*[https://www.linux.com/news/event/apachecon/2017/4/secure-web-apps-javaee-and-apache-fortress Secure Web Apps with JavaEE and Apache Fortress]
*[http://ramannanda.blogspot.com/search/label/Fortress Driving Security for ADF Essentials with Fortress]
*[http://www.profsandhu.com/confrnc/sacmat/02-oh-model.pdf A Model for Role Administration Using Organization Structure]
*[https://published-rs.lanyonevents.com/published/oracleus2015/sessionsFiles/3665/CON3568_Smith-Penn%20State%20-%20Federated%20RBAC%20Security%2026%20Oct.pptx Federated RBAC: Fortress, OAuth2 (Oltu), JWT, Java EE, and JASPIC]
*[https://www.youtube.com/watch?v=MWmR5Ynjoak&amp;feature=youtu.be Openstack needs RBAC with Apache Fortress and IDM using Midpoint]
*[https://feathercast.apache.org/2017/03/08/apachecon-seville-2016-how-i-built-an-access-management-system-using-java-and-apache-directory-fortress-shawn-mckinney/ How I Built an Access Management System Using Java and Apache Directory Fortress]
*[https://www.infoq.com/presentations/security-paas-cloud A How-to Guide to Security in the PAAS Cloud]

{{apache}}

[[Category:Apache Software Foundation|Fortress]]
[[Category:Directory services]]
[[Category:Access control]]

{{security-software-stub}}</text>
      <sha1>i9zuip52ueqpw7o23ccgc2wf79npngh</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Serf</title>
    <ns>0</ns>
    <id>50974924</id>
    <revision>
      <id>776816860</id>
      <parentid>728099277</parentid>
      <timestamp>2017-04-23T13:18:15Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="914">{{multiple issues|
{{primary sources|date=July 2016}}
{{orphan|date=July 2016}}
}}
'''Apache Serf''' is a high performance{{Prove it|reason=This statement needs non-primary sources for verification.|date=July 2016}} [[C (programming language)|C-based]] [[HTTP]] client library developed by [[Apache Software Foundation]]. It is built upon the [[Apache Portable Runtime]] (APR) library. It supports asynchronous connections, [[Transport Layer Security|SSL]] support, and full [[HTTP pipelining]].&lt;ref&gt;{{cite web|url=http://serf.apache.org|title=Apache Serf|website=Apache Software Foundation|access-date=July 2, 2016}}&lt;/ref&gt; It allows [[Apache Subversion]] to send HTTP requests. Releases are currently only available on [https://archive.apache.org archive.apache.org].

==References==
{{reflist}}

[[Category:Apache Software Foundation|Serf]]
[[Category:Software using the Apache license]]


{{free-software-stub}}</text>
      <sha1>f46qpr33b9al439pr4yvc29t2g66drt</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Yetus</title>
    <ns>0</ns>
    <id>50611947</id>
    <revision>
      <id>824090582</id>
      <parentid>824088816</parentid>
      <timestamp>2018-02-05T07:13:22Z</timestamp>
      <contributor>
        <ip>108.193.1.249</ip>
      </contributor>
      <comment>Fill in some data</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2280">{{Multiple issues|
{{unreferenced|date=May 2016}}
}}

&lt;!-- Don't mess with this line! --&gt;&lt;!-- Write your article below this line --&gt;
{{ Infobox Software
| name                   = Apache Yetus
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 0.7.0&lt;ref&gt;{{Cite web|url=https://yetus.apache.org/downloads|title=Apache Yetus|accessdate=4 February 2018}}&lt;/ref&gt;
| latest release date    = {{Release date| 2018 | 01 | 27}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Bash (Unix shell)|Bash]] [[Java (programming language)|Java]] [[Python (programming language)|Python]]
| genre                  = [[Software build|Build management]]
| license                = [[Apache License]] 2.0
| website                = https://yetus.apache.org/
}}

'''Apache Yetus''' is a collection of libraries and tools that enable contribution and release processes for software projects.&lt;ref&gt;{{Cite web|url=https://yetus.apache.org|title=Apache Yetus|accessdate=4 February 2018}}&lt;/ref&gt; Portions are used by a wide variety of Apache projects, including [[Apache Hadoop]] and [[Apache HBase]].&lt;ref&gt;{{Cite web|url=https://wiki.apache.org/incubator/YetusProposal|title=YetusProposal|accessdate=4 February 2018}}&lt;/ref&gt;

It consists of the following components:
* ''Precommit'' - Module-based [[Patch (computing)|patch]] and full build testing
* ''Release Doc Maker '' - Generates [[Markdown]] formatted [[Changelog|changelog]] and [[Release notes|release notes]] from [[Jira (software)|JIRA]]
* ''Shelldocs'' - Markdown formatted documentation for [[Bash (Unix shell)|Bash]] functions
* ''Audience Annotations'' - [[Java annotation|Java annotations]] that provide stability and audience information for [[Application programming interface|APIs]]

==References==
{{reflist}}

==External links==
*[http://yetus.apache.org/ Yetus  project home page]
*[https://lists.apache.org/list.html?dev@yetus.apache.org Yetus mailing list archives]

{{Apache}}

[[Category:Java (programming language) libraries]]
[[Category:Apache Software Foundation|Yetus]]


{{network-software-stub}}</text>
      <sha1>stqowm3amrged3akma0fp7fenuhdf56</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Samza</title>
    <ns>0</ns>
    <id>42672986</id>
    <revision>
      <id>805329649</id>
      <parentid>776817125</parentid>
      <timestamp>2017-10-14T17:14:09Z</timestamp>
      <contributor>
        <username>Fintler</username>
        <id>527595</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2153">{{About-distinguish|Apache Samza|Samba (software)}}
{{refimprove|date=January 2017}}
{{Infobox software
| name                   = Apache Samza
| logo                   = File:Samza Logo.png
| caption                = Logo of Samza
| developer              = Apache Software Foundation
| latest release version = 0.13.1
| latest release date    = {{Start date and age|2017|08|25|df=yes}}
| programming language   = [[Scala (programming language)|Scala]], [[Java (programming language)|Java]]
| operating system       = [[Cross-platform]]
| status                 = Active
| genre                  = Distributed [[stream processing]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://samza.apache.org/}}
}}

'''Apache Samza''' is an [[open source|open-source]] near-realtime, asynchronous computational framework for stream processing developed by the [[Apache Software Foundation]] in [[Scala (programming language)|Scala]] and [[Java (programming language)|Java]].

==History==
Apache Samza has been developed in conjunction with [[Apache Kafka]]. Both were originally developed by [[LinkedIn]].&lt;ref&gt;{{Cite web|url=https://www.infoq.com/articles/linkedin-samza|title=How LinkedIn Uses Apache Samza|website=InfoQ|access-date=2016-09-28}}&lt;/ref&gt;

==See also==
* [[Apache Kafka]]
* [[Storm (event processor)]]
* [[Apache Spark]]
* [[Apache Hadoop]]
* [[Druid (open-source data store)]]
* [[List of Apache Software Foundation projects]]

== References ==
{{Reflist}}

==External links==
* [https://samza.apache.org/ Apache Samza website]
* [http://thenewstack.io/apache-samza-linkedins-framework-for-stream-processing LinkedIn’s Framework for Stream Processing]
{{apache}}

[[Category:Apache Software Foundation|Samza]]
[[Category:Apache Software Foundation projects|Samza]]
[[Category:Java platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free software programmed in Scala]]
[[Category:Software using the Apache license]]
[[Category:Free software]]
[[Category:Distributed stream processing]]
[[Category:Distributed computing architecture]]
[[Category:Parallel computing]]</text>
      <sha1>52jxf0dl1c29tnjguc2p93g6cd4w4or</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Tika</title>
    <ns>0</ns>
    <id>50189796</id>
    <revision>
      <id>818061413</id>
      <parentid>809562348</parentid>
      <timestamp>2018-01-01T08:12:48Z</timestamp>
      <contributor>
        <username>4serendipity</username>
        <id>16605</id>
      </contributor>
      <minor/>
      <comment>1.17 released</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5347">{{Infobox software
| name = Tika
| logo = [[File:Apache Tika.png|130px|Tika logo]]
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| latest release version = 1.17
| latest release date = {{release date and age|2017|12|13}}
| latest preview version = 
| latest preview date = 
| status = Active
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| platform = 
| genre = [[Search algorithm|Search]] and [[index (search engine)|index]] [[Application programming interface|API]]
| license = [[Apache License]] 2.0
| website = {{URL|http://tika.apache.org/}}
}}

'''Apache Tika''' is a content detection and [[content analysis|analysis]] framework, written in [[Java (programming language)|Java]], stewarded at the [[Apache Software Foundation]].&lt;ref&gt;{{Cite web|url=http://tika.apache.org/|title=Apache Tika|access-date=2016-04-15}}&lt;/ref&gt; It detects and extracts metadata and text from over a thousand different [[file type]]s, and as well as providing a 
[[Java (programming language)|Java]] library, has server and command-line editions suitable for use from other programming languages.

== History ==
The project originated as part of the [[Apache Nutch]] codebase, to provide content identification and extraction when [[Web crawlers|crawling]]. In 2007, it was separated out, to make it more extensible and usable by [[content management systems]], other [[Web crawlers]], and information retrieval systems. The standalone Tika was founded by Jérôme Charron, [[Chris Mattmann]] and Jukka Zitting.&lt;ref&gt;{{Cite web|url=http://wiki.apache.org/incubator/TikaProposal|title=Tika Proposal|access-date=2016-04-15}}&lt;/ref&gt; In 2011 Chris Mattmann and Jukka Zitting released the Manning book "Tika in Action", and the project released version 1.0.

== Features ==
Tika provides capabilities for identification of more than 1400 file types from the [[Internet Assigned Numbers Authority]] taxonomy of [[MIME]] types. For most of the more common and popular formats,&lt;ref&gt;{{cite web|url=http://tika.apache.org/1.12/formats.html| title= The Apache Software Foundation| website=Apache Tika formats page|accessdate=16 April 2016}}&lt;/ref&gt; Tika then provides content extraction, metadata extraction and language identification capabilities.

While Tika is written in [[Java (programming language)|Java]], it is widely used from other languages.&lt;ref&gt;{{Cite web|url=https://wiki.apache.org/tika/API%20Bindings%20for%20Tika|title=API Bindings for Tika|last=|first=|date=|website=|publisher=Apache Tika|access-date=2016-04-17}}&lt;/ref&gt; The [[Representational state transfer|RESTful]] server and [[Command-line interface|CLI Tool]] permit non-Java programs to access the Tika functionality.

== Notable uses ==
Tika is used by financial institutions including the [[Fair Isaac Corporation]] (FICO),&lt;ref&gt;{{Cite web|url=http://www.fico.com/en/newsroom/fico-to-engage-kaggles-community-of-180000-data-scientists-to-drive-innovation-in-the-fico-analytic-cloud|title=FICO to Engage Kaggle's Community of 180,000 Data Scientists to Drive Innovation in the FICO Analytic Cloud {{!}} FICO®|website=FICO® {{!}} Decisions|access-date=2016-04-15}}&lt;/ref&gt; Goldman Sachs,&lt;ref&gt;{{Cite news|url=http://www.informationweek.com/software/enterprise-applications/goldman-sachs-puts-elasticsearch-to-work/d/d-id/1321778|title=Goldman Sachs Puts Elasticsearch To Work - InformationWeek|work=InformationWeek|access-date=2017-06-21|language=en}}&lt;/ref&gt; [[NASA]] and academic researchers&lt;ref&gt;{{Cite web|url=https://opensource.com/life/15/4/interview-annie-burgess-USC-JPL|title=Studying polar data with the help of Apache Tika|website=Opensource.com|access-date=2016-04-15}}&lt;/ref&gt; and by major content management systems including [[Drupal]],&lt;ref&gt;{{Cite web|url=https://www.drupal.org/project/text_extract|title=Text Extract for Drupal using Tika {{!}} Drupal.org|website=www.drupal.org|access-date=2016-04-15}}&lt;/ref&gt; and [[Alfresco (software)]]&lt;ref&gt;{{Cite web|url=https://wiki.alfresco.com/wiki/Content_Transformation_and_Metadata_Extraction_with_Apache_Tika|title=Content Transformation and Metadata Extraction with Apache Tika - alfrescowiki|website=wiki.alfresco.com|access-date=2016-04-15}}&lt;/ref&gt; to analyze large amounts of content, and to make it available in common formats using information retrieval techniques.

On April 4, 2016&lt;ref&gt;{{Cite web|url=https://www.forbes.com/sites/thomasbrewster/2016/04/05/panama-papers-amazon-encryption-epic-leak|title=From Encrypted Drives To Amazon's Cloud -- The Amazing Flight Of The Panama Papers|last=Fox-Brewster|first=Thomas|website=Forbes|access-date=2016-04-15}}&lt;/ref&gt; [[Forbes]] published an article identifying Tika as one of the key technologies used by more than 400 journalists to analyze 11.5 million leaked documents that expose an international scandal involving world leaders storing money in offshore [[shell corporation]]s. The leaked documents and the project to analyze them is referred to as the [[Panama Papers]].

==See also==
*[[Magic number (programming)|Magic number]]

==References==
{{Reflist}}

{{Apache}}

[[Category:Apache Software Foundation|Tika]]
[[Category:Java platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) libraries]]
[[Category:Software using the Apache license]]</text>
      <sha1>a38c8oobgngf5373zqnm7meb9ezwp1q</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Kudu</title>
    <ns>0</ns>
    <id>54098260</id>
    <revision>
      <id>843309713</id>
      <parentid>821661828</parentid>
      <timestamp>2018-05-28T08:41:17Z</timestamp>
      <contributor>
        <username>Nsaa</username>
        <id>113357</id>
      </contributor>
      <comment>Version 1.7</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3817">{{Infobox Software
| name = Apache Kudu
| logo = 
| screenshot =
| caption = Apache Kudu
| developer = 
| status = Active
| latest release version = 1.7.0
| latest release date = {{start date and age|2018|03|23|df=yes}}&lt;ref&gt;{{cite web|title=Apache Kudu - Releases|url=https://kudu.apache.org/releases/|access-date=28 May 2018|quote=Kudu 1.7.0 was released on March 23, 2018.|archive-url=https://web.archive.org/web/20180528083726/https://kudu.apache.org/releases/ |archive-date=28 May 2018 |dead-url=no}}&lt;/ref&gt;
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language =
| genre = [[Database management system]]
| license = [[Apache License]] 2.0 &lt;ref name="kudufaqprojectstatus"&gt;{{Cite web|url=https://kudu.apache.org/faq.html#project-status|title=
Project Status|date=2017-05-21|language=en-US|access-date=2017-05-21|archive-url=https://web.archive.org/web/20170521110002/https://kudu.apache.org/faq.html|archive-date=2017-05-21|dead-url=no|quote=Is Kudu open source? Yes, Kudu is open source and licensed under the Apache Software License, version 2.0. Apache Kudu is a top level project (TLP) under the umbrella of the Apache Software Foundation.}}&lt;/ref&gt;
}}

'''Apache Kudu''' is a [[free and open source]] [[Column-oriented DBMS|column-oriented]] data store of the [[Apache Hadoop]] ecosystem. It is compatible with most of the data processing frameworks in the [[Hadoop]] environment. It provides a completes Hadoop's storage layer to enable fast analytics on fast data.&lt;ref&gt;https://kudu.apache.org/&lt;/ref&gt;

The [[open source]] project to build Apache Kudu began as internal project at [[Cloudera]].&lt;ref&gt;{{Cite web|url=https://kudu.apache.org/faq.html#project-status|title=Why was Kudu developed internally at Cloudera before its release?|date=2017-05-21|language=en-US|access-date=2017-05-21}}&lt;/ref&gt; The first version Apache Kudu 1.0 was released 19 September 2016.&lt;ref&gt;{{Cite web|url=https://kudu.apache.org/releases/|title=
Apache Kudu releases|date=2017-05-21|language=en-US|access-date=2017-05-21|archive-url=https://web.archive.org/web/20170521105214/https://kudu.apache.org/releases/|archive-date=2017-05-21|dead-url=no|quote=Kudu 1.0.0 was released on September 19, 2016. It is the first release not considered “beta”. […] Kudu 0.5.0 (beta) was released on Sep 28, 2015. It was the first public version of Kudu.}}&lt;/ref&gt;

== Comparison with other storage engines ==

According to the project Kudu shares some  characteristics with [[HBase]]. Kudu supports "real-time store that supports key-indexed record lookup and mutation." as HBase.&lt;ref name="kudufaqprojectmotivation"&gt;{{Cite web|url=https://kudu.apache.org/faq.html#project-motivation|title=
Why build a new storage engine? Why not just improve Apache HBase to increase its scan speed?|date=2017-05-21|language=en-US|access-date=2017-05-21|archive-url=https://web.archive.org/web/20170521110002/https://kudu.apache.org/faq.html|archive-date=2017-05-21|dead-url=no|quote=}}&lt;/ref&gt; Kudu differ from HBase since Kudu's datamodel is more traditional relational, while HBase is schemaless. Kudu's "on-disk representation is truly columnar and follows an entirely different storage design than HBase/[[Bigtable]]".&lt;ref name="kudufaqprojectmotivation"/&gt;

==See also==
{{Portal|Free software}}
* [[Pig (programming tool)]]
* [[Apache Hive]]
* [[Cloudera Impala]]
* [[Apache Parquet]]
* [[Apache Drill]]
* [[Apache Spark]]
* [[Apache Thrift]]
* [[ClickHouse]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website|//kudu.apache.org/}}

{{Apache}}

{{DEFAULTSORT:Apache Kudu}}
[[Category:2016 software]]
[[Category:Apache Software Foundation|Kudu]]
[[Category:Cloud computing]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>rqhpgghilteej7jifrto60zyctzq0xh</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Ignite</title>
    <ns>0</ns>
    <id>55461969</id>
    <revision>
      <id>838364485</id>
      <parentid>832730807</parentid>
      <timestamp>2018-04-26T15:11:09Z</timestamp>
      <contributor>
        <ip>195.201.139.10</ip>
      </contributor>
      <comment>Updated latest versions number and release date</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13493">{{Infobox software
| name = Apache Ignite
| logo = Apache_Ignite_logo.svg
| author = [[GridGain Systems]]
| developer = [[Apache Software Foundation]]
| released = {{Start date and age|2015|03|24|df=yes}}
| latest release version = 2.4
| latest release date = {{Start date and age|2018|03|12|df=yes}}
| repo = {{URL|https://github.com/apache/ignite}}
| status = Active
| programming language = [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[C++]]
| operating system = [[Cross-platform]]
| platform = [[IA-32]], [[x86-64]], [[PowerPC]], [[Sparc]], [[Java platform]], [[.NET Framework]]
| genre = [[Database]], [[computing platform]]
| license = [[Apache License 2.0]]
| website = {{URL|https://ignite.apache.org}}
}}
   
'''Apache Ignite''' is an open-source distributed database, caching and processing platform designed to store and compute on large volumes of data across a cluster of nodes.&lt;ref&gt;{{Cite web|url=https://dzone.com/articles/what-is-apache-ignite-1|title=What Is Apache Ignite? - DZone Big Data|website=dzone.com|language=en|access-date=2017-11-02}}&lt;/ref&gt;

Ignite was open-sourced by [[GridGain Systems]] in late 2014 and accepted in the [[Apache Incubator]] program that same year.&lt;ref&gt;{{Cite web|url=https://www.infoq.com/news/2017/04/nikita-ivanov-apache-ignite|title=Nikita Ivanov on Apache Ignite In-Memory Computing Platform|website=InfoQ|access-date=2017-11-02}}&lt;/ref&gt;&lt;ref name="Ignite Status - Apache Incubator"&gt;{{Cite web|url=https://incubator.apache.org/projects/ignite.html|title=Ignite Status - Apache Incubator|website=incubator.apache.org|access-date=2017-11-02}}&lt;/ref&gt; The Ignite project graduated on September 18, 2015.&lt;ref name="Ignite Status - Apache Incubator"/&gt;

Apache Ignite's database utilizes [[RAM]] as the default storage and processing tier, thus, belonging to the class of [[in-memory computing]] platforms.&lt;ref&gt;{{Cite web|url=https://www.infoq.com/news/2017/04/nikita-ivanov-apache-ignite|title=Nikita Ivanov on Apache Ignite In-Memory Computing Platform|website=InfoQ|access-date=2017-10-11}}&lt;/ref&gt; The disk tier is optional but, once enabled, will hold the full data set whereas the memory tier&lt;ref name=":0"&gt;{{Cite web|url=https://dzone.com/articles/demystifying-the-apache-ignite-native-persistence|title=Apache Ignite Native Persistence, a Brief Overview - DZone Big Data|website=dzone.com|language=en|access-date=2017-10-11}}&lt;/ref&gt; will cache full or partial data set depending on its capacity.

Regardless of the API used, data in Ignite is stored in the form of key-value pairs. The database component scales horizontally, distributing key-value pairs across the cluster in such a way that every node owns a portion of the overall data set. Data is rebalanced automatically whenever a node is added to or removed from the cluster.

On top of its distributed foundation, Apache Ignite supports a variety of APIs including JCache-compliant key-value APIs,  [[SQL:1999|ANSI-99]] [[SQL]] with joins, [[ACID]] transactions, as well as [[MapReduce]] like computations.

Apache Ignite cluster can be deployed on-premise on a commodity hardware, in the cloud (e.g. [[Microsoft Azure]], [[AWS]], [[Google Compute Engine]]) or in a containerized and provisioning environments such as [[Kubernetes]], [[Docker (software)|Docker]], [[Apache Mesos]], [[VMWare]].&lt;ref&gt;{{Cite web|url=https://dzone.com/articles/deploying-apache-ignite-in-kubernetes-on-microsoft|title=Deploying Apache Ignite in Kubernetes on Microsoft Azure - DZone Cloud|website=dzone.com|language=en|access-date=2017-10-11}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://aws.amazon.com/blogs/big-data/real-time-in-memory-oltp-and-analytics-with-apache-ignite-on-aws/|title=Real-time in-memory OLTP and Analytics with Apache Ignite on AWS {{!}} Amazon Web Services|date=2016-05-14|work=Amazon Web Services|access-date=2017-10-11|language=en-US}}&lt;/ref&gt;

== Clustering ==
Apache Ignite clustering component is based on the [[shared nothing architecture]]. The nodes are divided into two main categories - server and client. Server nodes are storage and computational units of the cluster that hold both data and indexes and process incoming requests along with computations. Server nodes are also known as data nodes.&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/clients-vs-servers|title=Clients and Servers|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt;

Client nodes are connection points from applications and services to the distributed database represented as a cluster of server nodes. Client nodes are usually embedded in the application code written in [[Java]], [[C Sharp (programming language)|C#]] or [[C++]] that have special libraries developed.

Furthermore, Apache Ignite provides [[ODBC]],&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/odbc-driver|title=ODBC Driver|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt; [[JDBC]]&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/jdbc-driver|title=JDBC Driver|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt; and [[REST]] drivers as a way to work with the database from other programming languages or tools. The drivers utilize either client nodes or low-level socket connections internally in order to communicate to the cluster.

== Partitioning and replication ==
Ignite database organizes data in the form of key-value pairs in distributed "caches" (the cache notion is used for historical reasons because initially, the database supported the memory tier). Generally, each cache represents one entity type such as an employee or organization.

Every cache is split into a fixed set of "partitions" that are evenly distributed among cluster nodes using the [[rendezvous hashing]] algorithm. There is always one primary and zero or more backup copies of a partition. The number of copies is configured with a replication factor parameter.&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/primary-and-backup-copies|title=Primary &amp; Backup Copies|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt; If the full replication mode is configured, then every cluster node will store a partition's copy. The partitions are rebalanced&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/rebalancing|title=Data Rebalancing|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt; automatically if a node is added to or removed from the cluster in order to achieve an even data distribution and spread the workload.

The key-value pairs are kept in the partitions. Apache Ignite maps a pair to a partition by taking the key's value and passing it to a special [[hash function]].

== Memory architecture ==
The memory architecture in Apache Ignite consists of two storage tiers and is called "durable memory". Internally, it uses [[paging]] for memory space management and data reference,&lt;ref&gt;{{Cite web|url=https://blogs.apache.org/ignite/entry/apache-ignite-2-0-redesigned|title=Apache Ignite 2.0: Redesigned Off-heap Memory, DDL and Machine Learning : Apache Ignite|website=blogs.apache.org|access-date=2017-10-11}}&lt;/ref&gt; similar to the [[virtual memory]] of systems like [[Unix]]. However, one significant difference between the durable and virtual memory architectures is that the former always keeps the whole data set with indexes on disk (assuming that the disk tier is enabled), while the virtual memory uses the disk when it runs out of RAM, for swapping purposes only.

The first tier of the memory architecture, memory tier, keeps data and indexes in [[RAM]] out of Java heap in so-called "off-heap regions". The regions are preallocated and managed by the database on its own which prevents Java heap utilization for storage needs, as a result, helping to avoid long garbage collection pauses. The regions are split into [[Page (computer memory)|pages]] of fixed size that store data, indexes, and system metadata.&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/memory-architecture|title=Memory Architecture|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt;
 
Apache Ignite is fully operational from the memory tier but it is always possible to use the second tier, disk tier, for the sake of [[Durability (database systems)|durability]]. The database comes with its own native persistence and, plus, can use [[RDBMS]], [[NoSQL]] or [[Hadoop]] databases as its disk tier.

=== Native persistence ===
Apache Ignite native persistence is a distributed and strongly consistent disk store that always holds a superset of data and indexes on disk. The memory tier &lt;ref name=":0"/&gt; will only cache as much data as it can depending on its capacity. For example, if there are 1000 entries and the memory tier can fit only 300 of them, then all 1000 will be stored on disk and only 300 will be cached in RAM.

Persistence uses the [[write-ahead logging]] (WAL) technique for keeping immediate data modifications on disk.&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/distributed-persistent-store#section-write-ahead-log|title=Ignite Persistence|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt; In the background, the store runs the "checkpointing process" which purpose is to copy dirty pages from the memory tier to the partition files. A dirty page is a page that is modified in memory with the modification recorded in WAL but not written to a respective partition file. The checkpointing allows removing outdated WAL segments over the time and reduces cluster restart time replaying only that part of WAL that has not been applied to the partition files.&lt;ref&gt;{{Cite web|url=https://apacheignite.readme.io/docs/distributed-persistent-store#section-checkpointing|title=Ignite Persistence|website=apacheignite.readme.io|access-date=2017-10-11}}&lt;/ref&gt;

=== Third-party persistence ===
The native persistence became available starting version 2.1.&lt;ref&gt;{{Cite web|url=https://blogs.apache.org/ignite/entry/apache-ignite-2-1-a|title=Apache Ignite 2.1 - A Leap from In-Memory to Memory-Centric Architecture : Apache Ignite|website=blogs.apache.org|access-date=2017-10-11}}&lt;/ref&gt; Before that Apache Ignite supported only third party databases as its disk tier.

Apache Ignite can be configured as the in-memory tier on top of [[RDBMS]], [[NoSQL]] or [[Hadoop]] databases speeding up the latter.&lt;ref&gt;{{Cite web|url=https://dzone.com/articles/apache-ignite-for-database-caching-1|title=Apache Ignite for Database Caching - DZone Database|website=dzone.com|language=en|access-date=2017-10-11}}&lt;/ref&gt; However, there are some limitations in comparison to the native persistence. For instance, SQL queries will be executed only on the data that is in RAM, thus, requiring to preload all the data set from disk to memory beforehand.

=== Swap Space ===
When using pure memory storage, it is possible for the data size to exceed the physical RAM size, leading to OOMEs. To avoid this, the ideal approach would be to enable Ignite native persistence or use 3rd party persistence. However, if you do not want to use native or 3rd party persistence, you can enable swapping, in which case, Ignite in-memory data will be moved to the swap space located on disk. Please know that Ignite does not provide its own implementation of swap space. Instead, it takes advantage of the swapping functionality provided by the operating system (OS). When swap space is enabled, Ignites stores data in memory mapped files (MMF) whose content will be swapped to disk by the OS depending on the current RAM consumption

== Consistency ==
Apache Ignite is a [[Strong consistency|strongly consistent]] platform that implements [[two-phase commit protocol]].&lt;ref&gt;{{Cite web|url=http://gridgain.blogspot.com//2014/09/two-ph%E2%80%A6|title=Distributed Thoughts|access-date=2017-10-11}}&lt;/ref&gt; The consistency guarantees are met for both memory and disk tiers. Transactions in Apache Ignite are [[ACID|ACID-compliant]] and can span multiple cluster nodes and caches. The database supports pessimistic and optimistic concurrency modes, [[deadlock]]-free transactions and deadlock detection techniques.

In the scenarios where transactional guarantees are optional, Apache Ignite allows executing queries in the atomic mode that provides better performance.

== Distributed SQL ==
Apache Ignite can be accessed using SQL APIs exposed via JDBC and ODBC drivers, and native libraries developed for [[Java]], [[C Sharp (programming language)|C#]], [[C++]] programming languages. Both [[Data manipulation language|data manipulation]] and [[Data definition language|data definition]] languages' syntax complies with [[SQL:1999|ANSI-99]] specification.

Being a distributed database, Apache Ignite supports both distributed collocated and non-collocated [[Join (SQL)|joins]].&lt;ref&gt;{{Cite web|url=https://dzone.com/articles/big-change-in-apache-ignite-17-welcome-the-non-col|title=Apache Ignite 1.7: Welcome Non-Collocated Distributed Joins! - DZone Database|website=dzone.com|language=en|access-date=2017-10-11}}&lt;/ref&gt; When the data is collocated, joins are executed on the local data of cluster nodes avoiding data movement across the network. Non-collocated joins might move the data sets around the network in order to prepare a consistent result set.

== References ==
&lt;!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. --&gt;
{{reflist}}

[[Category:Free and open-source software]]
[[Category:Apache Software Foundation]]</text>
      <sha1>rqf8hnrhcmvj3lxwbu9aq4b0r8rpg2g</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Karaf</title>
    <ns>0</ns>
    <id>42473191</id>
    <revision>
      <id>838657475</id>
      <parentid>838656458</parentid>
      <timestamp>2018-04-28T12:38:52Z</timestamp>
      <contributor>
        <ip>89.244.77.213</ip>
      </contributor>
      <comment>source added for OSGi release version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2783">{{notability|date=December 2017}}
{{Infobox software
| name                   = Apache Karaf
| logo                   = 
| screenshot             = [[File:Apache Karaf.png|250px]]
| caption                = Apache Karaf logo
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 4.2.0
| latest release date    = {{release date|2018|04|09}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| size                   = 27 MB (archived)
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[OSGi|OSGi Container]]
| license                = [[Apache License|Apache 2.0 Licence]]
| website                = {{url|http://karaf.apache.org/}}
}}

'''Apache Karaf''' is a modular open source [[OSGi]] (Release 6&lt;ref name="Supported OSGi Release"&gt;{{cite web|title=Supported OSGi Version from Project page|url=http://karaf.apache.org/download.html|accessdate=2018-04-28}}&lt;/ref&gt;) runtime environment.&lt;ref name="OSGi in trenches"&gt;{{cite web|work=Jamie Goodyear|title=Apache Karaf in the Trenches|url=https://prezi.com/p8hwghvterdp/apache-karaf-in-the-trenches/|accessdate=2013-04-22}}&lt;/ref&gt; The project became a top level project on 2010, previously being a subproject of [[Apache ServiceMix]].&lt;ref name="ASF Apache Karaf TLP announcement"&gt;{{cite web|work=Charles Moulliard|title=Karaf - Top Level Project|url=https://mail-archives.apache.org/mod_mbox/karaf-dev/201006.mbox/%3CAANLkTim6K8elVhqoEYyvKpzHJ-K8I0tyTI6v4Q6l90oz%40mail.gmail.com%3E|accessdate=2010-06-25}}&lt;/ref&gt;

Apache Karaf can work on top of any of the two most used OSGi frameworks: [[Apache Felix]] or [[Equinox (OSGi)]], providing additional features &lt;ref name="stakoverflow-what_is_Karaf"&gt;{{cite web|work=Matthew Murdoch|title=OSGi: What are the differences between Apache Felix and Apache Karaf?|url=https://stackoverflow.com/a/1613274|accessdate=2016-05-25}}&lt;/ref&gt;.

==Components==
* '''Karaf Container'''
* '''Karaf Cellar'''
* '''Karaf Cave'''
* '''Karaf Decanter'''

==References==
{{Reflist}}

==External links==
{{Commons category|Apache Karaf}}
* [http://karaf.apache.org/ Official Project (link)]
* [https://cwiki.apache.org/confluence/display/KARAF/Index Official Wiki]
* [http://karaf.apache.org/manual/latest/ Official Documentation (latest)]

{{Apache}}
{{Web server software}}

[[Category:Apache Software Foundation|Karaf]]
[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]
[[Category:Software using the Apache license]]
[[Category:Web server software programmed in Java]]


{{software-stub}}</text>
      <sha1>e21gkgf3xjx61u4961vtt04q5782kfr</sha1>
    </revision>
  </page>
  <page>
    <title>Apache License</title>
    <ns>0</ns>
    <id>145908</id>
    <revision>
      <id>844718362</id>
      <parentid>844717789</parentid>
      <timestamp>2018-06-06T17:37:49Z</timestamp>
      <contributor>
        <username>Bruce1ee</username>
        <id>211905</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/71.213.156.210|71.213.156.210]] ([[User talk:71.213.156.210|talk]]) to last version by The 6th Floor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11156">{{Use dmy dates|date=January 2015}}
{{Infobox software license
| name            = Apache License
| image           = [[Image:Apache_Software_Foundation_Logo_(2016).svg|250px]]
| caption         = The Apache Software Foundation logo
| author          = [[Apache Software Foundation]]
| version         = 2.0
| copyright       = Apache Software Foundation
| date            = January 2004
| OSI approved    = Yes&lt;ref name="osi"&gt;{{cite web
 | title = OSI-approved licenses by name
 | url = http://www.opensource.org/licenses/alphabetical
 | publisher = [[Open Source Initiative]]
 | accessdate =31 March 2011
| archiveurl= https://web.archive.org/web/20110428141712/http://opensource.org/licenses/alphabetical| archivedate= 28 April 2011 | deadurl= no}}&lt;/ref&gt;
| Debian approved = Yes&lt;ref name="dfsglist"&gt;{{cite web
 |url=https://wiki.debian.org/DFSGLicenses#The_Apache_Software_License_.28ASL.29
 |title=The Apache Software License (ASL)
 |work=The Big DFSG-compatible Licenses
 |publisher=[[Debian Project]]
 |accessdate=6 July 2009
}}&lt;/ref&gt;
| Free Software   = Yes&lt;ref name="fsflist"&gt;{{cite web
 |url=https://www.gnu.org/licenses/license-list.html#apache2
 |title=Apache License, Version 2.0
 |work=Various Licenses and Comments about Them
 |publisher=[[Free Software Foundation]]
 |accessdate=6 July 2009
| archiveurl= https://web.archive.org/web/20090716201618/https://www.gnu.org/licenses/license-list.html| archivedate= 16 July 2009 | deadurl= no}}&lt;/ref&gt;
| copyfree = No&lt;ref&gt;{{cite web | url=http://copyfree.org/rejected/ | title=Copyfree Licenses | accessdate=16 November 2011}}&lt;/ref&gt;
| GPL compatible  = Yes – version 2.0 is compatible with [[GPL v3]],&lt;ref name="fsflist" /&gt; but 1.0 &amp; 1.1 are incompatible.&lt;ref&gt;{{cite web | url=https://www.gnu.org/licenses/license-list.html | title=GNU License List | accessdate=1 October 2013}}&lt;/ref&gt;
| copyleft        = no &lt;!-- see [[Copyleft]] --&gt;
| linking         = yes
| website         = {{URL|https://www.apache.org/licenses}}
}}

The '''Apache License''' is a [[Permissive free software licence|permissive]] [[free software license]] written by the [[Apache Software Foundation]] (ASF).&lt;ref name="nmr-permissive"&gt;{{cite web|url=http://www.newmediarights.org/open_source/new_media_rights_open_source_licensing_guide |title=Open Source Licensing Guide |author=New Media Rights |publisher=[[California Western School of Law]] |date=2008-09-12|accessdate=2015-11-28 |quote=''The ‘BSD-like’ licenses such as the BSD, MIT, and Apache licenses are extremely permissive, requiring little more than attributing the original portions of the licensed code to the original developers in your own code and/or documentation.''}}&lt;/ref&gt; The Apache License, Version 2.0 requires preservation of the [[copyright]] notice and [[disclaimer]]. Like other [[free software license]]s, the license allows the user of the software the freedom to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software, under the terms of the license, without concern for [[royalties]]. This makes ALv2 a [[Reasonable and non-discriminatory licensing|FRAND-RF]] license. The ASF and its projects release the software they produce under the Apache License and many non-ASF projects are also using the ALv2.

==History==
===Version 1.1===
The '''Apache License 1.1''' was approved by the ASF in 2000: ''The primary change from the 1.0 license is in the 'advertising clause' (section 3 of the 1.0 license); derived products are no longer required to include attribution in their advertising materials, but only in their documentation.''&lt;ref name='Apache licenses'&gt;{{cite web|url=http://www.apache.org/licenses/ |title=Licenses – The Apache Software Foundation |accessdate=7 July 2007 | archiveurl= https://web.archive.org/web/20070701222753/http://www.apache.org/licenses/| archivedate= 1 July 2007 | deadurl= no}}&lt;/ref&gt;

===Version 2.0===
The ASF adopted the '''Apache License 2.0''' in January 2004. The stated goals of the license included ''making the license easier for non-ASF projects to use, improving [[License compatibility|compatibility]] with [[GNU General Public License|GPL]]-based software, allowing the license to be included by reference instead of listed in every file, clarifying the license on contributions, and requiring a patent license on contributions that necessarily infringe a contributor's own patents''.&lt;ref name='Apache licenses'/&gt;

==Licensing conditions==
The Apache License is [[Permissive free software licence|permissive]] in that it does not require a [[derivative work]] of the software, or modifications to the original, to be distributed using the same license (unlike [[copyleft]] licenses – see [[comparison of free software licenses|comparison]]).  It still requires application of the same license to all unmodified parts and, in every licensed file, any original copyright, patent, trademark, and attribution notices in redistributed code must be preserved (excluding notices that do not pertain to any part of the derivative works); and, in every licensed file changed, a notification must be added stating that changes have been made to that file.

If a NOTICE text file is included as part of the distribution of the original work, then derivative works must include a readable copy of these notices within a NOTICE text file distributed as part of the derivative works, within the source form or documentation, or within a display generated by the derivative works (wherever such third-party notices normally appear).

The contents of the NOTICE file do not modify the license, as they are for informational purposes only, and adding more attribution notices as addenda to the NOTICE text is permissible, provided that these notices cannot be understood as modifying the license. Modifications may have appropriate copyright notices, and may provide different license terms for the modifications.

Unless explicitly stated otherwise, any contributions submitted by a licensee to a licensor will be under the terms of the license without any terms and conditions, but this does not preclude any separate agreements with the licensor regarding these contributions.

==GPL compatibility==
The Apache Software Foundation and the [[Free Software Foundation]] agree that the Apache License 2.0 is a [[free software license]], compatible with version 3 of the [[GNU General Public License]] (GPL),&lt;ref name=gnulist&gt;{{cite web|url=https://www.gnu.org/licenses/license-list.html#apache2|title=Various Licenses and Comments about Them|date=14 January 2008|publisher=Free Software Foundation|accessdate=30 January 2008| archiveurl= https://web.archive.org/web/20080118133219/https://www.gnu.org/licenses/license-list.html#DocumentationLicenses| archivedate= 18 January 2008 | deadurl= no}}&lt;/ref&gt; meaning that code under GPL version 3 and Apache License 2.0 can be combined, as long as the resulting software is licensed under the GPL version 3.&lt;ref&gt;{{cite web|url=http://www.apache.org/licenses/GPL-compatibility.html|title=Apache License v2.0 and GPL Compatibility|author=Apache Software Foundation|accessdate=30 January 2008| archiveurl= https://web.archive.org/web/20080115045635/http://www.apache.org/licenses/GPL-compatibility.html| archivedate= 15 January 2008 | deadurl= no}}&lt;/ref&gt;

The Free Software Foundation considers all versions of the Apache License to be [[License compatibility|incompatible]] with the previous GPL versions 1 and 2&lt;ref&gt;{{cite web|url=https://www.gnu.org/licenses/license-list.html|title=Licenses| date=28 February 2013| author=Free Software Foundation| archiveurl= https://web.archive.org/web/20130305182742/https://www.gnu.org/licenses/license-list.html| archivedate= 5 March 2013| deadurl= no}}&lt;/ref&gt; and, furthermore, considers Apache License versions before v2.0 incompatible with GPLv3. Because of version 2.0's patent license requirements, the Free Software Foundation recommends it over other non-copyleft licenses, specifically recommending it either for small programs or for developers who wish to use a permissive license for other reasons.&lt;ref&gt;{{cite web|url=https://gnu.org/licenses/license-recommendations.html|title=How to choose a license for your own work|deadurl=no}}&lt;/ref&gt;

==Reception and adoption==
In October 2012, 8,708 projects located at [[SourceForge.net]] were available under the terms of the Apache License.&lt;ref&gt;{{cite web|url=http://sourceforge.net/search/?&amp;fq%5B%5D=trove%3A401|title=Projects at SourceForge under Apache License|accessdate=28 October 2012}}&lt;/ref&gt;  In a blog post from May 2008, [[Google]] mentioned that over 25% of the nearly 100,000 projects then hosted on [[Google Code]] were using the Apache License,&lt;ref&gt;{{cite web|url=http://google-opensource.blogspot.com/2008/05/standing-against-license-proliferation.html|title=Standing Against License Proliferation|accessdate=24 October 2009}}&lt;/ref&gt; including the [[Android (operating system)|Android operating system]].&lt;ref&gt;[http://source.android.com/source/licenses.html Android Open Source licenses]&lt;/ref&gt;

{{As of|2015}}, according to Black Duck Software&lt;ref name="blackduck2015"&gt;{{cite web|url=http://www.blackducksoftware.com/resources/data/top-20-licenses |quote=''1. MIT license 24%, 2. GNU General Public License (GPL) 2.0 23%, 3. Apache License 16%, 4. GNU General Public License (GPL) 3.0 9%, 5. BSD License 2.0 (3-clause, New or Revised) License 6%, 6. GNU Lesser General Public License (LGPL) 2.1 5%, 7. Artistic License (Perl) 4%, 8. GNU Lesser General Public License (LGPL) 3.0 2%, 9. Microsoft Public License 2%, 10. Eclipse Public License (EPL) 2%'' |title=Top 20 licenses |publisher=Black Duck Software |accessdate=19 November 2015 |date=19 November 2015}}&lt;/ref&gt; and [[GitHub]],&lt;ref name="github"&gt;{{cite web|url=https://github.com/blog/1964-license-usage-on-github-com |quote="1 MIT 44.69%, 2 Other 15.68%, 3 GPLv2 12.96%, 4 Apache 11.19%, 5 GPLv3 8.88%, 6 BSD 3-clause 4.53%, 7 Unlicense 1.87%, 8 BSD 2-clause 1.70%, 9 LGPLv3 1.30%, 10 AGPLv3 1.05% |title=Open source license usage on GitHub.com |date=2015-03-09 |first=Ben |last=Balter |accessdate=2015-11-21 |publisher=[[github.com]]}}&lt;/ref&gt; the Apache license is the third most popular license in the [[FOSS]] domain after [[MIT license]] and [[GPLv2]].

The [[OpenBSD]] project does not consider the Apache License 2.0 to be an acceptable license due to its patent provisions.&lt;ref&gt;{{cite web|title=OpenBSD copyright policy|url=http://www.openbsd.org/policy.html|publisher=OpenBSD Project|accessdate=25 April 2017}}&lt;/ref&gt;

==See also==
{{Portal|Free software}}
* [[Free software license]]
* [[Comparison of free and open-source software licenses]]
*[[:Category:Software using the Apache license|Software using the Apache license (category)]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website|https://www.apache.org/licenses|Apache Licenses}}
* [https://tldrlegal.com/license/apache-license-2.0-%28apache-2.0%29 Quick Summary of the Apache License 2.0]

{{Apache}}
{{FOSS}}

[[Category:Apache Software Foundation|License]]
[[Category:Free and open-source software licenses]]
[[Category:Software using the Apache license]]</text>
      <sha1>1dndpyrpdwbrkv5w4kffkpmb2b7tw6c</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Celix</title>
    <ns>0</ns>
    <id>56913555</id>
    <revision>
      <id>844997580</id>
      <parentid>841985272</parentid>
      <timestamp>2018-06-08T16:41:33Z</timestamp>
      <contributor>
        <username>CommonsDelinker</username>
        <id>2304267</id>
      </contributor>
      <comment>Removing [[:c:File:Apache_Celix.png|Apache_Celix.png]], it has been deleted from Commons by [[:c:User:Taivo|Taivo]] because: [[:c:COM:LR|License review]]: Non-free license, or license disallowing commercial use and/or derivative works: [[:c:Commons:Deleti</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2170">{{Infobox software
| title = Apache Celix&lt;ref&gt;[https://github.com/apache/celix Repository Mirror at GitHub]&lt;/ref&gt;
| name = Apache Celix
| logo =
| developer = Apache Software Foundation
| released = {{Start date and age|2010|11}}
| latest release version = 2.1.0
| latest release date = {{Start date and age|2018|01|30}}
| programming language = [[C (programming language)|C]], [[C++]]
| operating system = [[Linux]], [[macOS]]
| license = [[Apache License]] 2.0
| website = https://celix.apache.org/
}}

'''Apache Celix''' is an [[Open source|open-source]] implementation of the [[OSGi]] specification adapted to [[C (programming language)|C]] and [[C++]] developed by the [[Apache Software Foundation]]. The project aims to provide a framework to develop (dynamic) modular software applications using component and/or [[service-oriented programming]].

Apache Celix is primarily developed in C and adds an additional abstraction, in the form of a library, to support for C++.

Modularity in Apache Celix is achieved by supporting - run-time installed - bundles. Bundles are zip files and can contain software modules in the form of shared libraries. Modules can provide and request dynamic services, for and from other modules, by interacting with a provided bundle context. Services in Apache Celix are "plain old" structs with function pointers or "plain old C++ Objects" (POCO).

== History ==
Apache Celix was welcomed in the [[Apache Incubator]] at November 2010 and graduated to Top Level Project from the Apache Incubator in July 2014.

== References ==
{{reflist}}
{{cite web
|url=https://celix.apache.org/
|title=Apache Celix website
|publisher=The Apache Software Foundation
|year=2018
|accessdate=2018-03-22
|quote="Prose in this article was copied from this source, which is released under an [http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0]"
}}

== External links ==
* [https://celix.apache.org/ Apache Celix website]
* [https://github.com/apache/celix Github mirror]

[[Category:Apache Software Foundation|Celix]]
[[Category:Embedded systems]]
[[Category:Free software]]
[[Category:Service-oriented architecture-related products]]</text>
      <sha1>7m5pkwoz4t6kholqb4reruty7565i6i</sha1>
    </revision>
  </page>
  <page>
    <title>Apache HTTP Server</title>
    <ns>0</ns>
    <id>2581</id>
    <revision>
      <id>846922042</id>
      <parentid>839224109</parentid>
      <timestamp>2018-06-21T18:17:12Z</timestamp>
      <contributor>
        <username>Jam699</username>
        <id>21062423</id>
      </contributor>
      <comment>Update 2016 usage stats with 2018 stats</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21194">{{Technical|date=September 2010}}

{{Infobox software
|name                 = Apache HTTP Server
|logo                 = [[File:Apache_HTTP_server_logo_(2016).svg|250px]]
|screenshot           =
|caption              =
|author               = [[Robert McCool]]
|developer            = [[Apache Software Foundation]]
|released             = {{Start date and age|1995}}&lt;ref&gt;{{Cite web|url=http://httpd.apache.org/ABOUT_APACHE.html|title=About the Apache HTTP Server Project|publisher=[[Apache Software Foundation]]|accessdate=2008-06-25|archiveurl= https://web.archive.org/web/20080607122013/http://httpd.apache.org/ABOUT_APACHE.html|archivedate= 7 June 2008 &lt;!--DASHBot--&gt;|deadurl= no}}&lt;/ref&gt;
|status               = Active
|operating system     = [[Unix-like]], [[Windows]]&lt;ref&gt;{{cite web|title=Compiling and Installing|url=https://httpd.apache.org/docs/2.4/install.html|website=httpd.apache.org|publisher=The Apache Software Foundation|accessdate=9 May 2016}}&lt;/ref&gt;
|programming language = [[C (programming language)|C]],&lt;ref&gt;{{cite web |url=https://projects-old.apache.org/indexes/language.html |title=Archived copy |accessdate=2016-02-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20160302011644/http://projects-old.apache.org/indexes/language.html |archivedate=2016-03-02 |df= }}&lt;/ref&gt; [[XML]]&lt;ref&gt;{{Cite web|title=Languages|work=
Apache HTTP Server|agency=Ohloh|publisher= Black Duck Software|url=https://www.ohloh.net/p/apache/analyses/latest/languages_summary|accessdate=2 April 2014}}&lt;/ref&gt;
|genre                = [[Web server]]
|license              = [[Apache License]] 2.0
}}

The '''Apache HTTP Server''', colloquially called '''Apache''' ({{IPAc-en|ə|ˈ|p|æ|tʃ|iː}} {{Respell|ə|PATCH|ee}}), is a [[free and open-source]] [[cross-platform]] [[web server]], released under the terms of [[Apache License]] 2.0. Apache is developed and maintained by an open community of developers under the auspices of the [[Apache Software Foundation]].

The Apache HTTP Server is cross-platform; {{as of|2017|6|1|lc=y}} 92% of Apache HTTPS Server copies run on [[Linux distribution]]s.&lt;ref&gt;{{cite web |url=https://secure1.securityspace.com/s_survey/data/man.201705/apacheos.html|title=OS/Linux Distributions using Apache |date=1 June 2017}}&lt;/ref&gt; Version 2.0 improved support for non-Unix operating systems such as Windows and OS/2&lt;!--do not include BeOS, while link also includes as later dropped--&gt;.&lt;ref&gt;{{cite web |url=https://httpd.apache.org/docs/2.2/new_features_2_0.html |title=Overview of new features in Apache 2.0 |quote=Apache 2.0 is faster and more stable on non-Unix platforms such as [..] OS/2, and Windows.}}&lt;/ref&gt; Old versions of Apache were [[porting|ported]] to run on [[OpenVMS]]&lt;ref&gt;{{cite web |url=http://h41379.www4.hpe.com/openvms/products/ips/apache/ |website=h41379.www4.hpe.com |title=HP OpenVMS systems - Secure Web Server (based on Apache)}}&lt;/ref&gt; and [[NetWare]].

Originally based on the [[NCSA HTTPd]] server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache played a key role in the initial growth of the [[World Wide Web]],&lt;ref&gt;[http://news.netcraft.com/archives/web_server_survey.html Netcraft Market Share] for Top Servers Across All Domains August 1995 - today (monthly updated)&lt;/ref&gt; quickly overtaking NCSA HTTPd as the dominant [[HTTP]] server, and has remained most popular since April 1996. In 2009, it became the first web server software to serve more than 100 million [[website]]s.&lt;ref name="100millionsites"&gt;{{Cite web|url=http://news.netcraft.com/archives/2009/02/18/february_2009_web_server_survey.html|title=February 2009 Web Server Survey|publisher=[[Netcraft]]|accessdate=2009-03-29|archiveurl= https://web.archive.org/web/20090226092501/http://news.netcraft.com//archives//2009//02//18//february_2009_web_server_survey.html|archivedate= 26 February 2009 &lt;!--DASHBot--&gt;|deadurl= no}}&lt;/ref&gt; {{As of|2018|03}}, it was estimated to serve 43% of all active websites and 37% of the top million websites.&lt;ref&gt;{{cite web|title=March 2018 Web Server Survey|url=https://news.netcraft.com/archives/2018/03/27/march-2018-web-server-survey.html|website=Netcraft|publisher=Netcraft|accessdate=21 June 2018|}}&lt;/ref&gt;

==Name==
According to the FAQ in the Apache project website, the name Apache was chosen out of respect to the Native American tribe [[Apache]] and their superior skills in warfare and strategy. The name was widely believed to be a pun on 'A Patchy Server' (since it was a set of [[Patch (computing)|software patches]]).&lt;ref&gt;{{Cite web|url = http://wiki.apache.org/httpd/FAQ#Why_the_name_.22Apache.22.3F|title = Why the name 'Apache'?|work = HTTPd Frequently Asked Questions }}&lt;/ref&gt; Official documentation used to give this explanation of the name,&lt;ref&gt;{{Cite web|url=//www.apache.org/info.html |title=Information on the Apache HTTP Server Project |date=1997-04-15 |deadurl=yes |archiveurl=https://web.archive.org/web/19970415054031/http://www.apache.org/info.html |archivedate=April 15, 1997 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Apache Server Frequently Asked Questions|url=http://www.apache.org/docs/misc/FAQ.html#relate|accessdate=15 January 2017|archiveurl=https://web.archive.org/web/19970106233141/http://www.apache.org/docs/misc/FAQ.html#relate|archivedate=1997-01-06}}&lt;/ref&gt; but in a 2000 interview, [[Brian Behlendorf]], one of the creators of Apache, set the record straight:&lt;ref&gt;{{cite web|url=http://www.linux-mag.com/id/472/|title=Apache Power|work=Linux Magazine}}&lt;/ref&gt;

{{Quote
|The name literally came out of the blue. I wish I could say that it was something fantastic, but it was out of the blue. I put it on a page and then a few months later when this project started, I pointed people to this page and said: "Hey, what do you think of that idea?" ... Someone said they liked the name and that it was a really good pun. And I was like, "A pun? What do you mean?" He said, "Well, we're building a server out of a bunch of software patches, right? So it's a patchy Web server." I went, "Oh, all right." ... When I thought of the name, no. It just sort of connotated: "Take no prisoners. Be kind of aggressive and kick some ass."
}}

When Apache is running, its process name is sometimes{{when|date=April 2018}} &lt;tt&gt;httpd&lt;/tt&gt;, which is short for "HTTP [[Daemon (computing)|daemon]]".

==Feature overview==
Apache supports a variety of features, many implemented as [[Compiler|compiled]] [[Modular programming|modules]] which extend the core functionality. These can range from server-side programming language support to authentication schemes. Some common language interfaces support &lt;!-- ATTENTION AUTO-EDITORS, THESE UNDERSCORES ARE INTENTIONAL --&gt;[[mod perl|Perl]], [[mod python|Python]], [[Tcl]] and [[PHP]]. Popular authentication modules include mod_access, mod_auth, mod_digest, and mod_auth_digest, the successor to mod_digest. A sample of other features include [[Secure Sockets Layer]] and [[Transport Layer Security]] support ([[mod_ssl]]), a [[proxy server|proxy]] module ([[mod_proxy]]), a [[URL rewriting]] module (mod_rewrite), custom log files (mod_log_config), and filtering support (mod_include and mod_ext_filter).

Popular compression methods on Apache include the external extension module, mod_gzip&lt;!-- redirects to here --&gt;, implemented to help with reduction of the size (weight) of Web pages served over [[HTTP]]. [[ModSecurity]] is an open source intrusion detection and prevention engine for Web applications. Apache logs can be analyzed through a Web browser using free scripts, such as [[AWStats]]/[[W3Perl]] or Visitors.

[[Virtual hosting]] allows one Apache installation to serve many different Web sites. For example, one machine with one Apache installation could simultaneously serve www.example.com, www.example.org, test47.test-server.example.edu, etc.

Apache features configurable error messages, [[Database management system|DBMS]]-based authentication databases, and [[content negotiation]]. It is also supported by several [[graphical user interface]]s (GUIs).

It supports password authentication and [[digital certificate]] authentication. Because the source code is freely available, anyone can adapt the server for specific needs, and there is a large public library of Apache add-ons.&lt;ref&gt;{{cite web|url=http://www.webopedia.com/TERM/A/Apache_Web_server.html|title=What is Apache Web Server? Webopedia|work=webopedia.com}}&lt;/ref&gt;

==HTTP  server and proxy features==
{{prose|date=August 2016}}
* Loadable Dynamic Modules
* Multiple Request Processing modes (MPMs) including [[Event driven programming|Event-based/Async]], Threaded and Prefork.
* Highly scalable (easily handles [[c10k problem|more than 10,000 simultaneous connections]])
* Handling of static files, index files, auto-indexing and content negotiation
* .htaccess support&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/howto/htaccess.html | title = Apache HTTP Server Tutorial: .htaccess files }}&lt;/ref&gt;
* [[Reverse proxy]] with caching&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/mod/mod_proxy.html | title = mod_proxy }}&lt;/ref&gt;
** [[Load balancing (computing)|Load balancing]]&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/mod/mod_proxy_balancer.html | title =  mod_proxy_balancer }}&lt;/ref&gt; with in-band health checks
** Multiple load balancing mechanisms
** [[Fault-tolerant design|Fault tolerance]] and Failover with automatic recovery
** [[WebSocket]], [[FastCGI]], [[SCGI]], [[Apache JServ Protocol|AJP]] and [[uWSGI]] support with caching
** Dynamic configuration&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/trunk/howto/reverse_proxy.html#manager | title = Balancer Manager }}&lt;/ref&gt;
* [[Transport Layer Security|TLS/SSL]] with [[Server Name Indication|SNI]] and [[OCSP stapling]] support, via [[OpenSSL]].
* Name- and IP address-based virtual servers
* [[IPv6]]-compatible
* [[HTTP/2]]  protocol support
* Fine-grained authentication and authorization access control&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/howto/auth.html | title = Authentication and Authorization }}&lt;/ref&gt;
* [[gzip]] compression and decompression
* URL rewriting&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/mod/mod_rewrite.html | title = mod_rewrite }}&lt;/ref&gt;
* Headers&lt;ref&gt;{{ cite web | url =  https://httpd.apache.org/docs/2.4/mod/mod_headers.html | title = mod_headers }}&lt;/ref&gt; and content&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/mod/mod_sed.html | title = mod_sed }}&lt;/ref&gt;&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/mod/mod_substitute.html | title = mod_substitute }}&lt;/ref&gt; rewriting
* Custom logging with rotation
* Concurrent connection limiting
* Request processing rate limiting
* [[Bandwidth throttling]]
* [[Server Side Includes]]&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/howto/ssi.html | title = Apache httpd Tutorial: Introduction to Server Side Includes }}&lt;/ref&gt;
* IP address-based [[geolocation]]
* User and Session tracking&lt;ref&gt;{{ cite web | url = http://httpd.apache.org/docs/2.4/mod/mod_usertrack.html | title = mod_usertrack }}&lt;/ref&gt;
* [[WebDAV]]
* Embedded [[Perl]], [[PHP]] and [[Lua (programming language)|Lua]] scripting
* [[Common Gateway Interface|CGI]] support&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/howto/cgi.html | title = Apache Tutorial: Dynamic Content with CGI }}&lt;/ref&gt;
* public_html per-user web-pages&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/howto/public_html.html | title = Per-user web directorie }}&lt;/ref&gt;
* Generic expression parser&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/expr.html| title = Expressions in Apache HTTP Server }}&lt;/ref&gt;
* Real-time status views&lt;ref&gt;{{ cite web | url = https://httpd.apache.org/docs/2.4/mod/mod_status.html | title =  mod_status }}&lt;/ref&gt;
* [[XML]] support&lt;ref&gt;{{ cite web | url = http://httpd.apache.org/docs/2.4/mod/mod_xml2enc.html | title =  mod_xml2enc }}&lt;/ref&gt;
* [[FTP]] support (by a separate module) &lt;ref&gt; {{cite web | url= https://httpd.apache.org/mod_ftp/mod/mod_ftp.html | title = Apache Module: mod_ftp}} &lt;/ref&gt;

==Performance==
Instead of implementing a single architecture, Apache provides a variety of MultiProcessing Modules (MPMs), which allow Apache to run in a process-based, hybrid (process and thread) or event-hybrid mode, to better match the demands of each particular infrastructure. This implies that the choice of correct MPM and the correct configuration is important. Where compromises in performance need to be made, the design of Apache is to reduce latency and increase [[throughput]], relative to simply handling more requests, thus ensuring consistent and reliable processing of requests within reasonable time-frames.

For delivery of static pages, Apache 2.2 series was considered significantly slower than [[nginx]] and [[Varnish (software)|varnish]].&lt;ref&gt;{{cite web|url=http://nbonvin.wordpress.com/2011/03/14/apache-vs-nginx-vs-varnish-vs-gwan/|title=Serving static files: a comparison between Apache, Nginx, Varnish and G-WAN|work=Spoot!}}&lt;/ref&gt; To address this issue, the Apache developers created the Event MPM, which mixes the use of several processes and several threads per process in an asynchronous event-based loop.&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/worker.html|title=worker - Apache HTTP Server Version 2.2|work=apache.org}}&lt;/ref&gt; This architecture, and the way it was implemented in the Apache 2.4 series, provides for performance equivalent or slightly better than event-based web servers, as is cited by [[Jim Jagielski]] and other independent sources.&lt;ref&gt;[http://people.apache.org/~jim/presos/ACNA11/Apache_httpd_cloud.pdf Apache httpd 2.4]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.slideshare.net/bryan_call/choosing-a-proxy-server-apachecon-2014|title=Picking a Proxy Server}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://blog.matsumoto-r.jp/?p=1812|title=Throughput evaluation of Apache 2.4.1}}&lt;/ref&gt; However, some independent, but significantly outdated, benchmarks show that it still is half as fast as nginx, e.g. &lt;ref&gt;{{cite web|url=http://www.eschrade.com/page/performance-of-apache-2-4-with-the-event-mpm-compared-to-nginx/|title=Performance of Apache 2.4 with the event MPM compared to Nginx|work=eschrade.com}}&lt;/ref&gt;

==Licensing==
The Apache HTTP Server [[codebase]] was [[Software relicensing|relicensed]] to the [[Apache License|Apache 2.0 License]] (from the previous 1.1 license) in January 2004,&lt;ref&gt;{{Cite web|url=https://www.apache.org/licenses/LICENSE-2.0.html|title=Apache License, Version 2.0|publisher=The Apache Software Foundation|date=January 2004|accessdate=2013-05-21}}&lt;/ref&gt; and Apache HTTP Server 1.3.31 and 2.0.49 were the first [[Software release life cycle|releases]] using the new license.&lt;ref&gt;{{Cite newsgroup|title=FYI: Apache HTTP Server 2.0.49 Released|last=Burton|first=Richard Antony|newsgroup=alt.apache.configuration|url=https://groups.google.com/d/msg/alt.apache.configuration/042hhGcLWUs/rDJdt5b927cJ|accessdate=2018-02-16}}&lt;/ref&gt;

The [[OpenBSD]] project did not like the change and continued the use of pre-2.0 Apache versions, effectively forking Apache 1.3.x for its purposes.&lt;ref&gt;{{Cite mailing list|url=http://marc.info/?l=openbsd-misc&amp;m=107714762916291|title=The new apache license|last=de Raadt|first=Theo|authorlink=Theo de Raadt|mailinglist=openbsd-misc|date=18 February 2004|accessdate=2013-05-21}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.openbsd.org/policy.html|title=Copyright Policy|publisher=OpenBSD|accessdate=2013-05-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://ports.su/www/apache-httpd-openbsd |title=apache-httpd-openbsd-1.3.20140502p2 – OpenBSD improved and secured version of Apache 1.3 |website=OpenBSD ports |date=|accessdate=2014-12-28}}&lt;/ref&gt; They initially replaced it with [[Nginx]], and soon after made their own replacement, OpenBSD Httpd, based on the relayd project.&lt;ref&gt;{{cite web|url=http://www.openbsd.org/faq/upgrade52.html#nginx|title=OpenBSD Upgrade Guide: 5.1 to 5.2|work=openbsd.org}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://undeadly.org/cgi?action=article&amp;sid=20140314080734|date=2014-03-14|title= Heads Up: Apache Removed from Base |website=[[OpenBSD Journal]]|editor=jj}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.openbsd.org/faq/upgrade56.html#ToPorts|title=OpenBSD Upgrade Guide: 5.5 to 5.6|work=openbsd.org}}&lt;/ref&gt;

===Versions===

Version 1.1:
The Apache License 1.1 was approved by the ASF in 2000: The primary change from the 1.0 license is in the 'advertising clause' (section 3 of the 1.0 license); derived products are no longer required to include attribution in their advertising materials, but only in their documentation.

Version 2.0:
The ASF adopted the Apache License 2.0 in January 2004. The stated goals of the license included making the license easier for non-ASF projects to use, improving compatibility with GPL-based software, allowing the license to be included by reference instead of listed in every file, clarifying the license on contributions, and requiring a patent license on contributions that necessarily infringe a contributor's own patents.

==Development==
{|class="wikitable" style="float: right; margin-left: 1em;"
|-
!Version
!Initial release
!Latest release
|-
|{{Version|o|1.3}}
|1998-06-06&lt;ref&gt;{{Cite web|url=http://marc.info/?l=apache-httpd-announce&amp;m=90221040625561&amp;w=2|title=Announcement: Apache 1.3.0 Released !|date=1998-06-06|accessdate=2015-01-06}}&lt;/ref&gt;
|2010-02-03 (1.3.42)&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/httpd-announce/201002.mbox/%3C20100203000334.GA19021%40infiltrator.stdlib.net%3E|title=Apache HTTP Server 1.3.42 released (final release of 1.3.x)|work=apache.org}}&lt;/ref&gt;
|-
|{{Version|o|2.0}}
|2002-04-06&lt;ref&gt;{{Cite web|url=http://marc.info/?l=apache-httpd-announce&amp;m=101810732100356&amp;w=2|title=Official Release: Apache 2.0.35 is now GA|date=2002-04-06|accessdate=2015-01-06}}&lt;/ref&gt;
|2013-07-10 (2.0.65)&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/httpd-announce/201307.mbox/%3C20130710124920.2b8793ed.wrowe%40rowe-clan.net%3E|title=[Announcement] Apache HTTP Server 2.0.65 Released|work=apache.org}}&lt;/ref&gt;
|-
|{{Version|o|2.2}}
|2005-12-01&lt;ref&gt;{{Cite web|url=http://marc.info/?l=apache-httpd-announce&amp;m=113347470201565&amp;w=2|title=Apache HTTP Server 2.2.0 Released|date=2005-12-01|accessdate=2015-01-06}}&lt;/ref&gt;
|2017-07-11 (2.2.34)&lt;ref&gt;{{cite web|url=https://www.apache.org/dist/httpd/Announcement2.2.html|title=[Announce] Apache HTTP Server 2.2.34 Released|work=apache.org}}&lt;/ref&gt;
|-
|{{Version|c|2.4}}
|2012-02-21&lt;ref&gt;{{Cite web|url=http://marc.info/?l=apache-httpd-announce&amp;m=132983471818384&amp;w=2|title=[ANNOUNCEMENT] Apache HTTP Server 2.4.1 Released|date=2012-02-21|accessdate=2015-07-17}}&lt;/ref&gt;
|2018-03-26 (2.4.33)&lt;ref&gt;{{cite web|url=https://www.apache.org/dist/httpd/Announcement2.4.html|title=Apache HTTP Server 2.4.33 Released|work=apache.org}}&lt;/ref&gt;
|-
|colspan="3"|&lt;small&gt;{{Version|l|show=011100}}&lt;/small&gt;
|}
The Apache HTTP Server Project is a collaborative software development effort aimed at creating a robust, commercial-grade, feature-rich and freely available source code implementation of an HTTP (Web) server. The project is jointly managed by a group of volunteers located around the world, using the Internet and the Web to communicate, plan, and develop the server and its related documentation. This project is part of the Apache Software Foundation. In addition, hundreds of users have contributed ideas, code, and documentation to the project.&lt;ref&gt;{{cite web|url=http://httpd.apache.org/ABOUT_APACHE.html|title=About the Apache HTTP Server Project - The Apache HTTP Server Project|author=Documentation Group|work=apache.org}}&lt;/ref&gt;&lt;ref&gt;[http://www.ohloh.net/p/apache The Apache HTTP Server Open Source Project on Ohloh. (n.d.). Ohloh, the open source network. Retrieved November 12, 2012]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://docs.fedoraproject.org/en-US/Fedora/13/html/Managing_Confined_Services/chap-Managing_Confined_Services-The_Apache_HTTP_Server.html|title=Chapter 4. The Apache HTTP Server|work=fedoraproject.org}}&lt;/ref&gt;

Apache 2.4 dropped support for [[BeOS]], [[Transaction Processing Facility|TPF]] and even older platforms.&lt;ref&gt;{{cite web |url=http://httpd.apache.org/docs/2.4/upgrading.html |title=Upgrading to 2.4 from 2.2 |quote=Platform support has been removed for BeOS, TPF, and even older platforms such as A/UX, Next, and Tandem. These were believed to be broken anyway.}}&lt;/ref&gt;

==See also==
{{Portal|Free software}}
* [[.htaccess]]
* [[.htpasswd]]
* [[ApacheBench]]
* [[Comparison of web server software]]
* [[IBM HTTP Server]]
* [[LAMP (software bundle)]]
* [[List of Apache modules]]
* [[POSSE project]]
* [[suEXEC]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}

{{Apache}}
{{Web server software}}
{{Web interfaces}}

{{Authority control}}

[[Category:1995 software]]
[[Category:Apache Software Foundation|HTTP Server]]
[[Category:Apache Software Foundation projects| ]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in C]]
[[Category:Free web server software]]
[[Category:Reverse proxy]]
[[Category:Software using the Apache license]]
[[Category:Unix network-related software]]
[[Category:Web server software for Linux]]</text>
      <sha1>aeqir9nv4ue6aihm9p3wdx28usgidgo</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OpenNLP</title>
    <ns>0</ns>
    <id>30972465</id>
    <revision>
      <id>840096605</id>
      <parentid>829390138</parentid>
      <timestamp>2018-05-07T18:11:37Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2173">{{Infobox software
| name                   = Apache OpenNLP
| logo                   = Apache OpenNLP Logo.svg
| screenshot             = 
| caption                = 
| collapsible            = 
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = {{release date|mf=yes|2004|04|22}}
| latest release version = 1.8.4
| latest release date    = {{release date|mf=yes|2017|12|26}}
| latest preview version = 
| latest preview date    = 
| programming language   = [[Java (programming language)|Java]]
| platform               = 
| size                   = 
| language               = 
| genre                  = [[Natural language processing]]
| license                = [[Apache License|Apache 2.0]]
}}
The '''Apache OpenNLP''' library is a [[machine learning]] based toolkit for the [[natural language processing|processing of natural language]] text. It supports the most common NLP tasks, such as [[Language identification|language detection]], [[Tokenization (lexical analysis)|tokenization]], [[Sentence boundary disambiguation|sentence segmentation]], [[part-of-speech tagging]], [[Named entity recognition|named entity extraction]], [[Shallow parsing|chunking]], [[Syntactic parsing|parsing]] and [[coreference|coreference resolution]]. These tasks are usually required to build more advanced text processing services.&lt;ref&gt;[http://opennlp.apache.org/index.html Apache OpenNLP Website]&lt;/ref&gt;&lt;ref&gt;[http://wiki.apache.org/incubator/OpenNLPProposal Apache OpenNLP Proposal]&lt;/ref&gt;

==See also== 
{{Portal|Free software}}
* [[List of natural language processing toolkits]]
** [[Unstructured Information Management Architecture]] (UIMA)
** [[General Architecture for Text Engineering]] (GATE)
* [[cTAKES]]

== References ==
&lt;references/&gt;

==External links==
*[https://opennlp.apache.org/index.html Apache OpenNLP Website]

{{Apache}}

[[Category:Natural language processing]]
[[Category:Statistical natural language processing]]
[[Category:Natural language processing toolkits]]
[[Category:Apache Software Foundation|OpenNLP]]
[[Category:Java (programming language) libraries]]
[[Category:Cross-platform software]]</text>
      <sha1>giz824ljtkiisasepm563904l4hmsgl</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Trafodion</title>
    <ns>0</ns>
    <id>43330944</id>
    <revision>
      <id>840096747</id>
      <parentid>825594011</parentid>
      <timestamp>2018-05-07T18:12:33Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4986">{{Infobox software
| name                   = Trafodion
| logo                   = 
| developer              = Apache Software Foundation
| latest release version = 2.1.0
| latest release date    = {{release date|2017|5|1}}
| programming language   = [[C++]], [[Java (programming language)|Java]]
| operating system       = [[Linux]]
| genre                  = SQL-on-Hadoop [[Relational database management system|DBMS]]
| status                 = Active
| license                = [[Apache License]] 2.0
| website                = [http://trafodion.apache.org trafodion.apache.org]
}}

'''Apache Trafodion''' is an [[open source|open-source]] Top-Level Project at the [[Apache Software Foundation]]. It was originally developed by the [[information technology]] division of [[Hewlett-Packard|Hewlett-Packard Company]] and [[HP Labs]] to provide the [[SQL]] query language on [[Apache HBase]] targeting [[big data]] transactional or operational workloads.&lt;ref name="trafodion-wiki"&gt;{{cite web|title=Trafodion: Transactional SQL-on-HBase |url=https://wiki.trafodion.org/wiki/index.php/Main_Page |date=June 9, 2014 |accessdate=July 17, 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140725065043/https://wiki.trafodion.org/wiki/index.php/Main_Page |archivedate=July 25, 2014 }}&lt;/ref&gt; The project was named after the Welsh word for transactions.&lt;ref name="trafodion-wiki" /&gt;

==Features==
Trafodion is a [[relational database management system]] that runs on [[Apache Hadoop]],&lt;ref name="trafodion-features"&gt;{{cite web|title=Trafodion: First Release Features (Release 0.8.0) |url=https://wiki.trafodion.org/wiki/index.php/Release_0.8.0_Features |date=May 29, 2014 |accessdate=October 21, 2015 }}{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; providing support for transactional or operational workloads in a [[big data]] environment.&lt;ref name="trafodion-wiki" /&gt; The following is a list of key features:

* [[SQL|ANSI SQL]] language support&lt;ref name="trafodion-features" /&gt;
* [[Java Database Connectivity|JDBC]] and [[Open Database Connectivity]] (ODBC) connectivity for Linux and Windows clients&lt;ref name="trafodion-features" /&gt;
* Distributed [[ACID]] transaction protection across multiple statements, tables, and rows&lt;ref name="trafodion-features" /&gt;
* Compile-time and run-time optimizations for real-time operational workloads&lt;ref name="trafodion-features" /&gt;
* Support for large data sets using a parallel-aware query optimizer and a parallel data-flow execution engine&lt;ref name="trafodion-features" /&gt;

Transaction management features include:

* Begin, commit, and rollback work syntax, including SET TRANSACTION&lt;ref name="trafodion-features" /&gt;
* READ COMMITTED transactional isolation level&lt;ref name="trafodion-features" /&gt;
* Multiple SQL processes participating in the same transaction concurrently&lt;ref name="trafodion-features" /&gt;
* Recovery after region server, transaction manager, or node failure&lt;ref name="trafodion-features" /&gt;
* Support for region splits and balancing&lt;ref name="trafodion-features" /&gt;

==History==
Trafodion was launched by HP as an [[open source|open-source]] project on June 10, 2014.&lt;ref name="introducing-trafodion"&gt;{{cite web | title=Introducing Trafodion - HP Enterprise Business Community | url=http://h30499.www3.hp.com/t5/Information-Faster-Blog/Introducing-Trafodion/ba-p/6512958#.U73FLvldW0I | date=June 17, 2014 | accessdate=July 17, 2014}}&lt;/ref&gt;

A version of Trafodion was released on January 29, 2015.&lt;ref name="Trafodion-1.0.0"&gt;{{cite web|title=Release of Trafodion 1.0.0 |url=https://wiki.trafodion.org/wiki/index.php/Event:2015/01/29_Release_of_Trafodion_1.0.0 |date=January 30, 2015 |accessdate=February 23, 2015 |deadurl=yes |archiveurl=https://web.archive.org/web/20150224040747/https://wiki.trafodion.org/wiki/index.php/Event%3A2015/01/29_Release_of_Trafodion_1.0.0 |archivedate=February 24, 2015 }}&lt;/ref&gt;

Trafodion became an Apache Incubation Project in May 2015.&lt;ref name="Apache-Incubation-ProjList"&gt;{{cite web | title=Apache Incubation Project List |
url=http://incubator.apache.org/projects/ | accessdate=October 19, 2015}}&lt;/ref&gt;

Trafodion graduated from the Apache Incubator to become a Top-Level Project at the Apache Software Foundation in January 2018.&lt;ref name="Apache-Foundation-Trafodion-TLP"&gt;{{cite web | title=The Apache Software Foundation Announces Apache Trafodion as a Top-Level Project |
url=https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces27 | accessdate=February 13, 2018}}&lt;/ref&gt;

==See also==
* [[Big data]]
* [[Apache Hadoop|Hadoop]]
* [[Apache HBase|HBase]]
* [[NewSQL]]

==References==
{{Reflist}}

==External links==
* [http://trafodion.apache.org Apache Trafodion website]


{{Apache}}

[[Category:Bigtable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:Structured storage]]

[[Category:Apache Software Foundation|Trafodion]] 
[[Category:Apache Software Foundation projects]]</text>
      <sha1>6sw1xdt9u202fnv0egea9o3h0zliu1n</sha1>
    </revision>
  </page>
  <page>
    <title>Apache FOP (Formatting Objects Processor)</title>
    <ns>0</ns>
    <id>632271</id>
    <revision>
      <id>840098042</id>
      <parentid>840097949</parentid>
      <timestamp>2018-05-07T18:21:16Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4298">{{Infobox software
| name                   = Apache FOP
| logo                   =
| screenshot             =
| caption                =
| author                 = [[James Tauber]]
| developer              = [[Apache Software Foundation]]
| latest release version = 2.2
| latest release date    = {{Start date and age|2017|4|10}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[XSL-FO]]
| license                = [[Apache License]] 2.0
| website                = http://xmlgraphics.apache.org/fop
}}

'''Formatting Objects Processor''' ('''FOP''', also known as '''Apache FOP''') is a [[Java (programming language)|Java]] application that converts [[XSL Formatting Objects]] (XSL-FO) files to [[Portable Document Format|PDF]] or other printable formats.
FOP was originally developed by [[James Tauber]] who donated it to the [[Apache Software Foundation]] in 1999. It is part of the [[Apache XML Graphics]] project.

FOP is [[open source software]], and is distributed under the [[Apache License]] 2.0.

==Current Status==

The latest version of Apache FOP is 2.2. This is the sixth stable release and implements a substantial subset of the W3C XSL-FO 1.1 Recommendation. In version 0.95, the code had undergone a substantial rewrite compared to 0.20.5 which was the previous stable version. As of release 0.91alpha, FOP became much more compliant to the XSL-FO Recommendation.

==Major Limitations==

Most important elements added in XSL-FO 1.1 (flow maps, table markers, indexes. etc.) are not available &lt;ref&gt;{{cite web|url=http://xmlgraphics.apache.org/fop/compliance-static.html|title=Apache FOP XSL-FO Compliance}}&lt;/ref&gt;

In addition, older XSL-FO 1.0 features are still not fully supported including automatic table layout, floats and more.

==Input Support==

Apache FOP supports embedding a number of image formats in the XSL-FO (through the &lt;code&gt;&lt;fo:external-graphic&gt;&lt;/code&gt; element). These include:

* [[Scalable Vector Graphics|SVG]]
* [[Portable Network Graphics|PNG]]
* Bitmap [[BMP file format|BMP]]
* [[PostScript]] (as EPS)
* [[JPEG]]
* Some [[TIFF]] formats.

Apache FOP implements the &lt;code&gt;&lt;fo:float&gt;&lt;/code&gt; element with some limitations.&lt;ref&gt;{{cite web |url=https://xmlgraphics.apache.org/fop/2.0/releaseNotes_2.0.html |title=Apache FOP 2.0 release notes |accessdate=2015-12-18}}&lt;/ref&gt; In versions prior to 2.0, external graphics objects were limited to being drawn inline or in a block with no wrapped text.

==Output Formats==

Apache FOP supports the following output formats:

* [[PDF]] (best output support), including [[PDF/X]] and [[PDF/A]] with some limitations&lt;ref&gt;{{cite web |url=http://xmlgraphics.apache.org/fop/0.95/pdfx.html |title=FOP 0.95 - PDF/X (ISO 15930) |accessdate=2011-05-22}}&lt;/ref&gt;
* [[ASCII]] text file facsimile
* [[PostScript]]
* Direct printer output ([[Printer Control Language|PCL]])
* [[Advanced Function Presentation|AFP]]
* [[Rich Text Format|RTF]]
* [[Java2D]]/[[Abstract Window Toolkit|AWT]] for display, printing, and page rendering to [[Portable Network Graphics|PNG]] and [[Tagged Image File Format|TIFF]]

In progress:
*[[Maker Interchange Format|MIF]]
*[[Scalable Vector Graphics|SVG]]

==See also==
{{Portal|Free software}}
* [[XSL Formatting Objects]] (XSL-FO)
* [[Extensible Stylesheet Language|XSL]]

==External links==
* [http://xmlgraphics.apache.org/fop/ Apache FOP Project]
* Ecrion Software [http://www.ecrion.com/Products/XFDesigner/Overview.aspx Visual XSL-FO Editor/Designer and FOP Cleanup Plug-In]
* [http://www.data2type.de/en/xml-xslt-xslfo/xsl-fo/formatter-comparison/ XSL-FO formatter comparison]
* [http://www.ibstaff.net/fmartinez/?p=15 VelFop: Dynamic FOP with Velocity]
* [http://www.data2type.de/software/antillesxml AntillesXML (GUI for FOP and other Processors)] 
* [http://xmlgraphics.apache.org/fop/dev/tools.html FOP Development Tools]
* [http://pdfnow.com Free, ready-to-use Apache FOP webservice]

==References==
{{Reflist}}

{{Apache}}

[[Category:Apache Software Foundation|FOP]]
[[Category:Free system software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) libraries]]


{{compu-library-stub}}</text>
      <sha1>eho51c5q91vqamii51l64zjrnb3o532</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OODT</title>
    <ns>0</ns>
    <id>31356221</id>
    <revision>
      <id>841860592</id>
      <parentid>840099668</parentid>
      <timestamp>2018-05-18T15:07:40Z</timestamp>
      <contributor>
        <username>Christian75</username>
        <id>1306352</id>
      </contributor>
      <minor/>
      <comment>Christian75 moved page [[Apache OODT (Object Oriented Data Technology)]] to [[Apache OODT]]: Only "disambiguation" if there is another article with the same name</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7783">{{Infobox software
| name = OODT
| logo = 
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| latest release version = 1.2
| latest release date = {{release date and age|2017|08|28}}
| latest preview version = 
| latest preview date = 
| status = Active
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| platform = 
| genre = [[Search algorithm|Search]] and [[index (search engine)|index]] [[Application programming interface|API]]
| license = [[Apache License]] 2.0
| website = {{URL|http://oodt.apache.org/}}
}}

The Apache '''Object Oriented Data Technology''' (OODT) is an open source [[data management system]] framework that is managed by the [[Apache Software Foundation]].  OODT was originally developed at [[NASA Jet Propulsion Laboratory]] to support capturing, processing and sharing of data for NASA's scientific archives.

== History ==
The project started out as an internal [[NASA Jet Propulsion Laboratory]] project incepted by Daniel J. Crichton, Sean Kelly and Steve Hughes. The early focus of the effort was on information integration and search using XML as described in Crichton et al.'s paper in the CODATA meeting in 2000.&lt;ref&gt;{{cite journal|last1=Crichton|first1=Daniel|last2=Hughes|first2=John|last3=Hyon|first3=Jason|last4=Kelly|first4=Sean|title=Science Search and Retrieval using XML|journal=The Second National Conference on Scientific and Technical Data, US National Committee for CODATA, National Research Council.|date=2000}}&lt;/ref&gt;

After deploying OODT to the [[Planetary Data System]] and to the [[National Cancer Institute]] [[EDRN]] or Early Detection Research Network project, OODT in 2005 moved into the era of large scale data processing and management via [[NASA]]'s [[Orbiting Carbon Observatory]] (OCO) project. OODT's role on OCO was to usher in a new data management processing framework that instead of 10s of jobs per day and 10s of gigabytes of data would handle 10,000 jobs per day and 100s of terabytes of data. This required an overhaul of OODT to support these new requirements. Dr. [[Chris Mattmann]] at NASA JPL led a team of 3-4 developers between 2005-2009 and completely re-engineered OODT to support these new requirements.

Influenced by the emerging efforts in [[Apache Nutch]] and [[Hadoop]] which Mattmann participated in, OODT was given an overhaul making it more amenable towards Apache Software Foundation like projects. In addition, Mattmann had a close relationship with Dr. [[Justin Erenkrantz]], who as the Apache Software Foundation President at the time, and the idea to bring OODT to the Apache Software Foundation emerged. In 2009, Mattmann and his team received approval from NASA and from JPL to bring OODT to Apache making it the first NASA project to be stewarded by the foundation. Seven years later, the project has released a version 1.0.

== Features ==
OODT focuses on two canonical use cases: [[Big Data]] processing and on [[Information integration]]. Both were described in Mattmann's ICSE 2006&lt;ref&gt;{{Cite journal|last=Mattmann|first=Chris A.|last2=Crichton|first2=Daniel J.|last3=Medvidovic|first3=Nenad|last4=Hughes|first4=Steve|date=2006-01-01|title=A Software Architecture-based Framework for Highly Distributed and Data Intensive Scientific Applications|url=http://doi.acm.org/10.1145/1134285.1134400|journal=Proceedings of the 28th International Conference on Software Engineering|series=ICSE '06|location=New York, NY, USA|publisher=ACM|pages=721–730|doi=10.1145/1134285.1134400|isbn=1595933751}}&lt;/ref&gt; and SMC-IT 2009&lt;ref&gt;{{Cite journal|last=Mattmann|first=C. A.|last2=Freeborn|first2=D.|last3=Crichton|first3=D.|last4=Foster|first4=B.|last5=Hart|first5=A.|last6=Woollard|first6=D.|last7=Hardman|first7=S.|last8=Ramirez|first8=P.|last9=Kelly|first9=S.|date=2009-07-01|title=A Reusable Process Control System Framework for the Orbiting Carbon Observatory and NPP Sounder PEATE Missions|url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5226834|journal=Third IEEE International Conference on Space Mission Challenges for Information Technology, 2009. SMC-IT 2009|pages=165–172|doi=10.1109/SMC-IT.2009.27}}&lt;/ref&gt; papers. It provides three core services. A File Manager is responsible for tracking file locations, their metadata, and for transferring files from a staging area to controlled access storage. A Workflow Manager captures control flow and data flow for complex processes, and allows for reproducibility and the construction of scientific pipelines. A Resource Manager handles allocation of Workflow Tasks and other jobs to underlying resources, e.g., Python jobs go to nodes with Python installed on them; jobs that require a large disk or CPU are properly sent to those nodes that fulfill those requirements.

In addition to the three core services, OODT provides three client-oriented frameworks that build on these services. A file Crawler automatically extracts metadata and uses [[Apache Tika]] to identify file types and ingest the associated information into the File Manager. A Push/Pull framework acquires remote files and makes them available to the system. Finally, a scientific algorithm wrapper (called CAS-PGE, for Catalog and Archive Service Production Generation Executive) encapsulates scientific codes and allows for their execution independent of environment, and while doing so capturing provenance, and making the algorithms easily integrated into a production system.

The overall motivation for OODT's re-architecting was described in a paper in [[Nature (journal)]] in 2013 by Mattmann called A Vision for Data Science.&lt;ref&gt;{{Cite journal|last=Mattmann|first=Chris A.|date=2013-01-24|title=Computing: A vision for data science|url=http://www.nature.com/nature/journal/v493/n7433/full/493473a.html|journal=Nature|language=en|volume=493|issue=7433|pages=473–475|doi=10.1038/493473a|issn=0028-0836}}&lt;/ref&gt;

OODT is written in the [[Java (programming language)|Java]], and through its [[REST API]] &lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/OODT/Apache+OODT+APIs|title=Apache OODT APIs - OODT - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-06-27}}&lt;/ref&gt; used in other languages including [[Python (programming language)]].

== Notable uses ==
OODT has been recently highlighted as contributing to NASA missions including [[Soil Moisture Active Passive]]&lt;ref&gt;{{Cite web|url=https://twitter.com/TheASF/status/561603593024061441|title=Apache - The ASF on Twitter|access-date=2016-06-27}}&lt;/ref&gt; and [[New Horizons]].&lt;ref&gt;{{Cite web|url=https://twitter.com/TheASF/status/621666129648484352|title=Apache - The ASF on Twitter|access-date=2016-06-27}}&lt;/ref&gt; OODT also helps to power the [[Square Kilometre Array]] telescope&lt;ref&gt;{{Cite web|url=https://twitter.com/TheASF/status/475973267233472512|title=Apache - The ASF on Twitter|access-date=2016-06-27}}&lt;/ref&gt; increasing the scope of its use from Earth science, Planetary science, radio astronomy, and to other sectors. OODT is also used within bioinformatics and is a part of the Knowledgent Big Data Platform.&lt;ref&gt;{{Cite web|url=https://blog.knowledgent.com/qa-arpan-bhattacharya-advantages-object-orientated-data-technology-oodt/|title=Q&amp;A on the Advantages of OODT - Object Oriented Data Technology - Knowledgent Perspectives|date=2014-07-30|language=en-US|access-date=2016-06-27}}&lt;/ref&gt;

==External links==
* http://oodt.apache.org
* http://edrn.nci.nih.gov
* http://pds.nasa.gov
* http://picu.net

==References==
{{Reflist}}

{{Apache}}

[[Category:Apache Software Foundation|OODT]]
[[Category:Java platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) libraries]]
[[Category:Software using the Apache license]]</text>
      <sha1>e791c5d6c0gz2aohm22swocempusmv8</sha1>
    </revision>
  </page>
  <page>
    <title>Apache MXNet</title>
    <ns>0</ns>
    <id>52513310</id>
    <revision>
      <id>840198987</id>
      <parentid>840099878</parentid>
      <timestamp>2018-05-08T10:27:01Z</timestamp>
      <contributor>
        <ip>2620:0:1008:1:E4BD:90A2:6835:E25</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5719">{{Infobox software
| name                   = Apache MXNet
| logo                   = 
| screenshot             =
| caption                =
| collapsible            =
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = 
| latest release version = 
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| programming language   = [[C++]], [[Python (programming language)|Python]], [[R (programming language)|R]], [[Julia (programming language)|Julia]], [[JavaScript]], [[Scala (programming language)|Scala]], [[Go (programming language)|Go]], [[Perl (programming language)|Perl]]
| operating system       = [[Microsoft Windows|Windows]], [[macOS]], [[Linux]]
| platform               =
| size                   =
| language               =
| status                 =
| genre                  = Library for [[machine learning]] and [[deep learning]]
| license                = [[Apache License 2.0]]
| website                = {{URL|https://mxnet.apache.org/}}
}}

'''Apache MXNet''' is a modern open-source [[deep learning]] framework used to train, and deploy deep neural networks. It is scalable, allowing for fast model training, and supports a flexible programming model and multiple languages ([[C++]], [[Python (programming language)|Python]], [[Julia (programming language)|Julia]], [[Matlab]], [[JavaScript]], [[Go (programming language)|Go]], [[R (programming language)|R]], [[Scala (programming language)|Scala]], [[Perl (programming language)|Perl]], [[Wolfram Language]])

The MXNet library is portable and can scale to multiple GPUs&lt;ref&gt;{{cite web|url=https://blogs.technet.microsoft.com/machinelearning/2016/09/15/building-deep-neural-networks-in-the-cloud-with-azure-gpu-vms-mxnet-and-microsoft-r-server/|title=Building Deep Neural Networks in the Cloud with Azure GPU VMs, MXNet and Microsoft R Server|publisher=|accessdate=13 May 2017}}&lt;/ref&gt; and multiple machines. MXNet is supported by major Public Cloud providers including AWS&lt;ref&gt;{{cite web|url=https://aws.amazon.com/mxnet/|title=Apache MXNet on AWS - Deep Learning on the Cloud|website=Amazon Web Services, Inc.|accessdate=13 May 2017}}&lt;/ref&gt; and Azure.&lt;ref&gt;{{citeweb|url=https://blogs.technet.microsoft.com/machinelearning/2016/09/15/building-deep-neural-networks-in-the-cloud-with-azure-gpu-vms-mxnet-and-microsoft-r-server/|title=Building Deep Neural Networks in the Cloud with Azure GPU VMs, MXNet and Microsoft R Server.|website=Microsoft TechNet Blogs|accessdate=6 September 2017}}&lt;/ref&gt; Amazon has chosen MXNet as its deep learning framework of choice at AWS.&lt;ref&gt;{{cite web|url=http://www.allthingsdistributed.com/2016/11/mxnet-default-framework-deep-learning-aws.html|title=MXNet - Deep Learning Framework of Choice at AWS - All Things Distributed|website=www.allthingsdistributed.com|accessdate=13 May 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://fortune.com/2016/11/22/amazon-deep-learning-mxnet/|title=Amazon Has Chosen This Framework to Guide Deep Learning Strategy|website=Fortune|accessdate=13 May 2017}}&lt;/ref&gt; Currently, MXNet is supported by [[Intel]], Dato, [[Baidu]], [[Microsoft]], [[Wolfram Research]], and research institutions such as [[Carnegie Mellon University|Carnegie Mellon]], [[Massachusetts Institute of Technology|MIT]], the [[University of Washington]], and the [[Hong Kong University of Science and Technology]].&lt;ref&gt;{{Cite news|url=http://techgenix.com/mxnet-amazon-apache-incubator/|title=MXNet, Amazon’s deep learning framework, gets accepted into Apache Incubator|access-date=2017-03-08|language=en-US}}&lt;/ref&gt;

==Features==
Apache MXNet is a lean, flexible, and ultra-scalable deep learning framework that supports state of the art in deep learning models, including convolutional neural networks (CNNs) and [[long short-term memory]] networks (LSTMs).

===Scalable===
MXNet is designed to be distributed on dynamic Cloud infrastructure, using distributed parameter server (based on &lt;ref&gt;{{cite web|url=https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf|title=Scaling Distributed Machine Learning with the Parameter Server|publisher=|accessdate=2014-10-08}}&lt;/ref&gt;), and can achieve almost linear scale with multiple GPU/CPU.

===Flexible===
MXNet supports both imperative and symbolic programming, which makes it easier for developers that are used to imperative programming to get started with deep learning. It also makes it easier to track, debug, save checkpoints, modify [[Hyperparameter_(machine_learning)|hyperparameters]], such as learning rate or perform [[early stopping]].

===Multiple Languages===
Supports C++ for the optimized backend to get the most of the GPU or CPU available, and Python, R, Scala, Julia, Perl, Matlab and Javascript for the simple to use frontend for the developers.

===Portable===
Supports an efficient deployment of a trained model to low-end devices for inference, such as mobile devices (using Amalgamation [https://mxnet.incubator.apache.org/faq/smart_device.html Amalgamation]), IoT devices (using AWS Greengrass), Serverless (Using AWS Lambda) or [[Container (virtualization)|containers]]. These low-end environments can have only weaker CPU or limited memory (RAM), and should be able to use the models that were trained on a higher-level environment (GPU based cluster, for example).

==See also==
* [[Comparison of deep learning software]]

==References==
{{reflist}}

{{Deep_Learning_Software}}
{{Apache}}

[[Category:Data mining and machine learning software]]
[[Category:Deep learning]]
[[Category:Free statistical software]]
[[Category:Apache Software Foundation|MXNet]]
[[Category:Cross-platform free software]]
[[Category:Software using the Apache license]]</text>
      <sha1>r5hhc2vya4o01s30q9n2i6dggo3yqxu</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Pig</title>
    <ns>0</ns>
    <id>29417433</id>
    <revision>
      <id>840100210</id>
      <parentid>840100146</parentid>
      <timestamp>2018-05-07T18:37:59Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9090">{{Infobox Software
| name                   = Apache Pig
| logo                   = 
| caption                =
| author                 =
| developer              = [[Apache Software Foundation]], Yahoo Research
| status                 = Active
| released               = {{Start date and age|2008|09|11}}
| latest release version = v0.17.0
| latest release date    = {{Start date and age|2017|06|19}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Microsoft Windows]], [[OS X]], [[Linux]]
| size                   = 
| programming language   =
| genre                  = Data analytics
| license                = [[Apache License]] 2.0 
| website                = {{URL|https://pig.apache.org}}
}}

'''Apache Pig'''&lt;ref name="mainpage"&gt;{{cite web |url=http://pig.apache.org/|title=Hadoop: Apache Pig|accessdate=Sep 2, 2011}}&lt;/ref&gt;
is a high-level platform for creating programs that run on [[Hadoop|Apache Hadoop]]. The language for this platform is called '''Pig Latin'''.&lt;ref name="mainpage"/&gt;  Pig can execute its Hadoop jobs in [[MapReduce]], Apache Tez, or [[Apache Spark]]{{Citation needed|reason=According to development documentation, Pig support for Apache Spark is in development and not currently supported|date=April 2017}}.  Pig Latin abstracts the programming from the [[Java (programming language)|Java]] MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of [[SQL]] for [[relational database management system]]s. Pig Latin can be extended using [[user-defined function]]s (UDFs) which the user can write in Java, [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]] or [[Groovy (programming language)|Groovy]]&lt;ref&gt;{{cite web|url= http://pig.apache.org/docs/r0.11.1/udf.html|title=Pig user defined functions|accessdate=May 3, 2013}}&lt;/ref&gt; and then call directly from the language.

==History==
Apache Pig was originally&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/pig-road-efficient-high-level-language-hadoop-413.html|title=Yahoo Blog:Pig – The Road to an Efficient High-level language for Hadoop|accessdate=May 23, 2015|deadurl=yes|archiveurl=https://web.archive.org/web/20160203181220/https://developer.yahoo.com/blogs/hadoop/pig-road-efficient-high-level-language-hadoop-413.html|archivedate=February 3, 2016|df=}}&lt;/ref&gt; developed at [[Yahoo]] Research around 2006 for researchers to have an ad-hoc way of creating and executing MapReduce jobs on very large data sets. In 2007,&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/pig-incubation-apache-software-foundation-393.html|title=Pig into Incubation at the Apache Software Foundation|accessdate=May 23, 2015|deadurl=yes|archiveurl=https://web.archive.org/web/20160203162733/https://developer.yahoo.com/blogs/hadoop/pig-incubation-apache-software-foundation-393.html|archivedate=February 3, 2016|df=}}&lt;/ref&gt; it was moved into the [[Apache Software Foundation]].&lt;ref&gt;{{cite web |url=http://apache.org/|title=The Apache Software Foundation|accessdate=Nov 1, 2010}}&lt;/ref&gt;

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date
|-
| {{Version|o|0.1}}
| 2008-09-11
| 0.1.1
| 2008-12-05
|-
| {{Version|o|0.2}}
| 2009-04-08
| 0.2.0
| 2009-04-08
|-
| {{Version|o|0.3}}
| 2009-06-25
| 0.3.0
| 2009-06-25
|-
| {{Version|o|0.4}}
| 2009-08-29
| 0.4.0
| 2009-08-29
|-
| {{Version|o|0.5}}
| 2009-09-29
| 0.5.0
| 2009-09-29
|-
| {{Version|o|0.6}}
| 2010-03-01
| 0.6.0
| 2010-03-01
|-
| {{Version|o|0.7}}
| 2010-05-13
| 0.7.0
| 2010-05-13
|-
| {{Version|o|0.8}}
| 2010-12-17
| 0.8.1
| 2011-04-24
|-
| {{Version|o|0.9}}
| 2011-07-29
| 0.9.2
| 2012-01-22
|-
| {{Version|o|0.10}}
| 2012-01-22
| 0.10.1
| 2012-04-25
|-
| {{Version|o|0.11}}
| 2013-02-21
| 0.11.1
| 2013-04-01
|-
| {{Version|o|0.12}}
| 2013-10-14
| 0.12.1
| 2014-04-14
|-
| {{Version|o|0.13}}
| 2014-07-04
| 0.13.0
| 2014-07-04
|-
| {{Version|o|0.14}}
| 2014-11-20
| 0.14.0
| 2014-11-20
|-
| {{Version|o|0.15}}
| 2015-06-06
| 0.15.0
| 2015-06-06
|-
| {{Version|o|0.16}}
| 2016-06-08
| 0.16.0
| 2016-06-08
|-
| {{Version|c|0.17}}
| 2017-06-19
| 0.17.0
| 2017-06-19
|-
| {{Version|c|0.18}}
| 2017-06-19
| 0.18.0
| 2017-12-10
|-
| colspan="5" | {{small|{{Version |l |show=111110}}}}
|}

==Example==
Below is an example of a "[[word count|Word Count]]" program in Pig Latin:

&lt;syntaxhighlight lang=pig&gt;
 input_lines = LOAD '/tmp/my-copy-of-all-pages-on-internet' AS (line:chararray);
 
 -- Extract words from each line and put them into a pig bag
 -- datatype, then flatten the bag to get one word on each row
 words = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;
 
 -- filter out any words that are just white spaces
 filtered_words = FILTER words BY word MATCHES '\\w+';
 
 -- create a group for each word
 word_groups = GROUP filtered_words BY word;
 
 -- count the entries in each group
 word_count = FOREACH word_groups GENERATE COUNT(filtered_words) AS count, group AS word;
 
 -- order the records by count
 ordered_word_count = ORDER word_count BY count DESC;
 STORE ordered_word_count INTO '/tmp/number-of-words-on-internet';
&lt;/syntaxhighlight&gt;

The above program will generate parallel executable tasks which can be distributed across multiple machines in a Hadoop cluster to count the number of words in a dataset such as all the webpages on the internet.

==Pig vs SQL==
In comparison to SQL, Pig
# uses [[lazy evaluation]], 
# uses [[extract, transform, load]] (ETL), 
# is able to store data at any point during a [[Pipeline (software)|pipeline]], 
# declares [[execution plan]]s, 
# supports pipeline splits, thus allowing workflows to proceed along [[directed acyclic graph|DAG]]s instead of strictly sequential pipelines. 
On the other hand, it has been argued [[DBMS]]s are substantially faster than the MapReduce system once the data is loaded, but that loading the data takes considerably longer in the database systems. It has also been argued [[Relational database management system|RDBMS]]s offer out of the box support for column-storage, working with compressed data, indexes for efficient random data access, and transaction-level fault tolerance.&lt;ref&gt;{{cite web|url=http://database.cs.brown.edu/papers/stonebraker-cacm2010.pdf|title=Communications of the ACM: MapReduce and Parallel DBMSs: Friends or Foes?|format=PDF|accessdate=May 23, 2015|deadurl=yes|archiveurl=https://web.archive.org/web/20150701205317/http://database.cs.brown.edu/papers/stonebraker-cacm2010.pdf|archivedate=July 1, 2015|df=}}&lt;/ref&gt;

Pig Latin is [[Procedural programming|procedural]] and fits very naturally in the pipeline paradigm while SQL is instead [[Declarative programming|declarative]]. In SQL users can specify that data from two tables must be joined, but not what join implementation to use (You can specify the implementation of JOIN in SQL, thus "... for many SQL applications the query writer may not have enough knowledge of the data or enough expertise to specify an appropriate join algorithm."). Pig Latin allows users to specify an implementation or aspects of an implementation to be used in executing a script in several ways.&lt;ref name = ypgd /&gt; In effect, Pig Latin programming is similar to specifying a query execution plan, making it easier for programmers to explicitly control the flow of their data processing task.&lt;ref&gt;{{cite web |url=http://infolab.stanford.edu/~olston/publications/sigmod08.pdf|title=ACM SigMod 08: Pig Latin: A Not-So-Foreign Language for Data Processing|format=PDF|accessdate=May 23, 2015}}&lt;/ref&gt;

SQL is oriented around queries that produce a single result. SQL handles trees naturally, but has no built in mechanism for splitting a data processing stream and applying different operators to each sub-stream. Pig Latin script describes a [[directed acyclic graph]] (DAG) rather than a pipeline.&lt;ref name = ypgd /&gt;

Pig Latin's ability to include user code at any point in the pipeline is useful for pipeline development. If SQL is used, data must first be imported into the database, and then the cleansing and transformation process can begin.&lt;ref name=ypgd&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/comparing-pig-latin-sql-constructing-data-processing-pipelines-444.html|title=Yahoo Pig Development Team: Comparing Pig Latin and SQL for Constructing Data Processing Pipelines|accessdate=May 23, 2015|deadurl=yes|archiveurl=https://web.archive.org/web/20150530103839/https://developer.yahoo.com/blogs/hadoop/comparing-pig-latin-sql-constructing-data-processing-pipelines-444.html|archivedate=May 30, 2015|df=}}&lt;/ref&gt;

==See also==
*[[Apache Hive]]
*[[Sawzall (programming language)|Sawzall]] — similar tool from Google

==References==
{{Reflist}}

==External links==
*{{Official website|https://pig.apache.org}}

{{Apache}}

[[Category:Cloud computing]]
[[Category:Query languages]]
[[Category:Data modeling languages]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation|Pig]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Software using the Apache license]]</text>
      <sha1>3bagxgatb79ardhg5yk1ua4d3z1u2oh</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OJB (retired)</title>
    <ns>0</ns>
    <id>17718315</id>
    <revision>
      <id>841237287</id>
      <parentid>840100848</parentid>
      <timestamp>2018-05-14T18:31:36Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2793">{{Other uses|Organization of the Jews in Bulgaria|Orthodox Jewish Bible}}
{{Infobox Software
| name                   = Apache ObJectRelationalBridge
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = 1.0.55
| latest release date    = {{release date|mf=yes|2005|12|31}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Object-relational mapping]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://db.apache.org/ojb/}}
}}

'''Apache ObJectRelationalBridge''' ('''OJB''') is an Object/Relational mapping tool that allows transparent persistence for [[Java (programming language)|Java]] Objects against [[relational database]]s. It was released on April 6, 2005.&lt;ref&gt;[http://www.linuxtoday.com/developer/2005040602826NWSVDV - Linux Today - Internetnews.com: Apache releases Object Relational Bridge]&lt;/ref&gt;

As of January 16, 2011 Apache ObJectRelationalBridge has been retired.&lt;ref&gt;[http://attic.apache.org/projects/ojb.html Apache ObJectRelationalBridge (OJB)], Apache Software Foundation.&lt;/ref&gt;

==Features==
OJB is  an ''[[open source]]'' project.
It is lightweight and easy to use requiring simply configure two files to implement a persistence layer.
It is easy to integrate into an existing application because it does not generate code.
It allows the use of different patterns of persistence: owner (PersistenceBroker [[API]]), [[Java Data Objects|JDO]] and Object Data Management Group  ([[ODMG]]).

==Functionality==
OJB uses an XML based Object/Relational mapping. The mapping resides in a dynamic [[Metadata|MetaData]] layer, which can be manipulated at runtime through a simple [[Metaobject|Meta-Object-Protocol]] (MOP) to change the behaviour of the [[Persistence (computer science)|persistence]] kernel.

==Configuration==
At least two files are required to configure OJB:  OJB.properties and repository.xml

==Allocation==
For mapping a 1-1 relationship, for example, you have two tables: person and account. In this case, a person has an account and vice versa.

==See also==
{{Portal|Free software}}
*[[Apache OpenJPA]]

==References==
{{Reflist}}

==External links==
*[http://db.apache.org/ojb/ Apache ObJectRelationalBridge]

{{Apache}}

[[Category:Apache Software Foundation|OJB]]
[[Category:Apache Software Foundation projects|OJB]]
[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]


{{Software-stub}}</text>
      <sha1>o9fyvt1ap7wl060nzfdip9369i6z430</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Traffic Server</title>
    <ns>0</ns>
    <id>25285165</id>
    <revision>
      <id>840379775</id>
      <parentid>840101244</parentid>
      <timestamp>2018-05-09T14:24:54Z</timestamp>
      <contributor>
        <username>Kahtar</username>
        <id>19297870</id>
      </contributor>
      <minor/>
      <comment>Repair CS1 error(s), replaced: website=https:// → website= using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6324">{{Infobox software
| name                   = Apache Traffic Server
| logo                   = 
| screenshot             =
| caption                =
| author                 =
| developer              = [[Apache Software Foundation]]
| released               =
| latest release version = 7.1.1
| latest release date    = {{Start date and age|2017|09|07}}&lt;ref&gt;{{cite web |url=https://www.apache.org/dyn/closer.cgi/trafficserver/trafficserver-7.1.1.tar.bz2 |title=Download |date=2017-09-07 |website=trafficserver.apache.org/ |publisher=Apache Software Foundation |accessdate=Sep 9, 2017}}&lt;/ref&gt;
| programming language   = [[C++]]
| operating system       = [[Cross-platform]]
| language               = English
| genre                  = [[Web cache]], [[Proxy server]]
| license                = [[Apache License]] 2.0&lt;ref&gt;{{ cite web | url=https://svn.apache.org/repos/asf/incubator/trafficserver/traffic/trunk/LICENSE | title=Traffic Server license file | publisher=[[Apache Software Foundation]] | accessdate=2009-12-24 }}&lt;/ref&gt;
}}
{{Portal|Free software}}
The [[Apache Software Foundation|Apache]] '''Traffic Server''' ('''ATS''') is a modular, high-performance [[reverse proxy]] and [[forward proxy]] server, generally comparable to [[Nginx]] and [[Squid (software)|Squid]].  It was created by [[Inktomi]], and distributed as a commercial product called the Inktomi Traffic Server, before Inktomi was acquired by [[Yahoo!]].

Shortly after Yahoo! released the TS source to Apache as an Apache Incubator project in July 2009,&lt;ref&gt;{{cite web|url=http://wiki.apache.org/incubator/August2009|title=Apache Incubator Wiki August 2009 Board reports|date=2009-08-12}}&lt;/ref&gt; a guest editor on Yahoo!'s online publication ''OStatic'' &lt;ref&gt;{{cite web|url=http://ostatic.com/blog/guest-post-yahoos-cloud-team-open-sources-traffic-server|title=Yahoo's Cloud Team Open Sources Traffic Server|date=2009-11-02}}&lt;/ref&gt; stated that Yahoo! uses TS in production to serve more than 30 billion objects per day on sites like the Yahoo! homepage, and Yahoo! Sports, Mail and Finance.

On April 21, 2010, the Apache board accepted Traffic Server as a TLP, graduating the project out of incubation.&lt;ref&gt;{{cite web|url=http://developer.yahoo.com/blogs/ydn/traffic-server-graduates-top-level-open-source-project-7941.html|title=Traffic Server graduates to top-level open-source project|date=2010-04-23}}&lt;/ref&gt;

==Current version==
The latest stable version is 7.1.1 and was released on September 7, 2017, The latest [[long-term support]] version is 6.2.2 and was released on August 10, 2017.&lt;ref&gt;{{Cite web|url=http://trafficserver.apache.org/downloads|title=Apache Traffic Server Downloads|website=trafficserver.apache.org|language=en|access-date=2017-09-09}}&lt;/ref&gt;

{{As of|2017|09}}, ATS is released in two stable versions. Version 6 is a [[long-term support]] version of ATS while version 7 is the latest stable release, with quarterly minor versions scheduled. Beginning with version 4.0, all releases are considered stable for production, and follow regular ''[[Software versioning|semantic versioning]]''. No more developer preview releases will be made, instead, the [[Git (software)|Git]] master branch is considered preview quality at all times. Long-term support is provided for the last minor version within a major release, for one added year.&lt;ref&gt;{{Cite web|url = https://cwiki.apache.org/confluence/display/TS/Release+Management|title = Official project release management process|date = 2013-09-06}}&lt;/ref&gt;

ATS has, as of v6.0.0, good support for the next generation HTTP protocol, [[HTTP/2]] (a.k.a. H2). On the [https://istlsfastyet.com Is TLS Fast Yet] site, it scores 100%. ATS is actively developed and supported by several [http://trafficserver.apache.org/users.html large companies], as well as many individual contributors.

==Features and performance==
The OStatic post describes TS as shipping ''"... with not only an HTTP web proxy and caching solution, but also ... a server framework, with which you can build very fast servers for other protocols"''.  Traffic Server has been benchmarked to handle 200,000 requests per second or more (small objects out of cache).&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces13|title=The Apache Software Foundation Announces Apache Traffic Server v3.0.0 : The Apache Software Foundation Blog|date=2011-06-14}}&lt;/ref&gt;  At a talk at the 2009 Cloud Computing Expo,&lt;ref&gt;{{cite web|url=http://cloudcomputingexpo.com|title=2009 Cloud Computing Expo|date=2009-08-12}}&lt;/ref&gt; members of the Yahoo! TS team stated that TS is used in production at Yahoo! to handle 400TB of traffic per day using only 150 commodity machines.  The OStatic post describes TS as the ''"product of literally hundreds of developer-years"''.

==Deployment==
In the context of cloud computing, TS would sit conceptually at the edge of the cloud, routing requests as they come in. In Yahoo!, it is used for the edge services as shown in a graphic&lt;ref&gt;{{cite web|url=http://farm3.static.flickr.com/2452/4078596535_a25824be31.jpg|title=Yahoo's edge services graphic| accessdate=2011-06-14|date=2011-06-14}}&lt;/ref&gt; distributed at the 2009 Cloud Computing Expo depicting Yahoo!'s private cloud architecture.  In practical terms, a typical server configuration might use TS to serve static content, such as images, [[JavaScript]], [[Cascading Style Sheets]] (CSS), and HyperText Markup Language ([[HTML]]) files, and route requests for dynamic content to a web server such as [[Apache HTTP Server]].

==References==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

==External links==
* {{Official website}}
* [http://static.usenix.org/events/lisa11/tech/slides/hedstrom.pdf Apache Traffic Server: More Than Just a Proxy] - talk by Leif Hedstrom at [[USENIX]] ({{YouTube|id=RNTw7jZwlKQ|title=Video Archive}})

{{Apache}}

[[Category:Apache Software Foundation|Traffic Server]]
[[Category:Free proxy servers]]
[[Category:Proxy server software for Linux]]
[[Category:Free software programmed in C++]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]
[[Category:Forward proxy]]
[[Category:Reverse proxy]]
[[Category:Unix network-related software]]</text>
      <sha1>ldnursncqycp62147rhlmcmtcgrdy3q</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Xalan</title>
    <ns>0</ns>
    <id>93466</id>
    <revision>
      <id>840102148</id>
      <parentid>840101721</parentid>
      <timestamp>2018-05-07T18:52:33Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Updated Xerces link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2003">{{Infobox software
| name                   = Apache Xalan
| logo                   = 
| logo caption           = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| latest release version = 
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| programming language   = [[C++]] and [[Java (programming language)|Java]]
| operating system       = 
| genre                  = [[Library (computing)|Software library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://xalan.apache.org}}
}}
'''Xalan''' is a popular [[open source]] [[Library (computing)|software library]] from the [[Apache Software Foundation]], that implements the [[XSLT]] 1.0 [[XML]] transformation language and the [[XPath]] 1.0 language. The Xalan XSLT processor is available for both the [[Java (programming language)|Java]] and [[C++]] [[programming language]]s. It combines technology from two main sources: an XSLT processor originally created by [[IBM]] under the name LotusXSL,&lt;ref&gt;{{cite web|url=http://www.alphaworks.ibm.com/formula/lotusxsl/|title=alphaWorks : LotusXSL : Overview}}&lt;/ref&gt; and an XSLT compiler created by [[Sun Microsystems]] under the name XSLTC.&lt;ref&gt;{{cite web|url=http://xml.coverpages.org/ni2002-08-26-a.html|title=Apache Xalan XSLT Compiler (XSLTC) Integrated into the Java Web Services Developer Pack (WSDP).|date=2002-08-26}}&lt;/ref&gt; A wrapper for the Eiffel language is available.&lt;ref&gt;{{cite web|url=https://launchpad.net/xel|title=launchpad : XEL}}&lt;/ref&gt;

==See also==
* [[Java XML]]
* [[Apache Xerces]]
* [[libxml2]]
* [[Saxon XSLT]]

==References==
{{Reflist|2}}

==External links==
* [http://xalan.apache.org Xalan Home page]
{{Apache}}

[[Category:Apache Software Foundation|Xalan]]
[[Category:Java (programming language) libraries]]
[[Category:Java platform]]
[[Category:Software using the Apache license]]
[[Category:XSLT processors]]

{{Compu-library-stub}}</text>
      <sha1>fobd8nwq6vexqpx4cw3kr1rfplx9e83</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Xerces</title>
    <ns>0</ns>
    <id>93465</id>
    <revision>
      <id>844520051</id>
      <parentid>840102082</parentid>
      <timestamp>2018-06-05T13:15:15Z</timestamp>
      <contributor>
        <ip>212.204.93.100</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2563">{{Other uses|Xerces (disambiguation)}}
{{Infobox software
| name                   = Apache Xerces
| logo                   = 
| logo caption           = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| latest release version = 2.12.0 (Xerces J)&lt;br&gt;3.2.1 (Xerces C++)
| latest release date    = 30 April 2018 (Xerces J)&lt;br&gt;1 March 2018 (Xerces C++)
| latest preview version = 
| latest preview date    = 
| programming language   = 
| operating system       = [[Cross-platform]]
| genre                  = [[XML]] parser [[Library (computing)|library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|//xerces.apache.org/}}
}}

In [[computing]], '''Xerces''' is [[Apache Software Foundation|Apache]]'s collection of [[software library|software libraries]] for parsing, validating, serializing and manipulating [[XML]]. The library implements a number of standard [[Application programming interface|API]]s for XML parsing, including [[Document Object Model|DOM]], [[Simple API for XML|SAX]] and SAX2. The implementation is available in the [[Java (programming language)|Java]], [[C++]] and [[Perl]] programming languages.

The name "Xerces" is believed to commemorate the extinct [[Xerces blue]] butterfly (''Glaucopsyche xerces'').&lt;ref&gt;
{{cite book
| last1                 = Benz
| first1                = Brian
| last2                 = Durant
| first2                = John 
| title                 = XML Programming Bible
| url                   = https://books.google.com/books?id=fsR0qWu2glcC
| publisher             = John Wiley &amp; Sons
| publication-date      = 2004
| page                  = 87
| isbn                  = 9780764555763
| accessdate            = 2014-10-01
| quote                 = Apparently, the parser was named after the now extinct Xerces blue butterfly, a native of the San Francisco peninsula.
}}
&lt;/ref&gt;

==See also==
*[[Apache License]]
*[[Java XML]]
*[[Apache Xalan]]

==References==
{{Reflist}}

==External links==
*[//xerces.apache.org/ Apache Xerces Project home]
**[//xerces.apache.org/xerces2-j/ Xerces2 Java Project]
***[//xerces.apache.org/xerces2-j/api.html Xerces2 Javadocs]
**[//xerces.apache.org/xerces-c/ Xerces C++ Project]
**[//xerces.apache.org/xerces-p/ Xerces Perl Project]

{{Apache}}

[[Category:Apache Software Foundation|Xerces]]
[[Category:C++ libraries]]
[[Category:Java (programming language) libraries]]
[[Category:Java platform]]
[[Category:Software using the Apache license]]
[[Category:XML parsers]]</text>
      <sha1>4tizta8iqqa4fdld8cdy3emfst4fnjr</sha1>
    </revision>
  </page>
  <page>
    <title>Apache XMLBeans (retired)</title>
    <ns>0</ns>
    <id>3765547</id>
    <revision>
      <id>840102296</id>
      <parentid>840102239</parentid>
      <timestamp>2018-05-07T18:53:40Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9554">{{Infobox software
| name                   = Apache XMLBeans
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Inactive
| latest release version = 2.6.0
| latest release date    = {{release date|2012|08|14}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[XML data binding|XML binding]]
| license                = [[Apache License]] 2.0
| website                = http://xmlbeans.apache.org
}}
'''XMLBeans''' is a [[Java (programming language)|Java]]-to-[[XML data binding|XML binding]] [[Software framework|framework]] which is part of the [[Apache Software Foundation]] [[XML]] project.

==Description==

XMLBeans is a tool that allows access to the full power of XML in a Java friendly way. The idea is to take advantage of the richness and features of XML and [[XML Schema (W3C)|XML Schema]] and have these features mapped as naturally as possible to the equivalent Java language and typing constructs. XMLBeans uses XML Schema to compile Java interfaces and classes that  can then be used to access and modify XML instance data. Using XMLBeans is similar to using any other Java interface/class: with methods like getFoo or setFoo, just as when working with Java. While a major use of XMLBeans is to access XML instance data with strongly typed Java classes there are also APIs that allow access to the full [[XML Information Set|XML infoset]] (XMLBeans keeps XML Infoset fidelity) as well as to allow reflection into the XML schema itself through an XML Schema Object model.

===Characteristics of XMLBeans===

# Large [[XML Schema (W3C)|XML Schema]] support.
# Large [[XML Information Set|XML Infoset]] support.

Large XML Schema support: XMLBeans fully supports XML Schema and the corresponding java classes provide constructs for all of the major functionality of XML Schema. This is critical since often one has no control over the features of XML Schema needed to work with in Java. Also, XML Schema oriented applications can take full advantage of the power of XML Schema and not have to restrict themselves to a subset.

Large XML Infoset support: When unmarshalling an XML instance the full XML infoset is kept and is available to the developer. This is critical because that subset of XML is not easily represented in Java. For example, order of the elements or comments might be needed in a particular application.

===Objective===

A major objective of XMLBeans has been its applicability in all non-streaming (in memory) XML programming situations. The developer should be able to compile their XML Schema into a set of Java classes and know that they will be able to:

# use XMLBeans for all of the schemas they encounter.
# access the XML at whatever level is necessary without other tools.

=== APIs ===

To accomplish the above objectives, XMLBeans provides three major APIs:

* XmlObject
* XmlCursor
* SchemaType

XmlObject: The java classes that are generated from an XML Schema are all derived from XmlObject.  These provide strongly typed getters and setters for each of the elements within the defined XML.  Complex types are in turn XmlObjects.  For example, getCustomer might return a CustomerType (which is an XmlObject).  Simple types turn into simple getters and setters with the correct java type. For example, getName might return a String.

XmlCursor: From any XmlObject the developer can get an XmlCursor. This provides efficient, low level access to the XML Infoset. A cursor represents a position in the XML instance. The cursor can be moved around the XML instance at any level of granularity needed from individual characters to Tokens.

SchemaType: XMLBeans provides a full XML Schema object model that can be used to reflect on the underlying schema meta information. For example, the developer might generate a sample XML instance for an XML schema or perhaps find the enumerations for an element so that they may be displayed.

== Example ==
An example of a simple XML Schema Definition to describe a country is given below.
&lt;source lang="xml"&gt;
 &lt;?xml version="1.0" encoding="UTF-8"?&gt;
 &lt;xs:schema targetNamespace="http://www.openuri.org/domain/country/v1"
            xmlns:tns="http://www.openuri.org/domain/country/v1"
            xmlns:xs="http://www.w3.org/2001/XMLSchema"
            elementFormDefault="qualified"
            attributeFormDefault="unqualified"
            version="1.0"&gt;
   &lt;xs:element name="Country" type="tns:Country"/&gt;
   &lt;xs:complexType name="Country"&gt;
     &lt;xs:sequence&gt;
       &lt;xs:element name="Name" type="xs:string"/&gt;
       &lt;xs:element name="Population" type="xs:int"/&gt;
       &lt;xs:element name="Iso" type="tns:Iso"/&gt;
     &lt;/xs:sequence&gt;
   &lt;/xs:complexType&gt;
   &lt;xs:complexType name="Iso"&gt;
     &lt;xs:annotation&gt;&lt;xs:documentation&gt;ISO 3166&lt;/xs:documentation&gt;&lt;/xs:annotation&gt;
     &lt;xs:sequence&gt;
       &lt;xs:element name="Alpha2" type="tns:IsoAlpha2"/&gt;
       &lt;xs:element name="Alpha3" type="tns:IsoAlpha3"/&gt;
       &lt;xs:element name="CountryCode" type="tns:IsoCountryCode"/&gt;
     &lt;/xs:sequence&gt;
   &lt;/xs:complexType&gt;
   &lt;xs:simpleType name="IsoCountryCode"&gt;
     &lt;xs:restriction base="xs:int"&gt;
       &lt;xs:totalDigits value="3"/&gt;
     &lt;/xs:restriction&gt;
   &lt;/xs:simpleType&gt;
   &lt;xs:simpleType name="IsoAlpha2"&gt;
     &lt;xs:restriction base="xs:string"&gt;
       &lt;xs:pattern value="[A-Z]{2}"/&gt;
       &lt;xs:whiteSpace value="collapse"/&gt;
     &lt;/xs:restriction&gt;
   &lt;/xs:simpleType&gt;
   &lt;xs:simpleType name="IsoAlpha3"&gt;
     &lt;xs:restriction base="xs:string"&gt;
       &lt;xs:pattern value="[A-Z]{3}"/&gt;
       &lt;xs:whiteSpace value="collapse"/&gt;
     &lt;/xs:restriction&gt;
   &lt;/xs:simpleType&gt;
 &lt;/xs:schema&gt;
&lt;/source&gt;
When the schema is compiled into XMLBean classes (e.g., using [[Apache Ant|Ant]]), it is very easy to create and manipulate XML data that conforms to the schema definition.  The following Java code is a simple example that illustrates how an XML document can be created and validated.
&lt;source lang="java"&gt;
 import org.openuri.domain.country.v1.Country;
 import org.openuri.domain.country.v1.Iso;
 public class CountrySample {
   public static void main(String[] args) {
     Country country = Country.Factory.newInstance();
     country.setName("Denmark");
     country.setPopulation(5450661);  // from Wikipedia :-)
     // print out country XMLBean as XML
     System.out.println(country.xmlText());
     // check if document is valid - will print "Document is invalid"
     // because required Iso child element in not in the object
     System.out.println ("Document is " + (country.validate() ? "valid" : "invalid"));
     // add child with complex type Iso to make the document valid
     Iso iso = country.addNewIso();
     iso.setAlpha2("DK");
     iso.setAlpha3("DNK");
     iso.setCountryCode(208);
     // print out country XMLBean as XML
     System.out.println(country.xmlText());
     // check if document is valid - will print "Document is valid"
     System.out.println ("Document is " + (country.validate() ? "valid" : "invalid"));
   }
 }
&lt;/source&gt;

== History ==

[[David Bau]] was the chief designer for the XMLBeans 1.0 project while he was working for [[BEA Systems|BEA]]. XMLBeans started on the grounds of [[XMLMaps]], an XML binding tool included in previous BEA [[WebLogic]] products. XMLBeans was originally developed as part of the proprietary BEA [[WebLogic]] Workshop Framework, but it was obvious from interviews conducted when it was first announced on January 27, 2003, that BEA wanted it to become an open standard.  At that time it was not decided which organization BEA wanted to involve in the standardization effort. Later that year it was donated to the Apache Software Foundation. The original team included Cezar Cristian Andrei and Eric Vasilik, later the team added Cliff Schmidt and Radu Preotiuc-Pietro, Jacob Danner, Kevin Krouse and Wing Yew Poon.

* January 27, 2003: BEA announces XMLBeans as a technology preview.
* September 24, 2003: BEA donates XMLBeans to the Apache Software Foundation where it joins the [[Apache Incubator|Apache Incubator Project]].
* April 23, 2004: XMLBeans Version 1.0.2 is released. This is the first release from the incubator project.
* June 25, 2004: XMLBeans graduated out of the Apache Incubator Project to become top level project.
* June 30, 2005: XMLBeans Version 2.0 is released.
* November 16, 2005: XMLBeans Version 2.1 is released.
* June 23, 2006: XMLBeans Version 2.2 is released.
* June 1, 2007: XMLBeans Version 2.3 is released.
* July 8, 2008: XMLBeans Version 2.4 is released.
* December 14, 2009: XMLBeans Version 2.5 is released.
* August 14, 2012: XMLBeans Version 2.6 is released.
* May 23, 2014: XMLBeans is officially retired and active development is ceased.

== See also ==

* [[Java Architecture for XML Binding]] (JAXB)
* [[xmlbeansxx]] &amp;mdash; [[XML Data Binding]] code generator for [[C++]]

== External links ==
* {{Official website|http://xmlbeans.apache.org/}}
* [http://xmlbeans.apache.org/resources/index.html XMLBeans resources]
* [http://xml.apache.org/ The Apache XML project]
* [http://xmlbeans.apache.org/docs/2.0.0/guide/antXmlbean.html XMLBean Ant task]
* [http://xmlbeans.googlepages.com/ XML Binding Resources - xbeans of popular schemas]
{{apache}}

{{DEFAULTSORT:Xmlbeans}}
[[Category:Java platform]]
[[Category:Java development tools]]
[[Category:XML parsers]]
[[Category:Apache Software Foundation|XMLBeans]]</text>
      <sha1>e7wbbj07ob3h81lb62dan9jptfs7wnm</sha1>
    </revision>
  </page>
  <page>
    <title>Apache cTAKES</title>
    <ns>0</ns>
    <id>32899106</id>
    <revision>
      <id>840102822</id>
      <parentid>840102736</parentid>
      <timestamp>2018-05-07T18:57:49Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Added project name to category for correct listing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6447">{{ Infobox Software
| name                   = cTAKES
| logo                   =
| screenshot             =
| caption                =
| collapsible            =
| developer              = [[Apache Software Foundation]]
| latest release date    = {{release date|mf=yes|2017|04|25}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Natural language processing]], [[Bioinformatics]], [[Text mining]], [[Information Extraction]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://ctakes.apache.org/}}
}}

'''Apache cTAKES: clinical Text Analysis and Knowledge Extraction System''' is an open-source natural language processing system for information extraction from [[electronic health record]] clinical free-text. It processes clinical notes, identifying types of clinical named entities — drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures. Each named entity has attributes for the text span, the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated.&lt;ref&gt;{{Cite book|url={{google books|yVp4CgAAQBAJ|plainurl=yes}}|title=Health Web Science: Social Media Data for Healthcare|last=Denecke|first=Kerstin|date=2015-08-31|publisher=Springer|isbn=978-3-319-20582-3 |chapter=Tools and Resources for Information Extraction |page=[{{google books|yVp4CgAAQBAJ|page=67|plainurl=yes}} 67] |via=Google Books }}&lt;/ref&gt;

cTAKES was built using the  [[UIMA|UIMA Unstructured Information Management Architecture framework]] and [[OpenNLP]] natural language processing toolkit.&lt;ref&gt;{{Cite journal|last=Khalifa|first=Abdulrahman|last2=Meystre|first2=Stéphane|date=2015-12-01|title=Adapting existing natural language processing resources for cardiovascular risk factors identification in clinical notes|url=http://www.sciencedirect.com/science/article/pii/S1532046415001690|journal=Journal of Biomedical Informatics|series=Proceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data|volume=58|issue=Supplement|pages=S128–S132|doi=10.1016/j.jbi.2015.08.002}}&lt;/ref&gt;&lt;ref&gt;{{Cite press release|url=https://globenewswire.com/news-release/2017/04/25/970806/0/en/The-Apache-Software-Foundation-Announces-Apache-cTAKES-v4-0.html|title=The Apache Software Foundation Announces Apache® cTAKES™ v4.0|publisher=The Apache Software Foundation|first=Sally |last=Khudairi |date=2017-04-25 |location=Forest Hill, MD |agency=Globe Newswire |access-date=2017-09-20}}&lt;/ref&gt; Its components are specifically trained for the clinical domain, and create rich linguistic and semantic annotations that can be utilized by clinical decision support systems and clinical research.

These components include:
* Named Section identifier
* Sentence boundary detector
* Rule-based tokenizer
* Formatted list identifier
* Normalizer
* Context dependent tokenizer
* Part-of-speech tagger
* Phrasal chunker
* Dictionary lookup annotator
* Context annotator
* Negation detector
* Uncertainty detector
* Subject detector
* Dependency parser
* patient smoking status identifier
* Drug mention annotator&lt;ref&gt;{{Cite journal|last=Savova|first=Guergana K|last2=Masanz|first2=James J|last3=Ogren|first3=Philip V|last4=Zheng|first4=Jiaping|last5=Sohn|first5=Sunghwan|last6=Kipper-Schuler|first6=Karin C|last7=Chute|first7=Christopher G|date=2010|title=Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications|journal=Journal of the American Medical Informatics Association : JAMIA|volume=17|issue=5|pages=507–513|doi=10.1136/jamia.2009.001560|issn=1067-5027|pmc=2995668|pmid=20819853}}&lt;/ref&gt;

== History ==
Development of cTAKES began at the [[Mayo Clinic]] in 2006. The development team, led by Dr. Guergana Savova and Dr. [[Christopher G. Chute|Christopher Chute]], included physicians, computer scientists and software engineers. After its deployment, cTAKES became an integral part of Mayo's clinical data management infrastructure, processing more than 80 million clinical notes.&lt;ref name=cTAKES_history&gt;{{cite web |date=2015-06-22 |title=History |website=Apache cTAKES™ - clinical Text Analysis Knowledge Extraction System |url=http://ctakes.apache.org/history.html |access-date=2018-01-11 }}&lt;/ref&gt;

When Dr. Savova's moved to [[Boston Children's Hospital]] in early 2010, the core development team grew to include members there. Further external collaborations include:&lt;ref name=cTAKES_history/&gt;
* University of Colorado
* Brandeis University
* University of Pittsburgh
* University of California at San Diego
Such collaborations have extended cTAKES' capabilities into other areas such as Temporal Reasoning, Clinical Question Answering, and coreference resolution for the clinical domain.&lt;ref name=cTAKES_history/&gt;

In 2010, cTAKES was adopted by the [http://www.i2b2.org i2b2] program and is a central component of the [https://www.healthit.gov/policy-researchers-implementers/secondary-use-ehr-data SHARP Area 4].&lt;ref name=cTAKES_history/&gt;

In 2013, cTAKES released their first release as an Apache incubator project: [http://incubator.apache.org/ctakes/ cTAKES 3.0].

In March 2013, cTAKES became an Apache Top Level Project (TLP).&lt;ref name=cTAKES_history/&gt;

== See also ==
* [[OpenNLP]]
* [[UIMA]]
* [[Electronic Health Record]]
* [[Unified Medical Language System]]

== References ==
&lt;references /&gt;

== External links ==
* [http://ctakes.apache.org cTAKES Official Website]
* [http://jamia.bmj.com/content/17/5/507.abstract Abstract (JAMIA)]
* [http://ohnlp.org/ Open Health Natural Language Processing (OHNLP) Consortium]
* [http://healthit.hhs.gov/portal/server.pt?open=512&amp;mode=2&amp;objID=3128&amp;PageID=20708 Strategic Health IT Advanced Research Projects (SHARP) Program]
* [http://sharpn.org/ SHARP Area 4 - Secondary Use of EHR Data]
* [http://maveric.org/mig/arc.html The Automated Retrieval Console (ARC)]
* [http://www.i2b2.org Informatics for Integrating Biology and the Bedside]

{{apache}}
{{Health software}}

[[Category:Apache Software Foundation|cTAKES]]
[[Category:Apache Software Foundation projects|cTAKES]]
[[Category:Electronic health record software]]
[[Category:Natural language processing software]]
[[Category:Free healthcare software]]
[[Category:Bioinformatics software]]</text>
      <sha1>grnu5kb962l7qh6b7q4ktbym5bjydgm</sha1>
    </revision>
  </page>
  <page>
    <title>Apache OpenMeetings</title>
    <ns>0</ns>
    <id>25034980</id>
    <revision>
      <id>841249869</id>
      <parentid>840506222</parentid>
      <timestamp>2018-05-14T19:29:32Z</timestamp>
      <contributor>
        <username>Roylenferink</username>
        <id>30326102</id>
      </contributor>
      <minor/>
      <comment>Roylenferink moved page [[OpenMeetings]] to [[Apache OpenMeetings]]: Naming conform the ASF project name</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5489">{{Infobox Software
| name                   = OpenMeetings
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = Maxim Solodovnik, Sebastian Wagner, community
| latest release version = 4.03| latest release date    = {{release date and age|2018|04|18}}
| operating system       = [[Mac OS X]], [[Microsoft Windows]], [[Solaris (operating system)|Solaris]], [[Red Hat Enterprise Linux]], [[Novell SUSE Linux]], [[Ubuntu (operating system)|Ubuntu]]
| genre                  = [[Collaborative]] [[workspace]], [[Web conferencing]]
| license                = [[Apache License 2.0]]
| website                = {{URL|http://openmeetings.apache.org}}
}}
'''OpenMeetings''' is software used for [[Web based presentation tools|presenting]], online training, [[web conferencing]], [[Collaborative software|collaborative whiteboard drawing and document editing]], and user desktop sharing. The product is based on   [[Red5 (media server)|Red5 media server,]] [[HTML5]] and  [[Adobe Flash|Flash]] which in turn are based on a number of open source components. Communication takes place in virtual "meeting rooms" which may be set to different communication, security and video quality modes. The recommended database engine for backend support is [[MySQL]]. The product can be set up as an installed server product, or used as a hosted service.

Work on OpenMeetings started in 2007 by Sebastian Wagner. Since 2009 the project became open which helped to involve other developers from different countries. Starting from 2011 main project development and technical support moved to Russia. In the meantime, web conferencing services based on OpenMeetings formally are offered by about a dozen companies around the world. Since 2012, the project is being developed under the auspices of open-source devoted [[Apache Software Foundation]] (ASF) and possesses [[Apache License]], which allows it to be used in commercial projects. Since 2012 OpenMeetings progress is presented regularly at the [[ApacheCon]].

Public facilities include the educational intranet "Koblenzer Schulnetz" in [[Koblenz]], Germany and two public demo-servers.&lt;ref&gt;http://openmeetings.apache.org/demo.html&lt;/ref&gt;

Articles have been published at ZDNet Blogs&lt;ref&gt;{{cite web|last=Stewart |first=Ryan |url=http://blogs.zdnet.com/Stewart/?p=406 |title=openmeetings - Open source video conferencing and collaboration with OpenLaszlo &amp;#124; ZDNet |publisher=Blogs.zdnet.com |date=2007-06-04 |accessdate=2013-09-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20100428081109/http://blogs.zdnet.com/Stewart/?p=406 |archivedate=2010-04-28 |df= }}&lt;/ref&gt; and a publication in LinuxMag France Page 40-44&lt;ref&gt;{{cite web|url=http://www.ed-diamond.com/feuille_lmag107/index.html |title=Les Editions Diamond |publisher=Ed-diamond.com |date= |accessdate=2013-09-01}}&lt;/ref&gt; and Ajax Magazine.&lt;ref&gt;{{cite web | url=http://ajax.phpmagazine.net/2007/05/openmeetings_openlaszlo_videoc.html | title=OpenMeetings, An OpenLaszlo Based Video-Conferencing Solution | date={{format date|2007|5|26}} | archiveurl=https://web.archive.org/web/20091006202153/http://ajax.phpmagazine.net/2007/05/openmeetings_openlaszlo_videoc.html | archivedate={{format date|2009|10|06}} | dead-url=yes | accessdate={{format date|2016|08|01}} }}&lt;/ref&gt;

OpenMeetings is used for web conferencing in [[Free and open-source software|FOSS]] e-learning solutions [[Moodle]]&lt;ref&gt;{{cite web|last=Huff |first=Mark |url=http://moodle.org/mod/forum/discuss.php?d=97133 |title=Using Moodle: new VideoConferencing Plugin(s) for OpenMeetings 0.5.1 |publisher=Moodle.org |date= |accessdate=2013-09-01}}&lt;/ref&gt; and [[Atutor]]&lt;ref&gt;{{cite web|url=http://www.atutor.ca/atutor/modules/index.php |title=ATutor: Learning Content Management System: Download: |publisher=Atutor.ca |date= |accessdate=2013-09-01}}&lt;/ref&gt;. Now OpenMeetings is integrated with several [[Content Management System|CMS]], [[Customer relationship management|CRM]] and other systems. Project has been downloaded over 250 000 times&lt;ref&gt;{{cite web|url=http://code.google.com/p/openmeetings/downloads/list?can=1&amp;q=&amp;colspec=Filename+OpSys+Summary+Uploaded+UploadedBy+Size+DownloadCount |title=Downloads - openmeetings - Open-Source Web-Conferencing - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2013-09-01}}&lt;/ref&gt;.OpenMeetings is available in 31 languages&lt;ref&gt;{{cite web|url=http://code.google.com/p/openmeetings/wiki/Internationalisation |title=Internationalisation - openmeetings - This Page should documentate the progress of internationalisation - Open-Source Web-Conferencing - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2013-09-01}}&lt;/ref&gt;.    

So far, OpenMeetings is almost ported to HTML5.   

==Features==
Open Meetings implements the following features:
*Audio communication
*Video conferencing
*Meeting recording
*Screen sharing
*Collaborative document editing
*Chat and white boarding
*User and room management
*Mobile client for Android

==See also==
* [[Comparison of web conferencing software]]

==References==
{{Reflist}}

==External links==
*{{official website|http://openmeetings.apache.org/}}
*[http://weblog.openlaszlo.org/archives/2007/05/sebastian-wagner-releases-openmeetings/ Release archives on weblog.openlaszlo.org]

[[Category:Remote desktop]]
[[Category:Web conferencing]]
[[Category:Free software]]
[[Category:Apache Software Foundation]]
[[Category:Software using the Apache license]]</text>
      <sha1>0ybzvansgkobr1opi3wpamzp88ewkxf</sha1>
    </revision>
  </page>
</mediawiki>
